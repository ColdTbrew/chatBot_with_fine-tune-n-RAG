VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization Seunghwan Choi*Sunghyun Park*Minsoo Lee*Jaegul Choo KAIST, Daejeon, South Korea fshadow2496, psh01087, alstn2022, jchoo g@kaist.ac.kr Reference Image Synthetic Image Figure 1: Given a pair of a reference image (containing a person) and a target clothing image, our method successfully synthesizes 1024768 virtual try-on images.Abstract The task of image-based virtual try-on aims to transfer a target clothing item onto the corresponding region of a person, which is commonly tackled by Ô¨Åtting the item to the desired body part and fusing the warped item with the person.While an increasing number of studies have been conducted, the resolution of synthesized images is still lim- ited to low ( e.g., 256192), which acts as the critical lim- itation against satisfying online consumers.
We argue that the limitation stems from several challenges: as the resolu- tion increases, the artifacts in the misaligned areas between the warped clothes and the desired clothing regions become noticeable in the Ô¨Ånal results; the architectures used in ex- *These authors contributed equally.isting methods have low performance in generating high- quality body parts and maintaining the texture sharpness of the clothes. To address the challenges, we propose a novel virtual try-on method called VITON-HD that successfully synthesizes 1024768 virtual try-on images.SpeciÔ¨Åcally, we Ô¨Årst prepare the segmentation map to guide our virtual try-on synthesis, and then roughly Ô¨Åt the target clothing item to a given person‚Äôs body.Next, we propose ALIgnment- Aware Segment (ALIAS) normalization and ALIAS genera- tor to handle the misaligned areas and preserve the details of 1024768 inputs.
Through rigorous comparison with ex- isting methods, we demonstrate that VITON-HD highly sur- passes the baselines in terms of synthesized image quality both qualitatively and quantitatively. Code is available at https://github.com/shadow2496/VITON-HD .arXiv:2103.16874v2 [cs.CV] 10 Sep 2021 1.Introduction Image-based virtual try-on refers to the image generation task of changing the clothing item on a person into a differ- ent item, given in a separate product image.With a growing trend toward online shopping, virtually wearing the clothes can enrich a customer‚Äôs experience, as it gives an idea about how these items would look on them.Virtual try-on is similar to image synthesis, but it has unique and challenging aspects.Given images of a person and a clothing product, the synthetic image should meet the following criteria: (1) The person‚Äôs pose, body shape, and identity should be preserved.
(2) The clothing prod- uct should be naturally deformed to the desired clothing re- gion of the given person, by reÔ¨Çecting his/her pose and body shape. (3) Details of the clothing product should be kept in- tact.(4) The body parts initially occluded by the person‚Äôs clothes in the original image should be properly rendered.Since the given clothing image is not initially Ô¨Åtted to the person image, fulÔ¨Ålling these requirements is challenging, which leaves the development of virtual try-on still far be- hind the expectations of online consumers.In particular, the resolution of virtual try-on images is low compared to the one of normal pictures on online shopping websites.After Han et al.[10] proposed VITON, various image- based virtual try-on methods have been proposed [31, 36, 35, 6].
These methods follow two processes in common: (1) warping the clothing image initially to Ô¨Åt the human body; (2) fusing the warped clothing image and the image of the person that includes pixel-level reÔ¨Ånement. Also, several recent methods [9, 36, 35] add a module that generates seg- mentation maps and determine the person‚Äôs layout from the Ô¨Ånal image in advance.However, the resolution of the synthetic images from the previous methods is low ( e.g., 256192) due to the follow- ing reasons.First, the misalignment between the warped clothes and a person‚Äôs body results in the artifacts in the misaligned regions, which become noticeable as the image size increases.It is difÔ¨Åcult to warp clothing images to Ô¨Åt the body perfectly, so the misalignment occurs as shown in Fig.2.Most of previous approaches utilize the thin-plate spline (TPS) transformation to deform clothing images.
To accurately deform clothes, ClothFlow [9] predicts the op- tical Ô¨Çow maps of the clothes and the desired clothing re- gions. However, the optical Ô¨Çow maps does not remove the misalignment completely on account of the regulariza- tion.In addition, the process requires more computational costs than other methods due to the need of predicting the movement of clothes at a pixel level.(The detailed analysis of ClothFlow is included in the supplementary.) Second, a simple U-Net architecture [25] used in existing approaches is insufÔ¨Åcient in synthesizing initially occluded body parts in Ô¨Ånal high-resolution ( e.g., 1024768) images.As noted in Wang et al.[32], applying a simple U-Net-based archi- (c)Misaligned Regions (b)Warped Clothes on Segmentation(a)Warped Clothes on Reference ImageFigure 2: An example of misaligned regions.tecture to generate high-resolution images leads to unstable training as well as unsatisfactory quality of generated im- ages.
Also, reÔ¨Åning the images once at the pixel level is in- sufÔ¨Åcient in preserving the details of high-resolution cloth- ing images. To address the above-mentioned challenges, we pro- pose a novel high-resolution virtual try-on method, called VITON-HD.In particular, we introduce a new clothing- agnostic person representation that leverages the pose in- formation and the segmentation map so that the clothing information is eliminated thoroughly.Afterwards, we feed the segmentation map and the clothing item deformed to Ô¨Åt the given human body to the model.Using the addi- tional information, our novel ALIgnment-Aware Segment (ALIAS) normalization removes information irrelevant to the clothing texture in the misaligned regions and propa- gates the semantic information throughout the network.The normalization separately standardizes the activations corre- sponding to the misaligned regions and the other regions, and modulates the standardized activations using the seg- mentation map.
Our ALIAS generator employing ALIAS normalization synthesizes the person image wearing the tar- get product while Ô¨Ålling the misaligned regions with the clothing texture and preserving the details of the clothing item through the multi-scale reÔ¨Ånement at a feature level. To validate the performance of our framework, we col- lected a 1024768 dataset that consists of pairs of a per- son and a clothing item for our research purpose.Our ex- periments demonstrate that VITON-HD signiÔ¨Åcantly out- performs the existing methods in generating 1024 768 im- ages, both quantitatively and qualitatively.We also conÔ¨Årm the superior capability of our novel ALIAS normalization module in dealing with the misaligned regions.We summarize our contributions as follows: ‚Ä¢ We propose a novel image-based virtual try-on ap- proach called VITON-HD, which is, to the best of our knowledge, the Ô¨Årst model to successfully synthesize 1024768 images.
‚Ä¢ We introduce a clothing-agnostic person representa- tion that allows our model to remove the dependency on the clothing item originally worn by the person. ‚Ä¢ To address the misalignment between the warped clothes and the desired clothing regions, we propose ALIAS normalization and ALIAS generator, which is effective in maintaining the details of clothes.‚Ä¢ We demonstrate the superior performance of our method through experiments with baselines on the newly collected dataset.2.Related Work Conditional Image Synthesis.Conditional generative adversarial networks (cGANs) utilize additional informa- tion, such as class labels [19, 2], text [24, 34], and at- tributes [28], to steer the image generation process.Since the emergence of pix2pix [14], numerous cGANs condi- tioned on input images have been proposed to generate high-resolution images in a stable manner [32, 1, 21].
How- ever, these methods tend to generate blurry images when handling a large spatial deformation between the input im- age and the target image. In this paper, we propose a method that can address the spatial deformation of input images and properly generate 1024 768 images.Normalization Layers.Normalization layers [13, 30] have been widely applied in modern deep neural networks.Normalization layers, whose afÔ¨Åne parameters are esti- mated with external data, are called conditional normaliza- tion layers.Conditional batch normalization [4] and adap- tive instance normalization [12] are such conditional nor- malization techniques and have been used in style transfer tasks.SPADE [20] and SEAN [39] utilize segmentation maps to apply spatially varying afÔ¨Åne transformations.Us- ing the misalignment mask as external data, our proposed normalization layer computes the means and the variances of the misaligned area and the other area within an instance separately.
After standardization, we modulate standard- ized activation maps with afÔ¨Åne parameters inferred from human-parsing maps to preserve semantic information. Virtual Try-On Approaches.There are two main cat- egories for virtual try-on approaches: 3D model-based approaches [8, 27, 23, 22] and 2D image-based ap- proaches [10, 31, 9, 36, 35, 5].3D model-based approaches can accurately simulate the clothes but are not widely appli- cable due to their dependency on 3D measurement data.2D image-based approaches do not rely on any 3D in- formation, thus being computationally efÔ¨Åcient and appro- priate for practical use.Jetchev and Bergmann [15] pro- posed CAGAN, which Ô¨Årst introduced the task of swap- ping fashion articles on human images.VITON [10] ad- dressed the same problem by proposing a coarse-to-Ô¨Åne synthesis framework that involves TPS transformation of clothes.
Most existing virtual try-on methods tackle differ- ent aspects of VITON to synthesize perceptually convincing photo-realistic images. CP-VTON [31] adopted a geometric matching module to learn the parameters of TPS transfor-mation, which improves the accuracy of deformation.VT- NFP [36] and ACGPN [35] predicted the human-parsing maps of a person wearing the target clothes in advance to guide the try-on image synthesis.Even though the image quality at high resolution is an essential factor in evaluating the practicality of the generated images, none of the meth- ods listed above could generate such photo-realistic images at high resolution.3.Proposed Method Model Overview.As described in Fig.
3, given a ref- erence image I2R3HWof a person and a clothing imagec2R3HW(HandWdenote the image height and width, respectively), the goal of VITON-HD is to gen- erate a synthetic image ^I2R3HWof the same person wearing the target clothes c, where the pose and body shape ofIand the details of care preserved. While training the model with (I;c;^I)triplets is straightforward, construction of such dataset is costly.Instead, we use (I;c;I )where the person in the reference image Iis already wearing c.Since directly training on (I;c;I )can harm the model‚Äôs generalization ability at test time, we Ô¨Årst compose a clothing-agnostic person representation that leaves out the clothing information in Iand use it as an input.Our new clothing-agnostic person representation uses both the pose map and the segmentation map of the person to eliminate the clothing information in I(Section 3.1).
The model gen- erates the segmentation map from the clothing-agnostic per- son representation to help the generation of ^I(Section 3.2). We then deform cto roughly align it to the human body (Section 3.3).Lastly, we propose the ALIgnment-Aware Segment (ALIAS) normalization that removes the mislead- ing information in the misaligned area after deforming c.ALIAS generator Ô¨Ålls the misaligned area with the clothing texture and maintains the clothing details (Section 3.4).3.1.Clothing-Agnostic Person Representation To train the model with pairs of candIalready wearing c, a person representation without the clothing information inIhas been utilized in the virtual try-on task.
Such rep- resentations have to satisfy the following conditions: (1) the original clothing item to be replaced should be deleted; (2) sufÔ¨Åcient information to predict the pose and the body shape of the person should be maintained; (3) the regions to be preserved ( e.g., face and hands) should be kept to main- tain the person‚Äôs identity. Problems of Existing Person Representations.In or- der to maintain the person‚Äôs shape, several approaches [10, 31, 36] provide a coarse body shape mask as a cue to syn- thesize the image, but fail to reproduce the body parts elab- orately ( e.g., hands).
To tackle this issue, ACGPN [35] em- ploys the detailed body shape mask as the input, and the neural network attempts to discard the clothing informa- ·àòùëÜ ùúÉ Warpùêºùëé‚®ÅùëÉ‚®Å·àòùëÜùëê ùëêùí≤(ùëê,ùúÉ) ùëÜùëÜùëé Remove Clothing & Arms ùêº ùëÉ Remove Clothing & Arms ùëÜùëé‚®ÅùëÉ‚®Åùëê ·àòùêº(c) Clothes Deformation(a) Pre -processing (b) Segmentation Generation (d) Try -On Synthesis ALIAS GeneratorSegmentation Generator ùêºùëé ·àòùëÜ Geometric Matching Module ùêºùëé‚®ÅùëÉ‚®Åùí≤(ùëê,ùúÉ)ùëÄùëöùëñùë†ùëéùëôùëñùëîùëõFigure 3: Overview of a VITON-HD. (a) First, given a reference image Icontaining a target person, we predict the seg- mentation map Sand the pose map P, and utilize them to pre-process IandSas a clothing-agnostic person image Ia and segmentation Sa.(b) Segmentation generator produces the synthetic segmentation ^Sfrom (Sa;P;c).(c) Geometric matching module deforms the clothing image caccording to the predicted clothing segmentation ^Scextracted from ^S.
(d) Finally, ALIAS generator synthesizes the Ô¨Ånal output image ^Ibased on the outputs from the previous stages via our ALIAS normalization. tion to be replaced.However, since the body shape mask includes the shape of the clothing item, neither the coarse body shape mask nor the neural network could perfectly eliminate the clothing information.As a result, the original clothing item that is not completely removed causes prob- lems in the test phase.Clothing-Agnostic Person Representation.We pro- pose a clothing-agnostic image Iaand a clothing-agnostic segmentation map Saas inputs of each stage, which truly eliminate the shape of clothing item and preserve the body parts that need to be reproduced.We Ô¨Årst predict the segmentation map S2LHWand the pose map P2 R3HWof the image Iby utilizing the pre-trained net- works [7, 3], where Lis a set of integers indicating the se- mantic labels.
The segmentation map Sis used to remove the clothing region to be replaced and preserve the rest of the image. The pose map Pis utilized to remove the arms, but not the hands, as they are difÔ¨Åcult to reproduce.Based onSandP, we generate the clothing-agnostic image Ia and the clothing-agnostic segmentation map Sa, which al- low the model to remove the original clothing information thoroughly, and preserve the rest of the image.In addition, unlike other previous work, which adopts the pose heatmap with each channel corresponded to one keypoint, we con-catenateIaorSato the RGB pose map Prepresenting a skeletal structure that improves generation quality.3.2.Segmentation Generation Given the clothing-agnostic person representation (Sa;P), and the target clothing item c, the segmentation generatorGSpredicts the segmentation map ^S2LHW of the person in the reference image wearing c.
We train GSto learn the mapping between Sand(Sa;P;c), in which the original clothing item information is completely removed. As the architecture of GS, we adopt U-Net [25], and the total loss LSof the segmentation generator are written as LS=LcGAN +CELCE; (1) whereLCEandLcGAN denote the pixel-wise cross-entropy loss and conditional adversarial loss between ^SandS, re- spectively.CEis the hyperparameter corresponding to the relative importance between two losses.3.3.Clothing Image Deformation In this stage, we deform the target clothing item cto align it with ^Sc, which is the clothing area of ^S.We employ the geometric matching module proposed in CP- VTON [31] with the clothing-agnostic person representa- ConvALIAS ResBlkUpALIAS ResBlk ‚ãØConvResize Resize Resize Resize ‚ãØ 1024x768·àòùêº Conv ALIAS NormReLUConvALIAS NormReLUConv ALIAS NormReLUConv·àòùëÜ,ùëÄùëöùëñùë†ùëéùëôùëñùëîùëõ ‚Ñéùëñ‚Ñéùëñ+1 ùëØi√óùëæùíäConcat Add(b)ALIAS ResBlk ùêºùëé‚®ÅùëÉ‚®Åùí≤(ùëê,ùúÉ) (a)ALIAS GeneratorConv ùëÄùëöùëñùë†ùëéùëôùëñùëîùëõ·àòùëÜùêºùëé‚®ÅùëÉ‚®Åùëä(ùëê,ùúÉ)Figure 4: ALIAS generator.
(a) The ALIAS generator is composed of a series of ALIAS residual blocks, along with up- sampling layers. The input (Ia;P;W(c;))is resized and injected into each layer of the generator.(b) A detailed view of a ALIAS residual block.Resized (Ia;P;W(c;))is concatenated to hiafter passing through a convolution layer.Each ALIAS normalization layer leverages resized ^SandMmisalign to normalize the activation.tion(Ia;P)and^Scas inputs.A correlation matrix be- tween the features extracted from (Ia;P)andcis Ô¨Årst cal- culated .With the correlation matrix as an input, the regres- sion network predicts the TPS transformation parameters 2R255, and thencis warped by .In the training phase, the model takes Scextracted from Sinstead of ^Sc.The module is trained with the L1 loss between the warped clothes and the clothes Icthat is extracted from I.In addi- tion, the second-order difference constraint [35] is adopted to reduce obvious distortions in the warped clothing images from deformation.
The overall objective function to warp the clothes to Ô¨Åt the human body is written as Lwarp =jjIc W(c;)jj1;1+constLconst; (2) whereWis the function that deforms cusing,Lconst is a second-order difference constraint, and const is the hyper- parameter forLconst . 3.4.Try-On Synthesis via ALIAS Normalization We aim to generate the Ô¨Ånal synthetic image ^Ibased on the outputs from the previous stages.Overall, we fuse the clothing-agnostic person representation (Ia;P) and the warped clothing image W(c;), guided by ^S.(Ia;P;W(c;))is injected into each layer of the generator.For^S, we propose a new conditional normalization method called the ALIgnment-Aware Segment (ALIAS) normaliza- tion.ALIAS normalization enables the preservation of se- mantic information, and the removal of misleading infor- mation from the misaligned regions by leveraging ^Sand the mask of these regions.Alignment-Aware Segment Normalization.
Let us de- notehi2RNCiHiWias the activation of the i-th layer of a network for a batch of Nsamples, where Hi,Wi, and Ciindicate the height, width, and the number of channels ofhi, respectively. ALIAS normalization has two inputs: StandardizationConv ConvùõΩ ùõæ ùëÄùëöùëñùë†ùëéùëôùëñùëîùëõ ‚àí= ùëÄùëéùëôùëñùëîùëõ·àòùëÜùëêConv ·àòùëÜùëëùëñùë£Figure 5: ALIAS normalization.First, the activation is separately standardized according to the regions divided by Mmisalign , which can be obtained from the difference be- tween ^ScandMalign .Next, ^Sdivis convolved to create the modulation parameters and , and then the standardized activation is modulated with the parameters and .(1) the synthetic segmentation map ^S; (2) the misalign- ment binary mask Mmisalign2LHW, which excludes the warped mask of the target clothing image W(Mc;)from ^Sc(Mcdenotes the target clothing mask), i.e., Malign =^Sc\W(Mc;) (3) Mmisalign =^Sc Malign: (4) Fig.5 illustrates the workÔ¨Çow of the ALIAS normaliza- tion.We Ô¨Årst obtain Malign andMmisalign from Eq.
(3) and Eq. (4).We deÔ¨Åne the modiÔ¨Åed version of ^Sas^Sdiv, where ^Scin^Sseparates into Malign andMmisalign .ALIAS nor- malization standardizes the regions of Mmisalign and the other regions in hiseparately, and then modulates the stan- dardized activation using afÔ¨Åne transformation parameters 256192 512 384 1024 768 SSIM "LPIPS #FID # SSIM "LPIPS #FID # SSIM "LPIPS #FID # CP-VTON 0.739 0.159 56.23 0.791 0.141 31.96 0.786 0.158 43.28 ACGPN 0.842 0.064 26.45 0.863 0.067 15.22 0.856 0.102 43.39 VITON-HD* - - - - - - 0.893 0.054 12.47 VITON-HD 0.844 0.062 27.83 0.870 0.052 14.05 0.895 0.053 11.74 Table 1: Quantitative comparison with baselines across different resolutions.VITON-HD* is a VITON-HD variant where the standardization in ALIAS normalization is replaced by channel-wise standardization as in the original instance normalization.For the SSIM, higher is better.For the LPIPS and the FID, lower is better.inferred from ^Sdiv.
The activation value at site ( n2N;k2 Ci;y2Hi;x2Wi) is calculated by i k;y;x(^Sdiv)hi n;k;y;x i;m n;k i;m n;k+ i k;y;x(^Sdiv); (5) wherehi n;k;y;x is the activation at the site before normal- ization and i k;y;x and i k;y;x are the functions that convert ^Sdivto modulation parameters of the normalization layer. i;m n;kandi;m n;kare the mean and standard deviation of the activation in sample nand channel k.i;m n;kandi;m n;kare calculated by i;m n;k=1 j i;m njX (y;x)2 i;m nhi n;k;y;x (6) i;m n;k=vuut1 j i;m njX (y;x)2 i;m n(hi n;k;y;x i;m n;k)2;(7) where i;m ndenotes the set of pixels in region m, which is Mmisalign or the other region, and j i;m njis the number of pixels in i;m n.Similar to instance normalization [30], the activation is standardized per channel.However, ALIAS normalization divides the activation in channel kinto the activation in the misaligned region and the other region.
The rationale behind this strategy is to remove the mis- leading information in the misaligned regions. SpeciÔ¨Åcally, the misaligned regions in the warped clothing image match the background that is irrelevant to the clothing texture.Per- forming a standardization separately on these regions leads to a removal of the background information that causes the artifacts in the Ô¨Ånal results.In modulation, afÔ¨Åne pa- rameters inferred from the segmentation map modulate the standardized activation.Due to injecting semantic informa- tion at each ALIAS normalization layer, the layout of the human-parsing map in the Ô¨Ånal result is preserved.ALIAS Generator.Fig.4 describes the overview of the ALIAS generator, which adopts the simpliÔ¨Åed architecture that discards the encoder part of an encoder-decoder net- work.The generator employs a series of residual blocks (ResBlk) with upsampling layers.Each ALIAS ResBlkconsists of three convolutional layers and three ALIAS nor- malization layers.
Due to the different resolutions that Res- Blks operate at, we resize the inputs of the normalization layers, ^SandMmisalign , before injecting them into each layer. Similarly, the input of the generator, (Ia;P;W(c;)), is resized to different resolutions.Before each ResBlk, the resized inputs (Ia;P;W(c;))are concatenated to the acti- vation of the previous layer after passing through a convolu- tion layer, and each ResBlk utilizes the concatenated inputs to reÔ¨Åne the activation.In this manner, the network per- forms the multi-scale reÔ¨Ånement at a feature level that bet- ter preserves the clothing details than a single reÔ¨Ånement at the pixel level.We train the ALIAS generator with the con- ditional adversarial loss, the feature matching loss, and the perceptual loss following SPADE [20] and pix2pixHD [32].Details of the model architecture, hyperparameters, and the loss function are described in the supplementary.4.Experiments 4.1.Experiment Setup Dataset.
We collected 1024 768 virtual try-on dataset for our research purpose, since the resolution of images on the dataset provided by Han et al. [10] was low.SpeciÔ¨Åcally, we crawled 13,679 frontal-view woman and top clothing image pairs on an online shopping mall website.The pairs are split into a training and a test set with 11,647 and 2,032 pairs, respectively.We use the pairs of a person and a cloth- ing image to evaluate a paired setting, and we shufÔ¨Çe the clothing images for an unpaired setting.The paired setting is to reconstruct the person image with the original clothing item, and the unpaired setting is to change the clothing item on the person image with a different item.Training and Inference.With the goal of reconstruct- ingIfrom (Ia;c), the training of each stage proceeds in- dividually.During the training of the geometric matching module and the ALIAS generator, we use Sinstead of ^S.
While we aim to generate 1024 768 try-on images, we train the segmentation generator and the geometric match- ing module at 256 192. In the inference phase, after be- ing predicted by the segmentation generator at 256 192, the segmentation map is upscaled to 1024 768 and passed Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Segmentation (Ours) Figure 6: Qualitative comparison of the baselines.Target ClothesAgnostic (ACGPN)Output (ACGPN)Agnostic (Ours)Reference ImageOutput (Ours) Figure 7: Qualitative comparison of the segmentation gen- erator of ACGPN and VITON-HD.The clothing-agnostic segmentation map used by each model is also reported.to subsequent stages.Similarly, the geometric matching module predicts the TPS parameters at 256192, and the 1024768 clothing image deformed by the parameters is used in the ALIAS generator.We empirically found that this approach makes these two modules perform better with a lower memory cost than those trained at 1024 768.
De- tails of the model architecture and hyperparameters are de- scribed in the supplementary.4.2. Qualitative Analysis We compare VITON-HD with CP-VTON [31] and ACGPN [35], whose codes are publicly available.Follow- ing the training and inference procedure of our model, seg- mentation generators and geometric matching modules of the baselines are trained at 256 192, and the outputs from the modules are upscaled to 1024 768 during the inference.Comparison with Baselines.Fig.6 demonstrates that VITON-HD generates more perceptually convincing 1024768 images compared to the baselines.Our model clearly preserves the details of the target clothes, such as the logos and the clothing textures, due to the multi-scale reÔ¨Ånement at a feature level.In addition, regardless of what clothes the person is wearing in the reference image, our model synthesizes the body shape naturally.As shown in Fig.
7, the shape of the original clothing item remains in the synthetic segmentation map generated by ACGPN. On the other hand, the segmentation generator in VITON-HD suc- cessfully predicts the segmentation map regardless of the original clothing item, due to our newly proposed clothing- agnostic person representation.Although our model sur- passes the baselines qualitatively, there are a few limitations w/o ALIAS norm with ALIAS norm w/o ALIAS norm with ALIAS normFigure 8: Effects of ALIAS normalization.The orange colored areas in the enlarged images indicate the misaligned regions.0.040.060.080.10.120.140.160.18 Small Medium LargeLPIPS Misaligned AreaCP-VTON ACGPN OursFigure 9: LPIPS scores according to the degree of misalignment.to VITON-HD, which are reported in the supplementary with the additional qualitative results.Effectiveness of the ALIAS Normalization.
We study the effectiveness of ALIAS normalization by comparing our model to VITON-HD*, where the standardization in ALIAS normalization is replaced by channel-wise standard- ization, as in the original instance normalization [30]. Fig.8 shows that ALIAS normalization has the capability to Ô¨Åll the misaligned areas with the target clothing texture by re- moving the misleading information.On the other hand, without utilizing ALIAS normalization, the artifacts are produced in the misaligned areas, because the background information in the warped clothing image is not removed as described in Section 3.4.ALIAS normalization, however, can handle the misaligned regions properly.4.3.Quantitative Analysis We perform the quantitative experiments in both a paired and an unpaired settings, in which a person wears the origi- nal clothes or the new clothes, respectively.We evaluate our method using three metrics widely used in virtual try-on.
The structural similarity (SSIM) [33] and the learned per- ceptual image patch similarity (LPIPS) [38] are used in the paired setting, and the frechet inception distance (FID) [11] score is adopted in the unpaired setting. The inception score [26] is not included in the experiments, since it cannot reÔ¨Çect whether the details of the clothing image are main- tained [10].The input of the each model contains different amount of information that offers advantages in reconstruct- ing the segmentation maps, thus we use the segmentation maps from the test set instead of the synthetic segmentation maps in the paired setting.Comparison across Different Resolutions.We com- pare the baselines quantitatively across different resolutions (256192, 512384, and 1024768) as shown in Table 1.Our model outperforms the baselines for SSIM and LPIPS across all resolutions.For FID score, our model signiÔ¨Å- cantly surpasses CP-VTON, regardless of the resolutions.
The FID score in ACGPN is slightly lower than that of our model at the 256 192 resolution. However, at the1024768 resolution, our model achieves a lower FID score than ACGPN with a large margin.The results indicate that the baselines cannot handle 1024 768 images, while our model is trained in a stable manner, even at a high resolu- tion.This may be due to the limited capability of the U-Net architecture employed in the baseline models.Comparison According to the Degree of Misalign- ment.To verify the ability of Ô¨Ålling the misaligned areas with the clothing texture, we perform experiments in the paired setting according to the degree of the misalignment.According to the number of pixels in the misaligned areas, we divide the test dataset in three types: small, medium, and large.For a fair comparison, each model uses the same seg- mentation maps and the same warped clothes as inputs to match the misaligned regions.
We evaluate LPIPS to mea- sure the semantic distances between the reference images and the reconstructed images. As shown in Fig.9, the wider the misaligned areas, the worse the performance of models, which means that the misalignment hinders the models from generating photo-realistic virtual try-on images.Compared to the baselines, our model consistently performs better, and the performance of our model decreases less as the degree of misalignment increases.5.Conclusions We propose the VITON-HD that synthesizes photo- realistic 1024768 virtual try-on images.The proposed ALIAS normalization can properly handle the misaligned areas and propagate the semantic information throughout the ALIAS generator, which preserves the details of the clothes via the multi-scale reÔ¨Ånement.Qualitative and quantitative experiments demonstrate that VITON-HD sur- passes existing virtual try-on methods with a large margin.Acknowledgments.
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. NRF-2019R1A2C4070420) and Seoul R&BD Program (CD200024) through the Seoul Business Agency (SBA) funded by the Seoul Metropolitan Government.References [1] Ivan Anokhin, Pavel Solovev, Denis Korzhenkov, Alexey Kharlamov, Taras Khakhulin, Aleksei Silvestrov, Sergey Nikolenko, Victor Lempitsky, and Gleb Sterkin.High- resolution daytime translation without domain labels.In Proc.of the IEEE conference on computer vision and pat- tern recognition (CVPR) , pages 7488‚Äì7497, 2020.3 [2] Andrew Brock, Jeff Donahue, and Karen Simonyan.Large scale gan training for high Ô¨Ådelity natural image synthesis.In Proc.the International Conference on Learning Representa- tions (ICLR) , 2018.3 [3] Z Cao, T Simon, SE Wei, YA Sheikh, et al.Openpose: Realtime multi-person 2d pose estimation using part afÔ¨Ån- ity Ô¨Åelds.
The IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2019. 4 [4] Harm De Vries, Florian Strub, J ¬¥er¬¥emie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C Courville.Modu- lating early visual processing by language.In Proc.the Ad- vances in Neural Information Processing Systems (NeurIPS) , pages 6594‚Äì6604, 2017.3 [5] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bochao Wang, Hanjiang Lai, Jia Zhu, Zhiting Hu, and Jian Yin.Towards multi-pose guided virtual try-on network.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 9026‚Äì9035, 2019.3 [6] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bowen Wu, Bing-Cheng Chen, and Jian Yin.Fw-gan: Flow-navigated warping gan for video virtual try-on.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 1161‚Äì1170, 2019.2 [7] Ke Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, Ming Yang, and Liang Lin.Instance-level human parsing via part grouping network.In Proc.
of the European Conference on Computer Vision (ECCV) , pages 770‚Äì785, 2018. 4 [8] Peng Guan, Loretta Reiss, David A Hirshberg, Alexander Weiss, and Michael J Black.Drape: Dressing any person.ACM Transactions on Graphics (TOG) , 31(4):1‚Äì10, 2012.3 [9] Xintong Han, Xiaojun Hu, Weilin Huang, and Matthew R Scott.ClothÔ¨Çow: A Ô¨Çow-based model for clothed person generation.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 10471‚Äì10480, 2019.2, 3, 13 [10] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry S Davis.Viton: An image-based virtual try-on network.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 7543‚Äì7552, 2018.2, 3, 6, 8 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.Gans trained by a two time-scale update rule converge to a local nash equilib- rium.In Proc.the Advances in Neural Information Process- ing Systems (NeurIPS) , pages 6629‚Äì6640, 2017.
8 [12] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 1501‚Äì1510, 2017.3 [13] Sergey Ioffe and Christian Szegedy.Batch normalization: Accelerating deep network training by reducing internal co-variate shift.In Proc.the International Conference on Ma- chine Learning (ICML) , pages 448‚Äì456, 2015.3 [14] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.Image-to-image translation with conditional adversar- ial networks.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 1125‚Äì1134, 2017.3 [15] Nikolay Jetchev and Urs Bergmann.The conditional analogy gan: Swapping fashion articles on people images.In Proc.of the IEEE international conference on computer vision work- shop (ICCVW) , pages 2287‚Äì2292, 2017.3 [16] Diederik P Kingma and Jimmy Ba.Adam: A method for stochastic optimization.arXiv:1412.6980 , 2014.
13 [17] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks.In Proc.of the IEEE international con- ference on computer vision (ICCV) , pages 2794‚Äì2802, 2017.12 [18] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.Spectral normalization for generative ad- versarial networks.In Proc.the International Conference on Learning Representations (ICLR) , 2018.11 [19] Augustus Odena, Christopher Olah, and Jonathon Shlens.Conditional image synthesis with auxiliary classiÔ¨Åer gans.InProc.the International Conference on Machine Learning (ICML) , pages 2642‚Äì2651, 2017.3 [20] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.Semantic image synthesis with spatially-adaptive nor- malization.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 2337‚Äì2346, 2019.3, 6, 12 [21] Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A.
Efros, and Richard Zhang. Swapping autoencoder for deep image manipulation.In Proc.the Ad- vances in Neural Information Processing Systems (NeurIPS) , 2020.3 [22] Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons- Moll.Tailornet: Predicting clothing in 3d as a function of hu- man pose, shape and garment style.In Proc.of the IEEE con- ference on computer vision and pattern recognition (CVPR) , pages 7365‚Äì7375, 2020.3 [23] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black.Clothcap: Seamless 4d clothing capture and retar- geting.ACM Transactions on Graphics (TOG) , 36(4):1‚Äì15, 2017.3 [24] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo- geswaran, Bernt Schiele, and Honglak Lee.Generative adversarial text to image synthesis.In Proc.the Inter- national Conference on Machine Learning (ICML) , pages 1060‚Äì1069, 2016.3 [25] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.U-net: Convolutional networks for biomedical image segmentation.
InInternational Conference on Medical Image Computing and Computer Assisted Intervention , pages 234‚Äì241, 2015. 2, 4, 11 [26] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.Improved techniques for training gans.In Proc.the Advances in Neural Informa- tion Processing Systems (NeurIPS) , pages 2234‚Äì2242, 2016.8 [27] Masahiro Sekine, Kaoru Sugita, Frank Perbet, Bj ¬®orn Stenger, and Masashi Nishiyama.Virtual Ô¨Åtting by single-shot body shape estimation.In International Conference on 3D Body Scanning Technologies , pages 406‚Äì413, 2014.3 [28] Wei Shen and Rujie Liu.Learning residual images for face attribute manipulation.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 4030‚Äì4038, 2017.3 [29] Karen Simonyan and Andrew Zisserman.Very deep convo- lutional networks for large-scale image recognition.arXiv preprint arXiv:1409.1556 , 2014.13 [30] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
In- stance normalization: The missing ingredient for fast styliza- tion. arXiv preprint arXiv:1607.08022 , 2016.3, 6, 8 [31] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang.Toward characteristic- preserving image-based virtual try-on network.In Proc.of the European Conference on Computer Vision (ECCV) , pages 589‚Äì604, 2018.2, 3, 4, 7, 13 [32] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.High-resolution image syn- thesis and semantic manipulation with conditional gans.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 8798‚Äì8807, 2018.2, 3, 6, 11, 12 [33] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si- moncelli.Image quality assessment: from error visibility to structural similarity.IEEE Transactions on Image Process- ing, 13(4):600‚Äì612, 2004.8 [34] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.
Attngan: Fine- grained text to image generation with attentional genera- tive adversarial networks. In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 1316‚Äì1324, 2018.3 [35] Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wang- meng Zuo, and Ping Luo.Towards photo-realistic virtual try-on by adaptively generating-preserving image content.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 7850‚Äì7859, 2020.2, 3, 5, 7, 12, 13 [36] Ruiyun Yu, Xiaoqi Wang, and Xiaohui Xie.Vtnfp: An image-based virtual try-on network with body and cloth- ing feature preservation.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 10511‚Äì10520, 2019.2, 3 [37] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus- tus Odena.Self-attention generative adversarial networks.InProc.the International Conference on Machine Learning (ICML) , pages 7354‚Äì7363, 2019.
13 [38] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric.In Proc.of the IEEE con- ference on computer vision and pattern recognition (CVPR) , 2018.8[39] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka.Sean: Image synthesis with semantic region-adaptive nor- malization.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 5104‚Äì5113, 2020.3 Supplementary Material A.Implementation Details A.1.Pre-processing Details This section introduces the details of generating our clothing-agnostic person representation.To remove the de- pendency on the clothing item originally worn by a person, regions that can provide any original clothing information, such as the arms that hint at the sleeve length, should be eliminated.Therefore, when generating a clothing-agnostic imageIa, we remove the arms from the reference image I.
For the same reason, legs should be removed if the pants are the target clothing items. We mask the regions with a gray color, so that the masked pixels of the normalized im- age would have a value of 0.We add padding to the masks to thoroughly remove these regions, and the width of the padding is empirically determined.A.2.Model Architectures This section introduces the architectures of the seg- mentation generator, the geometric matching module, and ALIAS generator in detail.Segmentation Generator.The segmentation generator has the structure of U-Net [25], which consists of convolu- tional layers, downsampling layers, and upsampling layers.Two multi-scale discriminators [32] are employed for the conditional adversarial loss.The details of the segmenta- tion generator architecture are shown in Fig.10.Geometric Matching Module.The geometric matching module consists of two feature extractors and a regression network.
A correlation matrix is calculated from the two extracted features, and the regression network predicts the TPS parameter with the correlation matrix. The feature extractor is composed of a series of convolutional layers, and the regression network consists of a series of convolu- tional layers followed by a fully connected layer.The de- tails are shown in Fig.11.ALIAS Generator.The architecture of the ALIAS gen- erator consists of a series of ALIAS ResBlks with nearest- neighbor upsampling layers.We employ two multi-scale discriminators with instance normalization.Spectral nor- malization [18] is applied to all the convolutional layers.Note that we separately standardize the activation based on the misalignment mask Mmisalign only in the Ô¨Årst Ô¨ÅveALIAS ResBlks.The details of the ALIAS generator archi- tecture is shown in Fig.12.A.3.
Training Details This section introduces the losses and the hyperparame- ters for the segmentation generator, the geometric matching module, and the ALIAS generator. Segmentation Generator.The segmentation genera- torGSuses the clothing-agnostic segmentation map Sa, the pose map P, and the clothing item cas inputs ( ^S= GS(Sa;P;c)) to predict the segmentation map ^Sof the person in the reference image wearing the target cloth- ing item.The segmentation generator is trained with the ConvBlk (64), MaxPool (2) ConvBlk (128), MaxPool (2) ConvBlk (256), MaxPool (2) ConvBlk (512), MaxPool (2) ConvBlk (1024) Upsample (2), ConvBlk (512) Upsample (2), ConvBlk (256) Upsample (2), ConvBlk (128) Upsample (2), ConvBlk (64) 3x3 Conv (13), Softmax Figure 10: Segmentation Generator.kkConv (x) denotes a convolutional layer where the kernel size is kand the out- put channel is x.
Also, ConvBlk ( x) denotes a block, which consists of two series of 3 3 convolutional layer, instance normalization, and ReLU activation. 4x4-‚Üì2 Conv (64), BN, ReLU 4x4-‚Üì2 Conv (64), BN, ReLU 4x4-‚Üì2 Conv (128), BN, ReLU 4x4-‚Üì2 Conv (128), BN, ReLU 4x4-‚Üì2 Conv (256), BN, ReLU 4x4-‚Üì2 Conv (256), BN, ReLU 4x4-‚Üì2 Conv (512), BN, ReLU 4x4-‚Üì2 Conv (512), BN, ReLU 3x3 Conv (512), BN, ReLU 3x3 Conv (512), BN, ReLU 4x4-‚Üì2 Conv (512), BN, ReLU 4x4-‚Üì2 Conv (256), BN, ReLU 3x3 Conv (128), BN, ReLU 3x3 Conv (64), BN, ReLU Linear (2x5x5), Tanh ùúÉ3x3 Conv (512), ReLU 3x3 Conv (512), ReLUFigure 11: Geometric Matching Module.kk#2 Conv (x) denotes a convolutional layer where the kernel size is k, the stride is 2, and the output channel is x.cross-entropy lossLCEand the conditional adversarial loss LcGAN , which is LSGAN loss [17].
The full loss LSfor the segmentation generator are written as LS=LcGAN +CELCE (8) LCE= 1 HWX k2C;y2H;x2WSk;y;xlog(^Sk;y;x) (9) LcGAN =E(X;S)[log(D(X;S))] +EX[1 log(D(X;^S))];(10) whereCEis the hyperparameter for the cross-entropy loss. In the experiment, CEis set to 10.In Eq.(9), Syxkand ^Syxkindicate the pixel values of the segmentation map of the reference image Sand^Scorresponding to the coordi- nates (x;y)in channelk.The symbols H,WandCindi- cate the height, width, and the number of channels of S.In Eq.(10), the symbol Xindicates the inputs of the generator (Sa;P;c ), andDdenotes the discriminator.The learning rate of the generator and the discriminator is 0.0004.We adopt the Adam optimizer with 1= 0:5 and 2= 0:999.We train the segmentation generator for 200,000 iterations with the batch size of 8.Geometric Matching Module.The inputs of the geo- metric matching module are c,P, clothing-agnostic image Ia, and ^Sc, which is the clothing area of ^S.
The output is 3x3 Conv (1024) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (512), Upsample (2) ALIAS ResBlk (256), Upsample (2) ALIAS ResBlk (128), Upsample (2) ALIAS ResBlk (64), Upsample (2) ALIAS ResBlk (32) 3x3 Conv (3), Tanh Figure 12: ALIAS Generator. The segmentation map Sand the misalignment mask Mmisalign are passed to the gener- ator through the proposed ALIAS ResBlks.the TPS transformation parameters .The overall objective function is written as Lwarp =jjIc W(c;)jj1;1+constLconst (11) Lconst =X p2Pj(jjjpp0jj2 jjpp1jj2j+jjjpp2jj2 jjpp3jj2j) +(jS(p;p0) S(p;p1)j+jS(p;p2) S(p;p3)j); (12) whereWis the function that deforms cusing, andIcis the clothing item extracted from the reference image I.Lconst is a second-order difference constraint [35], and const is the hyperparameter for Lconst .In the experiment, we set const to 0.04.In Eq.
(12), the symbol pindicates a sam- pled TPS control point from the entire control points set P, andp0,p1,p2, andp3are top, bottom, left and right point ofp, respectively. The function S(p;pi)denotes the slope betweenpandpi.The learning rate of the geometric matching module is 0.0002.We adopt the Adam optimizer with 1= 0:5and 2= 0:999.We train the geometric matching module for 50,000 iterations with the batch size of 8.ALIAS Generator.The loss function of ALIAS genera- tor follows those of SPADE [20] and pix2pixHD [32], as it Grid 5x5 Grid 10x10 Grid 20x20 ClothFlow Figure 13: Qualitative comparisons of TPS transformation with various grid numbers and the Ô¨Çow estimation from ClothFlow.Method Warp-SSIM"MACs#Mask-SSIM" ClothFlow 0.841?8.13G 0.803?VITON-HD 0.782 4.47G 0.852 Table 2:?denotes a score taken from the ClothFlow paper, and we train VITON-HD in the same setting ( e.g., dataset and resolution).We compute MACs of their warping mod- ules at 256192.
contains the conditional adversarial loss LcGAN , the feature matching lossLFM, and the perceptual loss Lpercept . Let DIbe the discriminator, Iandcbe the given reference and target clothing images, and ^Ibe the synthetic image gener- ated by the generator.Sdivis the modiÔ¨Åed version of the segmentation map S.The full lossLIof our generator is written as LI=LcGAN +FMLFM+perceptLpercept (13) LcGAN =EI[log(DI(Sdiv;I))] +E(I;c)[1 log(DI(Sdiv;^I))](14) LFM=E(I;c)TX i=11 Ki[jjD(i) I(Sdiv;I) D(i) I(Sdiv;^I)jj1;1] (15) Lpercept =E(I;c)VX i=11 Ri[jjF(i)(I) F(i)(^I)jj1;1];(16) whereFMandpercept are hyperparameters.In the ex- periment, both FMandpercept are set to 10.Tis the number of layers in DI, andD(i) IandKiare the activa- tion and the number of elements in the i-th layer of DI, respectively.Similarly, Vis the number of layers used in the VGG network F[29], andF(i)andRiare the acti- vation and the number of elements in the i-th layer of F, respectively.
We replace the standard adversarial loss with the Hinge loss [37]. The learning rate of the generator and the discriminator is 0.0001 and 0.0004, respectively.We adopt the Adam op- timizer [16] with 1= 0and 2= 0:9.We train the ALIAS generator for 200,000 iterations with the batch size of 4.B.Additional Experiments B.1.Comparison with ClothFlow To demonstrate that the optical Ô¨Çow estimation does not solve the misalignment completely, we re-implement the Ô¨Çow estimation module of ClothFlow [9] based on the orig- inal paper.Fig.13 shows that the misalignment still oc- curs, although both TPS with a higher grid number (e.g., a 1010 or 2020 grid) and the Ô¨Çow estimation module of ClothFlow can reduce the misaligned regions.The reason is that the regularization to avoid the artifacts ( e.g., TV loss) prevents the warped clothes from Ô¨Åtting perfectly into the target region.
In addition, we evaluate the accuracy and the computational cost of warping modules in VITON-HD and ClothFlow with Warp-SSIM [9] and MACs, respectively. We also measure how well the models reconstruct the cloth- ing using Mask-SSIM [9].Table 2 shows that the ClothFlow warping module has the better accuracy than ours, whereas the higher Mask-SSIM in VITON-HD proves that ALIAS normalization is more effective at solving the misalignment problem than the improved warping method.We found that the ClothFlow warping module needs a huge computational cost (MACs: 130.03G) at 1024 768, but the cost could be reduced when predicting the optical Ô¨Çow map at 256 192.Table 2 demonstrates that the ClothFlow warping module still needs more computational cost than ours, yet it is a viable option to combine the Ô¨Çow estimation module with ALIAS generator.
0.960.010.03 0.020.240.74 0.020.750.23 0% 20% 40% 60% 80% 100%OursACGPNCP-VTONQuality and Realism (%) Top1 Top2 Top3 0.880.030.09 0.10.280.62 0.020.690.29 0% 20% 40% 60% 80% 100%OursACGPNCP-VTONPreservation of Clothing Details (%) Top1 Top2 Top3 Figure 14: User study results. We compare our model with CP-VTON [31] and ACGPN [35].Figure 15: Failure cases of VITON-HD.B.2.User Study We further evaluate our model and other baselines via a user study in the unpaired setting.We randomly select 30 sets of a reference image and a target clothing image from the test dataset.Given the reference images and the target clothes, the users are asked to rank the 1024 768 outputs of our model and baselines according to the follow- ing questions: (1) Which image is the most photo-realistic?(2) Which image preserves the details of the target cloth- ing the most?As shown in Fig.14, it can be observed that our approach achieves the rank 1 votes more than 88% for the both questions.
The result demonstrates that our model generates more realistic images, and preserves the details of the clothing items compared to the baselines. B.3.Qualitative Results We provide additional qualitative results to demonstrate our model‚Äôs capability of handling high quality image syn- thesis.Fig.16, 17, 18, and 19 show the qualitative com- parison of the baselines across different resolutions.Fig.20, 21, 22, and 23 show additional results of VITON-HD at 1024768 resolution.C.Failure Cases and Limitations Fig.15 shows the failure cases of our model caused by the inaccurately predicted segmentation map or the inner collar region indistinguishable from the other clothing re- gion.Also, the boundaries of the clothing textures occa- sionally fade away.The limitations of our model are as follows.VITON-HD is trained to preserve the bottom clothing items, limiting the presentation of the target clothes ( e.g., whether they are tucked in).
It can be a valuable future direction to gener- ate multiple possible outputs from a single input pair. Next, our dataset mostly consists of slim women and top cloth- ing images, which makes VITON-HD handle only a limited range of body shapes and clothing during the inference.We believe that VITON-HD has the capability to cover more diverse cases when the images of various body shapes and clothing types are provided.Finally, existing virtual try-on methods including VITON-HD do not provide robust per- formance for in-the-wild images.We think generating real- istic try-on images for the in-the-wild images is an interest- ing topic for future work.Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Figure 16: Qualitative comparison of the baselines (256 192).Reference Image ACGPN VITON -HD (Ours) Target Clothes CP-VTON Figure 17: Qualitative comparison of the baselines (512 384).
Figure 18: Qualitative comparison of the baselines (1024 768). Figure 19: Qualitative comparison of the baselines (1024 768).Reference ImageTarget ClothesFigure 20: Additional qualitative results of VITON-HD.Figure 21: Sample 1 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.Figure 22: Sample 2 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.Figure 23: Sample 3 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.
Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.edu≈Åukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been Ô¨Årmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the Ô¨Årst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efÔ¨Åcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiÔ¨Åcant improvements in computational efÔ¨Åciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiÔ¨Åcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difÔ¨Åcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the Ô¨Årst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The Ô¨Årst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efÔ¨Åcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneÔ¨Åcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the Ô¨Ånal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information Ô¨Çow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and Ô¨Åxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any Ô¨Åxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneÔ¨Åt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiÔ¨Åcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the Ô¨Årst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conÔ¨Åguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of Ô¨Çoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision Ô¨Çoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneÔ¨Åcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-Ô¨Åtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the Ô¨Årst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiÔ¨Åcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efÔ¨Åciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770‚Äì778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber.Gradient Ô¨Çow in recurrent nets: the difÔ¨Åculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and J√ºrgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735‚Äì1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] ≈Åukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio ≈Åukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] OÔ¨År Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overÔ¨Åtting.Journal of Machine Learning Research , 15(1):1929‚Äì1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D. D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440‚Äì2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104‚Äì3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google‚Äôs neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11
ÔªøInstruction,Input,Response Í≥†Í∞ùÏù¥ ÏöîÍ∏àÏ†ú Î≥ÄÍ≤ΩÏùÑ ÏöîÏ≤≠Ìï®,ÌòÑÏû¨ 5G ÏöîÍ∏àÏ†úÎ•º ÏÇ¨Ïö© Ï§ëÏù∏Îç∞ LTE ÏöîÍ∏àÏ†úÎ°ú Î≥ÄÍ≤ΩÌïòÍ≥† Ïã∂Ïñ¥Ïöî.,"Í≥†Í∞ùÎãò, ÌòÑÏû¨ ÏÇ¨Ïö© Ï§ëÏù∏ 5G ÏöîÍ∏àÏ†úÎ•º LTE ÏöîÍ∏àÏ†úÎ°ú Î≥ÄÍ≤ΩÌïòÏãúÎ†§Î©¥, Í∞ÄÍπåÏö¥ SKT ÎåÄÎ¶¨Ï†êÏùÑ Î∞©Î¨∏ÌïòÏãúÍ±∞ÎÇò TÏõîÎìú Ïï±ÏóêÏÑú Î≥ÄÍ≤Ω Í∞ÄÎä•Ìï©ÎãàÎã§.ÏõêÌïòÏãúÎäî ÏöîÍ∏àÏ†úÎ•º ÎßêÏîÄÌï¥ Ï£ºÏãúÎ©¥ ÏûêÏÑ∏Ìûà ÏïàÎÇ¥Ìï¥ ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©ÎüâÏùÑ Ï°∞ÌöåÌïòÍ≥† Ïã∂Ïñ¥Ìï®,Ïù¥Î≤à Îã¨ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©ÎüâÏù¥ ÏñºÎßàÎÇò ÎêòÎÇòÏöî?,"Í≥†Í∞ùÎãò, ÌòÑÏû¨ÍπåÏßÄ ÏÇ¨Ïö©ÌïòÏã† Îç∞Ïù¥ÌÑ∞Îäî 8.5GBÏù¥Î©∞, ÎÇ®ÏùÄ Îç∞Ïù¥ÌÑ∞Îäî 1.5GBÏûÖÎãàÎã§.Ï∂îÍ∞ÄÎ°ú Îç∞Ïù¥ÌÑ∞Í∞Ä ÌïÑÏöîÌïòÏãúÎ©¥ Îç∞Ïù¥ÌÑ∞ Ïø†Ìè∞ÏùÑ Íµ¨Îß§ÌïòÏã§ ÏàòÎèÑ ÏûàÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Ìï¥Ïô∏ Î°úÎ∞ç ÏöîÍ∏àÏ†úÎ•º Î¨∏ÏùòÌï®,"Îã§Ïùå Ï£ºÏóê ÏùºÎ≥∏ÏúºÎ°ú Ïó¨Ìñâ Í∞ÄÎäîÎç∞, Î°úÎ∞ç ÏöîÍ∏àÏ†ú Ïñ¥ÎñªÍ≤å ÎêòÎÇòÏöî?","Í≥†Í∞ùÎãò, ÏùºÎ≥∏ÏóêÏÑú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î°úÎ∞ç ÏöîÍ∏àÏ†úÎäî 'Î∞îÎ°ú Î°úÎ∞ç 300MB(9,900Ïõê/Ïùº)' ÎòêÎäî 'Î°úÎ∞ç Î¨¥Ï†úÌïú(13,900Ïõê/Ïùº)' Îì±Ïù¥ ÏûàÏäµÎãàÎã§.Îçî ÏûêÏÑ∏Ìïú ÏÇ¨Ìï≠ÏùÄ TÏõîÎìúÏóêÏÑú ÌôïÏù∏ Í∞ÄÎä•Ìï©ÎãàÎã§." Í≥†Í∞ùÏù¥ Í≤∞Ï†úÏùº Î≥ÄÍ≤ΩÏùÑ ÏöîÏ≤≠Ìï®,Îß§Ïõî ÏöîÍ∏à Í≤∞Ï†úÏùºÏùÑ Î∞îÍæ∏Í≥† Ïã∂ÏùÄÎç∞ Í∞ÄÎä•ÌïúÍ∞ÄÏöî?,"ÎÑ§, Í≥†Í∞ùÎãò.ÏöîÍ∏à Í≤∞Ï†úÏùº Î≥ÄÍ≤ΩÏùÄ Îß§Ïõî 1Ïùº, 15Ïùº, 25Ïùº Ï§ë ÏÑ†ÌÉù Í∞ÄÎä•Ìï©ÎãàÎã§.ÏõêÌïòÏãúÎäî ÎÇ†ÏßúÎ•º ÎßêÏîÄÌï¥ Ï£ºÏãúÎ©¥ Î≥ÄÍ≤Ω ÎèÑÏôÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Í∏∞Í∏∞ Ìï†Î∂ÄÍ∏à ÏûîÏï°ÏùÑ ÌôïÏù∏ÌïòÎ†§ Ìï®,ÌòÑÏû¨ ÏÇ¨Ïö© Ï§ëÏù∏ Ìú¥ÎåÄÌè∞ Ìï†Î∂ÄÍ∏àÏù¥ ÏñºÎßàÎÇò ÎÇ®ÏïòÎÇòÏöî?,"Í≥†Í∞ùÎãò, ÌòÑÏû¨ Îã®ÎßêÍ∏∞ Ìï†Î∂ÄÍ∏àÏùÄ 12Í∞úÏõî ÎÇ®ÏïÑ ÏûàÏúºÎ©∞, ÎÇ®ÏùÄ Í∏àÏï°ÏùÄ Ï¥ù 360,000ÏõêÏûÖÎãàÎã§.Ï°∞Í∏∞ ÏôÑÎÇ©ÏùÑ ÏõêÌïòÏãúÎ©¥ TÏõîÎìúÏóêÏÑú ÎÇ©Î∂Ä Í∞ÄÎä•Ìï©ÎãàÎã§." Í≥†Í∞ùÏù¥ Í∞ÄÏ°± Í≤∞Ìï© Ìï†Ïù∏ ÌòúÌÉùÏùÑ Î¨∏ÏùòÌï®,"Í∞ÄÏ°± Í≤∞Ìï© Ìï†Ïù∏ÏùÑ Î∞õÍ≥† Ïã∂ÏùÄÎç∞, Ïñ¥ÎñªÍ≤å ÌïòÎ©¥ ÎêòÎÇòÏöî?","Í≥†Í∞ùÎãò, Í∞ÄÏ°± Í≤∞Ìï© Ìï†Ïù∏ÏùÄ Í∞ÄÏ°± Íµ¨ÏÑ±Ïõê 2Ïù∏ Ïù¥ÏÉÅÏù¥ SKT ÌöåÏÑ†ÏùÑ ÏÇ¨Ïö© Ï§ëÏùº Í≤ΩÏö∞ Í∞ÄÎä•Ìï©ÎãàÎã§.Í∞ÄÏ°±Í¥ÄÍ≥Ñ Ï¶ùÎπôÏÑúÎ•òÎ•º Ï§ÄÎπÑÌïòÏÖîÏÑú Í∞ÄÍπåÏö¥ ÎåÄÎ¶¨Ï†êÏùÑ Î∞©Î¨∏Ìï¥ Ï£ºÏãúÎ©¥ Ïã†Ï≤≠ ÎèÑÏôÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Í∞úÌÜµ Í∞ÄÎä•Ìïú Ïä§ÎßàÌä∏Ìè∞ Í∏∞Ï¢ÖÏùÑ Î¨∏ÏùòÌï®,ÏïÑÏù¥Ìè∞ 15 Í∞úÌÜµ Í∞ÄÎä•ÌïúÍ∞ÄÏöî?,"Í≥†Í∞ùÎãò, ÌòÑÏû¨ ÏïÑÏù¥Ìè∞ 15Îäî Ï†ïÏÉÅ Í∞úÌÜµ Í∞ÄÎä•ÌïòÎ©∞, ÏÉâÏÉÅ Î∞è Ï†ÄÏû• Ïö©ÎüâÏóê Îî∞Îùº Ïû¨Í≥†Í∞Ä Îã§Î•º Ïàò ÏûàÏäµÎãàÎã§.
Í∞ÄÍπåÏö¥ SKT ÎåÄÎ¶¨Ï†êÏóê Î∞©Î¨∏ÌïòÏãúÎ©¥ Ï¶âÏãú Í∞úÌÜµ Í∞ÄÎä•Ìï©ÎãàÎã§." Í≥†Í∞ùÏù¥ Îç∞Ïù¥ÌÑ∞ Ïø†Ìè∞ Íµ¨Îß§ Î∞©Î≤ïÏùÑ Î¨∏ÏùòÌï®,Îç∞Ïù¥ÌÑ∞ 1GB Ï∂îÍ∞ÄÌïòÎ†§Î©¥ Ïñ¥ÎñªÍ≤å Ìï¥Ïïº ÌïòÎÇòÏöî?,"Í≥†Í∞ùÎãò, Îç∞Ïù¥ÌÑ∞ 1GB Ï∂îÍ∞ÄÎäî TÏõîÎìú Ïï± ÎòêÎäî SKT Í≥†Í∞ùÏÑºÌÑ∞(114)ÏóêÏÑú Íµ¨Îß§ Í∞ÄÎä•Ìï©ÎãàÎã§. 1GB Îç∞Ïù¥ÌÑ∞ Ïø†Ìè∞ÏùÄ 5,500ÏõêÏóê Ï†úÍ≥µÎêòÎ©∞, Ï¶âÏãú Ï†ÅÏö©Îê©ÎãàÎã§." Í≥†Í∞ùÏù¥ 5G ÏÜçÎèÑ Ï†ÄÌïò Î¨∏Ï†úÎ•º Ï†úÍ∏∞Ìï®,5GÏù∏Îç∞ ÏÜçÎèÑÍ∞Ä ÎÑàÎ¨¥ ÎäêÎ†§Ïöî.Ïôú Í∑∏Îü∞Í∞ÄÏöî?,"Í≥†Í∞ùÎãò, ÌòÑÏû¨ Í≥ÑÏã† ÏßÄÏó≠Ïùò ÎÑ§Ìä∏ÏõåÌÅ¨ ÌôòÍ≤ΩÏóê Îî∞Îùº ÏÜçÎèÑÍ∞Ä Ï†ÄÌïòÎê† Ïàò ÏûàÏäµÎãàÎã§.Ïû†Ïãú ÌõÑÏóêÎèÑ ÎèôÏùºÌïú Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌïòÎ©¥ SKT Í≥†Í∞ùÏÑºÌÑ∞Î°ú Î¨∏Ïùò Ï£ºÏãúÎ©¥ Ïã†ÏÜçÌûà Ï†êÍ≤Ä ÎèÑÏôÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Î≤àÌò∏ Î≥ÄÍ≤ΩÏùÑ ÏöîÏ≤≠Ìï®,ÌòÑÏû¨ Ï†ÑÌôîÎ≤àÌò∏Î•º Î∞îÍæ∏Í≥† Ïã∂ÏùÄÎç∞ Ïñ¥ÎñªÍ≤å ÌïòÎ©¥ ÎêòÎÇòÏöî?,"Í≥†Í∞ùÎãò, Î≤àÌò∏ Î≥ÄÍ≤ΩÏùÄ SKT ÎåÄÎ¶¨Ï†ê Î∞©Î¨∏ ÎòêÎäî Í≥†Í∞ùÏÑºÌÑ∞Î•º ÌÜµÌï¥ Í∞ÄÎä•Ìï©ÎãàÎã§.Î≤àÌò∏ Î≥ÄÍ≤Ω Ïãú Í∏∞Ï°¥ Î≤àÌò∏Î°ú Í±∏Î†§Ïò§Îäî Ï†ÑÌôîÎäî ÏùºÏ†ï Í∏∞Í∞Ñ ÏïàÎÇ¥ Î©îÏãúÏßÄÍ∞Ä Ï†úÍ≥µÎê©ÎãàÎã§." Í≥†Í∞ùÏù¥ 2G ÏÑúÎπÑÏä§ Ï¢ÖÎ£åÏóê ÎåÄÌï¥ Î¨∏ÏùòÌï®,"2G ÏÑúÎπÑÏä§Í∞Ä Ï¢ÖÎ£åÎêúÎã§Í≥† ÌïòÎçòÎç∞, Ïñ¥ÎñªÍ≤å Ìï¥Ïïº ÌïòÎÇòÏöî?","Í≥†Í∞ùÎãò, SKT 2G ÏÑúÎπÑÏä§Îäî Ï¢ÖÎ£åÎêòÏóàÏúºÎ©∞, 3G, LTE, 5G Í∏∞Í∏∞Î°ú Í∏∞Í∏∞ Î≥ÄÍ≤ΩÏùÑ Í∂åÏû•ÎìúÎ¶ΩÎãàÎã§.Î≥¥ÏÉÅ ÌîÑÎ°úÍ∑∏Îû®ÎèÑ ÏßÑÌñâ Ï§ëÏù¥Îãà ÏûêÏÑ∏Ìïú ÏÇ¨Ìï≠ÏùÄ Í≥†Í∞ùÏÑºÌÑ∞Î°ú Î¨∏Ïùò Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§." Í≥†Í∞ùÏù¥ Î∂ÑÏã§Ìè∞ ÏúÑÏπò Ï∂îÏ†ÅÏùÑ ÏöîÏ≤≠Ìï®,Ìï∏ÎìúÌè∞ÏùÑ ÏûÉÏñ¥Î≤ÑÎ†∏ÎäîÎç∞ ÏúÑÏπò Ï∂îÏ†ÅÏù¥ Í∞ÄÎä•Ìï†ÍπåÏöî?,"Í≥†Í∞ùÎãò, 'ÎÇ¥ ÎîîÎ∞îÏù¥Ïä§ Ï∞æÍ∏∞' Í∏∞Îä•ÏùÑ Ïù¥Ïö©ÌïòÏãúÎ©¥ ÏúÑÏπò Ï∂îÏ†ÅÏù¥ Í∞ÄÎä•Ìï©ÎãàÎã§.Ï∂îÍ∞ÄÎ°ú Í≥†Í∞ùÏÑºÌÑ∞Î•º ÌÜµÌï¥ Î∂ÑÏã§ Ïã†Í≥†Î•º ÌïòÏãúÎ©¥ ÏûÑÏãú Ï†ïÏßÄ Î∞è Î≥¥Ìò∏ Ï°∞ÏπòÎ•º ÎèÑÏôÄÎìúÎ¶¥ Ïàò ÏûàÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Î©§Î≤ÑÏã≠ ÌòúÌÉùÏùÑ Î¨∏ÏùòÌï®,SKT VIP Î©§Î≤ÑÏã≠ ÌòúÌÉùÏóêÎäî Ïñ¥Îñ§ Í≤ÉÏù¥ ÏûàÎÇòÏöî?,"Í≥†Í∞ùÎãò, SKT VIP Î©§Î≤ÑÏã≠ ÌòúÌÉùÏúºÎ°ú ÏòÅÌôî Ìï†Ïù∏, Ïπ¥Ìéò Î¨¥Î£å ÏùåÎ£å, Îç∞Ïù¥ÌÑ∞ Î¶¨ÌïÑ Ïø†Ìè∞ Ï†úÍ≥µ Îì±Ïù¥ ÏûàÏäµÎãàÎã§.ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ TÏõîÎìú Ïï±ÏóêÏÑú ÌôïÏù∏ Í∞ÄÎä•Ìï©ÎãàÎã§."
Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.edu≈Åukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been Ô¨Årmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the Ô¨Årst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efÔ¨Åcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiÔ¨Åcant improvements in computational efÔ¨Åciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiÔ¨Åcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difÔ¨Åcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the Ô¨Årst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The Ô¨Årst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efÔ¨Åcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneÔ¨Åcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the Ô¨Ånal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information Ô¨Çow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and Ô¨Åxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any Ô¨Åxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneÔ¨Åt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiÔ¨Åcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the Ô¨Årst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conÔ¨Åguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of Ô¨Çoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision Ô¨Çoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneÔ¨Åcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-Ô¨Åtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the Ô¨Årst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiÔ¨Åcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efÔ¨Åciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770‚Äì778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber.Gradient Ô¨Çow in recurrent nets: the difÔ¨Åculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and J√ºrgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735‚Äì1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] ≈Åukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio ≈Åukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] OÔ¨År Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overÔ¨Åtting.Journal of Machine Learning Research , 15(1):1929‚Äì1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D. D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440‚Äì2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104‚Äì3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google‚Äôs neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11
Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.edu≈Åukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been Ô¨Årmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the Ô¨Årst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efÔ¨Åcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiÔ¨Åcant improvements in computational efÔ¨Åciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiÔ¨Åcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difÔ¨Åcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the Ô¨Årst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The Ô¨Årst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efÔ¨Åcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneÔ¨Åcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the Ô¨Ånal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information Ô¨Çow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and Ô¨Åxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any Ô¨Åxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneÔ¨Åt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiÔ¨Åcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the Ô¨Årst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conÔ¨Åguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of Ô¨Çoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision Ô¨Çoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneÔ¨Åcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-Ô¨Åtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the Ô¨Årst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiÔ¨Åcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efÔ¨Åciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770‚Äì778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber.Gradient Ô¨Çow in recurrent nets: the difÔ¨Åculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and J√ºrgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735‚Äì1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] ≈Åukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio ≈Åukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] OÔ¨År Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overÔ¨Åtting.Journal of Machine Learning Research , 15(1):1929‚Äì1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D. D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440‚Äì2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104‚Äì3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google‚Äôs neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11
VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization Seunghwan Choi*Sunghyun Park*Minsoo Lee*Jaegul Choo KAIST, Daejeon, South Korea fshadow2496, psh01087, alstn2022, jchoo g@kaist.ac.kr Reference Image Synthetic Image Figure 1: Given a pair of a reference image (containing a person) and a target clothing image, our method successfully synthesizes 1024768 virtual try-on images.Abstract The task of image-based virtual try-on aims to transfer a target clothing item onto the corresponding region of a person, which is commonly tackled by Ô¨Åtting the item to the desired body part and fusing the warped item with the person.While an increasing number of studies have been conducted, the resolution of synthesized images is still lim- ited to low ( e.g., 256192), which acts as the critical lim- itation against satisfying online consumers.
We argue that the limitation stems from several challenges: as the resolu- tion increases, the artifacts in the misaligned areas between the warped clothes and the desired clothing regions become noticeable in the Ô¨Ånal results; the architectures used in ex- *These authors contributed equally.isting methods have low performance in generating high- quality body parts and maintaining the texture sharpness of the clothes. To address the challenges, we propose a novel virtual try-on method called VITON-HD that successfully synthesizes 1024768 virtual try-on images.SpeciÔ¨Åcally, we Ô¨Årst prepare the segmentation map to guide our virtual try-on synthesis, and then roughly Ô¨Åt the target clothing item to a given person‚Äôs body.Next, we propose ALIgnment- Aware Segment (ALIAS) normalization and ALIAS genera- tor to handle the misaligned areas and preserve the details of 1024768 inputs.
Through rigorous comparison with ex- isting methods, we demonstrate that VITON-HD highly sur- passes the baselines in terms of synthesized image quality both qualitatively and quantitatively. Code is available at https://github.com/shadow2496/VITON-HD .arXiv:2103.16874v2 [cs.CV] 10 Sep 2021 1.Introduction Image-based virtual try-on refers to the image generation task of changing the clothing item on a person into a differ- ent item, given in a separate product image.With a growing trend toward online shopping, virtually wearing the clothes can enrich a customer‚Äôs experience, as it gives an idea about how these items would look on them.Virtual try-on is similar to image synthesis, but it has unique and challenging aspects.Given images of a person and a clothing product, the synthetic image should meet the following criteria: (1) The person‚Äôs pose, body shape, and identity should be preserved.
(2) The clothing prod- uct should be naturally deformed to the desired clothing re- gion of the given person, by reÔ¨Çecting his/her pose and body shape. (3) Details of the clothing product should be kept in- tact.(4) The body parts initially occluded by the person‚Äôs clothes in the original image should be properly rendered.Since the given clothing image is not initially Ô¨Åtted to the person image, fulÔ¨Ålling these requirements is challenging, which leaves the development of virtual try-on still far be- hind the expectations of online consumers.In particular, the resolution of virtual try-on images is low compared to the one of normal pictures on online shopping websites.After Han et al.[10] proposed VITON, various image- based virtual try-on methods have been proposed [31, 36, 35, 6].
These methods follow two processes in common: (1) warping the clothing image initially to Ô¨Åt the human body; (2) fusing the warped clothing image and the image of the person that includes pixel-level reÔ¨Ånement. Also, several recent methods [9, 36, 35] add a module that generates seg- mentation maps and determine the person‚Äôs layout from the Ô¨Ånal image in advance.However, the resolution of the synthetic images from the previous methods is low ( e.g., 256192) due to the follow- ing reasons.First, the misalignment between the warped clothes and a person‚Äôs body results in the artifacts in the misaligned regions, which become noticeable as the image size increases.It is difÔ¨Åcult to warp clothing images to Ô¨Åt the body perfectly, so the misalignment occurs as shown in Fig.2.Most of previous approaches utilize the thin-plate spline (TPS) transformation to deform clothing images.
To accurately deform clothes, ClothFlow [9] predicts the op- tical Ô¨Çow maps of the clothes and the desired clothing re- gions. However, the optical Ô¨Çow maps does not remove the misalignment completely on account of the regulariza- tion.In addition, the process requires more computational costs than other methods due to the need of predicting the movement of clothes at a pixel level.(The detailed analysis of ClothFlow is included in the supplementary.) Second, a simple U-Net architecture [25] used in existing approaches is insufÔ¨Åcient in synthesizing initially occluded body parts in Ô¨Ånal high-resolution ( e.g., 1024768) images.As noted in Wang et al.[32], applying a simple U-Net-based archi- (c)Misaligned Regions (b)Warped Clothes on Segmentation(a)Warped Clothes on Reference ImageFigure 2: An example of misaligned regions.tecture to generate high-resolution images leads to unstable training as well as unsatisfactory quality of generated im- ages.
Also, reÔ¨Åning the images once at the pixel level is in- sufÔ¨Åcient in preserving the details of high-resolution cloth- ing images. To address the above-mentioned challenges, we pro- pose a novel high-resolution virtual try-on method, called VITON-HD.In particular, we introduce a new clothing- agnostic person representation that leverages the pose in- formation and the segmentation map so that the clothing information is eliminated thoroughly.Afterwards, we feed the segmentation map and the clothing item deformed to Ô¨Åt the given human body to the model.Using the addi- tional information, our novel ALIgnment-Aware Segment (ALIAS) normalization removes information irrelevant to the clothing texture in the misaligned regions and propa- gates the semantic information throughout the network.The normalization separately standardizes the activations corre- sponding to the misaligned regions and the other regions, and modulates the standardized activations using the seg- mentation map.
Our ALIAS generator employing ALIAS normalization synthesizes the person image wearing the tar- get product while Ô¨Ålling the misaligned regions with the clothing texture and preserving the details of the clothing item through the multi-scale reÔ¨Ånement at a feature level. To validate the performance of our framework, we col- lected a 1024768 dataset that consists of pairs of a per- son and a clothing item for our research purpose.Our ex- periments demonstrate that VITON-HD signiÔ¨Åcantly out- performs the existing methods in generating 1024 768 im- ages, both quantitatively and qualitatively.We also conÔ¨Årm the superior capability of our novel ALIAS normalization module in dealing with the misaligned regions.We summarize our contributions as follows: ‚Ä¢ We propose a novel image-based virtual try-on ap- proach called VITON-HD, which is, to the best of our knowledge, the Ô¨Årst model to successfully synthesize 1024768 images.
‚Ä¢ We introduce a clothing-agnostic person representa- tion that allows our model to remove the dependency on the clothing item originally worn by the person. ‚Ä¢ To address the misalignment between the warped clothes and the desired clothing regions, we propose ALIAS normalization and ALIAS generator, which is effective in maintaining the details of clothes.‚Ä¢ We demonstrate the superior performance of our method through experiments with baselines on the newly collected dataset.2.Related Work Conditional Image Synthesis.Conditional generative adversarial networks (cGANs) utilize additional informa- tion, such as class labels [19, 2], text [24, 34], and at- tributes [28], to steer the image generation process.Since the emergence of pix2pix [14], numerous cGANs condi- tioned on input images have been proposed to generate high-resolution images in a stable manner [32, 1, 21].
How- ever, these methods tend to generate blurry images when handling a large spatial deformation between the input im- age and the target image. In this paper, we propose a method that can address the spatial deformation of input images and properly generate 1024 768 images.Normalization Layers.Normalization layers [13, 30] have been widely applied in modern deep neural networks.Normalization layers, whose afÔ¨Åne parameters are esti- mated with external data, are called conditional normaliza- tion layers.Conditional batch normalization [4] and adap- tive instance normalization [12] are such conditional nor- malization techniques and have been used in style transfer tasks.SPADE [20] and SEAN [39] utilize segmentation maps to apply spatially varying afÔ¨Åne transformations.Us- ing the misalignment mask as external data, our proposed normalization layer computes the means and the variances of the misaligned area and the other area within an instance separately.
After standardization, we modulate standard- ized activation maps with afÔ¨Åne parameters inferred from human-parsing maps to preserve semantic information. Virtual Try-On Approaches.There are two main cat- egories for virtual try-on approaches: 3D model-based approaches [8, 27, 23, 22] and 2D image-based ap- proaches [10, 31, 9, 36, 35, 5].3D model-based approaches can accurately simulate the clothes but are not widely appli- cable due to their dependency on 3D measurement data.2D image-based approaches do not rely on any 3D in- formation, thus being computationally efÔ¨Åcient and appro- priate for practical use.Jetchev and Bergmann [15] pro- posed CAGAN, which Ô¨Årst introduced the task of swap- ping fashion articles on human images.VITON [10] ad- dressed the same problem by proposing a coarse-to-Ô¨Åne synthesis framework that involves TPS transformation of clothes.
Most existing virtual try-on methods tackle differ- ent aspects of VITON to synthesize perceptually convincing photo-realistic images. CP-VTON [31] adopted a geometric matching module to learn the parameters of TPS transfor-mation, which improves the accuracy of deformation.VT- NFP [36] and ACGPN [35] predicted the human-parsing maps of a person wearing the target clothes in advance to guide the try-on image synthesis.Even though the image quality at high resolution is an essential factor in evaluating the practicality of the generated images, none of the meth- ods listed above could generate such photo-realistic images at high resolution.3.Proposed Method Model Overview.As described in Fig.
3, given a ref- erence image I2R3HWof a person and a clothing imagec2R3HW(HandWdenote the image height and width, respectively), the goal of VITON-HD is to gen- erate a synthetic image ^I2R3HWof the same person wearing the target clothes c, where the pose and body shape ofIand the details of care preserved. While training the model with (I;c;^I)triplets is straightforward, construction of such dataset is costly.Instead, we use (I;c;I )where the person in the reference image Iis already wearing c.Since directly training on (I;c;I )can harm the model‚Äôs generalization ability at test time, we Ô¨Årst compose a clothing-agnostic person representation that leaves out the clothing information in Iand use it as an input.Our new clothing-agnostic person representation uses both the pose map and the segmentation map of the person to eliminate the clothing information in I(Section 3.1).
The model gen- erates the segmentation map from the clothing-agnostic per- son representation to help the generation of ^I(Section 3.2). We then deform cto roughly align it to the human body (Section 3.3).Lastly, we propose the ALIgnment-Aware Segment (ALIAS) normalization that removes the mislead- ing information in the misaligned area after deforming c.ALIAS generator Ô¨Ålls the misaligned area with the clothing texture and maintains the clothing details (Section 3.4).3.1.Clothing-Agnostic Person Representation To train the model with pairs of candIalready wearing c, a person representation without the clothing information inIhas been utilized in the virtual try-on task.
Such rep- resentations have to satisfy the following conditions: (1) the original clothing item to be replaced should be deleted; (2) sufÔ¨Åcient information to predict the pose and the body shape of the person should be maintained; (3) the regions to be preserved ( e.g., face and hands) should be kept to main- tain the person‚Äôs identity. Problems of Existing Person Representations.In or- der to maintain the person‚Äôs shape, several approaches [10, 31, 36] provide a coarse body shape mask as a cue to syn- thesize the image, but fail to reproduce the body parts elab- orately ( e.g., hands).
To tackle this issue, ACGPN [35] em- ploys the detailed body shape mask as the input, and the neural network attempts to discard the clothing informa- ·àòùëÜ ùúÉ Warpùêºùëé‚®ÅùëÉ‚®Å·àòùëÜùëê ùëêùí≤(ùëê,ùúÉ) ùëÜùëÜùëé Remove Clothing & Arms ùêº ùëÉ Remove Clothing & Arms ùëÜùëé‚®ÅùëÉ‚®Åùëê ·àòùêº(c) Clothes Deformation(a) Pre -processing (b) Segmentation Generation (d) Try -On Synthesis ALIAS GeneratorSegmentation Generator ùêºùëé ·àòùëÜ Geometric Matching Module ùêºùëé‚®ÅùëÉ‚®Åùí≤(ùëê,ùúÉ)ùëÄùëöùëñùë†ùëéùëôùëñùëîùëõFigure 3: Overview of a VITON-HD. (a) First, given a reference image Icontaining a target person, we predict the seg- mentation map Sand the pose map P, and utilize them to pre-process IandSas a clothing-agnostic person image Ia and segmentation Sa.(b) Segmentation generator produces the synthetic segmentation ^Sfrom (Sa;P;c).(c) Geometric matching module deforms the clothing image caccording to the predicted clothing segmentation ^Scextracted from ^S.
(d) Finally, ALIAS generator synthesizes the Ô¨Ånal output image ^Ibased on the outputs from the previous stages via our ALIAS normalization. tion to be replaced.However, since the body shape mask includes the shape of the clothing item, neither the coarse body shape mask nor the neural network could perfectly eliminate the clothing information.As a result, the original clothing item that is not completely removed causes prob- lems in the test phase.Clothing-Agnostic Person Representation.We pro- pose a clothing-agnostic image Iaand a clothing-agnostic segmentation map Saas inputs of each stage, which truly eliminate the shape of clothing item and preserve the body parts that need to be reproduced.We Ô¨Årst predict the segmentation map S2LHWand the pose map P2 R3HWof the image Iby utilizing the pre-trained net- works [7, 3], where Lis a set of integers indicating the se- mantic labels.
The segmentation map Sis used to remove the clothing region to be replaced and preserve the rest of the image. The pose map Pis utilized to remove the arms, but not the hands, as they are difÔ¨Åcult to reproduce.Based onSandP, we generate the clothing-agnostic image Ia and the clothing-agnostic segmentation map Sa, which al- low the model to remove the original clothing information thoroughly, and preserve the rest of the image.In addition, unlike other previous work, which adopts the pose heatmap with each channel corresponded to one keypoint, we con-catenateIaorSato the RGB pose map Prepresenting a skeletal structure that improves generation quality.3.2.Segmentation Generation Given the clothing-agnostic person representation (Sa;P), and the target clothing item c, the segmentation generatorGSpredicts the segmentation map ^S2LHW of the person in the reference image wearing c.
We train GSto learn the mapping between Sand(Sa;P;c), in which the original clothing item information is completely removed. As the architecture of GS, we adopt U-Net [25], and the total loss LSof the segmentation generator are written as LS=LcGAN +CELCE; (1) whereLCEandLcGAN denote the pixel-wise cross-entropy loss and conditional adversarial loss between ^SandS, re- spectively.CEis the hyperparameter corresponding to the relative importance between two losses.3.3.Clothing Image Deformation In this stage, we deform the target clothing item cto align it with ^Sc, which is the clothing area of ^S.We employ the geometric matching module proposed in CP- VTON [31] with the clothing-agnostic person representa- ConvALIAS ResBlkUpALIAS ResBlk ‚ãØConvResize Resize Resize Resize ‚ãØ 1024x768·àòùêº Conv ALIAS NormReLUConvALIAS NormReLUConv ALIAS NormReLUConv·àòùëÜ,ùëÄùëöùëñùë†ùëéùëôùëñùëîùëõ ‚Ñéùëñ‚Ñéùëñ+1 ùëØi√óùëæùíäConcat Add(b)ALIAS ResBlk ùêºùëé‚®ÅùëÉ‚®Åùí≤(ùëê,ùúÉ) (a)ALIAS GeneratorConv ùëÄùëöùëñùë†ùëéùëôùëñùëîùëõ·àòùëÜùêºùëé‚®ÅùëÉ‚®Åùëä(ùëê,ùúÉ)Figure 4: ALIAS generator.
(a) The ALIAS generator is composed of a series of ALIAS residual blocks, along with up- sampling layers. The input (Ia;P;W(c;))is resized and injected into each layer of the generator.(b) A detailed view of a ALIAS residual block.Resized (Ia;P;W(c;))is concatenated to hiafter passing through a convolution layer.Each ALIAS normalization layer leverages resized ^SandMmisalign to normalize the activation.tion(Ia;P)and^Scas inputs.A correlation matrix be- tween the features extracted from (Ia;P)andcis Ô¨Årst cal- culated .With the correlation matrix as an input, the regres- sion network predicts the TPS transformation parameters 2R255, and thencis warped by .In the training phase, the model takes Scextracted from Sinstead of ^Sc.The module is trained with the L1 loss between the warped clothes and the clothes Icthat is extracted from I.In addi- tion, the second-order difference constraint [35] is adopted to reduce obvious distortions in the warped clothing images from deformation.
The overall objective function to warp the clothes to Ô¨Åt the human body is written as Lwarp =jjIc W(c;)jj1;1+constLconst; (2) whereWis the function that deforms cusing,Lconst is a second-order difference constraint, and const is the hyper- parameter forLconst . 3.4.Try-On Synthesis via ALIAS Normalization We aim to generate the Ô¨Ånal synthetic image ^Ibased on the outputs from the previous stages.Overall, we fuse the clothing-agnostic person representation (Ia;P) and the warped clothing image W(c;), guided by ^S.(Ia;P;W(c;))is injected into each layer of the generator.For^S, we propose a new conditional normalization method called the ALIgnment-Aware Segment (ALIAS) normaliza- tion.ALIAS normalization enables the preservation of se- mantic information, and the removal of misleading infor- mation from the misaligned regions by leveraging ^Sand the mask of these regions.Alignment-Aware Segment Normalization.
Let us de- notehi2RNCiHiWias the activation of the i-th layer of a network for a batch of Nsamples, where Hi,Wi, and Ciindicate the height, width, and the number of channels ofhi, respectively. ALIAS normalization has two inputs: StandardizationConv ConvùõΩ ùõæ ùëÄùëöùëñùë†ùëéùëôùëñùëîùëõ ‚àí= ùëÄùëéùëôùëñùëîùëõ·àòùëÜùëêConv ·àòùëÜùëëùëñùë£Figure 5: ALIAS normalization.First, the activation is separately standardized according to the regions divided by Mmisalign , which can be obtained from the difference be- tween ^ScandMalign .Next, ^Sdivis convolved to create the modulation parameters and , and then the standardized activation is modulated with the parameters and .(1) the synthetic segmentation map ^S; (2) the misalign- ment binary mask Mmisalign2LHW, which excludes the warped mask of the target clothing image W(Mc;)from ^Sc(Mcdenotes the target clothing mask), i.e., Malign =^Sc\W(Mc;) (3) Mmisalign =^Sc Malign: (4) Fig.5 illustrates the workÔ¨Çow of the ALIAS normaliza- tion.We Ô¨Årst obtain Malign andMmisalign from Eq.
(3) and Eq. (4).We deÔ¨Åne the modiÔ¨Åed version of ^Sas^Sdiv, where ^Scin^Sseparates into Malign andMmisalign .ALIAS nor- malization standardizes the regions of Mmisalign and the other regions in hiseparately, and then modulates the stan- dardized activation using afÔ¨Åne transformation parameters 256192 512 384 1024 768 SSIM "LPIPS #FID # SSIM "LPIPS #FID # SSIM "LPIPS #FID # CP-VTON 0.739 0.159 56.23 0.791 0.141 31.96 0.786 0.158 43.28 ACGPN 0.842 0.064 26.45 0.863 0.067 15.22 0.856 0.102 43.39 VITON-HD* - - - - - - 0.893 0.054 12.47 VITON-HD 0.844 0.062 27.83 0.870 0.052 14.05 0.895 0.053 11.74 Table 1: Quantitative comparison with baselines across different resolutions.VITON-HD* is a VITON-HD variant where the standardization in ALIAS normalization is replaced by channel-wise standardization as in the original instance normalization.For the SSIM, higher is better.For the LPIPS and the FID, lower is better.inferred from ^Sdiv.
The activation value at site ( n2N;k2 Ci;y2Hi;x2Wi) is calculated by i k;y;x(^Sdiv)hi n;k;y;x i;m n;k i;m n;k+ i k;y;x(^Sdiv); (5) wherehi n;k;y;x is the activation at the site before normal- ization and i k;y;x and i k;y;x are the functions that convert ^Sdivto modulation parameters of the normalization layer. i;m n;kandi;m n;kare the mean and standard deviation of the activation in sample nand channel k.i;m n;kandi;m n;kare calculated by i;m n;k=1 j i;m njX (y;x)2 i;m nhi n;k;y;x (6) i;m n;k=vuut1 j i;m njX (y;x)2 i;m n(hi n;k;y;x i;m n;k)2;(7) where i;m ndenotes the set of pixels in region m, which is Mmisalign or the other region, and j i;m njis the number of pixels in i;m n.Similar to instance normalization [30], the activation is standardized per channel.However, ALIAS normalization divides the activation in channel kinto the activation in the misaligned region and the other region.
The rationale behind this strategy is to remove the mis- leading information in the misaligned regions. SpeciÔ¨Åcally, the misaligned regions in the warped clothing image match the background that is irrelevant to the clothing texture.Per- forming a standardization separately on these regions leads to a removal of the background information that causes the artifacts in the Ô¨Ånal results.In modulation, afÔ¨Åne pa- rameters inferred from the segmentation map modulate the standardized activation.Due to injecting semantic informa- tion at each ALIAS normalization layer, the layout of the human-parsing map in the Ô¨Ånal result is preserved.ALIAS Generator.Fig.4 describes the overview of the ALIAS generator, which adopts the simpliÔ¨Åed architecture that discards the encoder part of an encoder-decoder net- work.The generator employs a series of residual blocks (ResBlk) with upsampling layers.Each ALIAS ResBlkconsists of three convolutional layers and three ALIAS nor- malization layers.
Due to the different resolutions that Res- Blks operate at, we resize the inputs of the normalization layers, ^SandMmisalign , before injecting them into each layer. Similarly, the input of the generator, (Ia;P;W(c;)), is resized to different resolutions.Before each ResBlk, the resized inputs (Ia;P;W(c;))are concatenated to the acti- vation of the previous layer after passing through a convolu- tion layer, and each ResBlk utilizes the concatenated inputs to reÔ¨Åne the activation.In this manner, the network per- forms the multi-scale reÔ¨Ånement at a feature level that bet- ter preserves the clothing details than a single reÔ¨Ånement at the pixel level.We train the ALIAS generator with the con- ditional adversarial loss, the feature matching loss, and the perceptual loss following SPADE [20] and pix2pixHD [32].Details of the model architecture, hyperparameters, and the loss function are described in the supplementary.4.Experiments 4.1.Experiment Setup Dataset.
We collected 1024 768 virtual try-on dataset for our research purpose, since the resolution of images on the dataset provided by Han et al. [10] was low.SpeciÔ¨Åcally, we crawled 13,679 frontal-view woman and top clothing image pairs on an online shopping mall website.The pairs are split into a training and a test set with 11,647 and 2,032 pairs, respectively.We use the pairs of a person and a cloth- ing image to evaluate a paired setting, and we shufÔ¨Çe the clothing images for an unpaired setting.The paired setting is to reconstruct the person image with the original clothing item, and the unpaired setting is to change the clothing item on the person image with a different item.Training and Inference.With the goal of reconstruct- ingIfrom (Ia;c), the training of each stage proceeds in- dividually.During the training of the geometric matching module and the ALIAS generator, we use Sinstead of ^S.
While we aim to generate 1024 768 try-on images, we train the segmentation generator and the geometric match- ing module at 256 192. In the inference phase, after be- ing predicted by the segmentation generator at 256 192, the segmentation map is upscaled to 1024 768 and passed Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Segmentation (Ours) Figure 6: Qualitative comparison of the baselines.Target ClothesAgnostic (ACGPN)Output (ACGPN)Agnostic (Ours)Reference ImageOutput (Ours) Figure 7: Qualitative comparison of the segmentation gen- erator of ACGPN and VITON-HD.The clothing-agnostic segmentation map used by each model is also reported.to subsequent stages.Similarly, the geometric matching module predicts the TPS parameters at 256192, and the 1024768 clothing image deformed by the parameters is used in the ALIAS generator.We empirically found that this approach makes these two modules perform better with a lower memory cost than those trained at 1024 768.
De- tails of the model architecture and hyperparameters are de- scribed in the supplementary.4.2. Qualitative Analysis We compare VITON-HD with CP-VTON [31] and ACGPN [35], whose codes are publicly available.Follow- ing the training and inference procedure of our model, seg- mentation generators and geometric matching modules of the baselines are trained at 256 192, and the outputs from the modules are upscaled to 1024 768 during the inference.Comparison with Baselines.Fig.6 demonstrates that VITON-HD generates more perceptually convincing 1024768 images compared to the baselines.Our model clearly preserves the details of the target clothes, such as the logos and the clothing textures, due to the multi-scale reÔ¨Ånement at a feature level.In addition, regardless of what clothes the person is wearing in the reference image, our model synthesizes the body shape naturally.As shown in Fig.
7, the shape of the original clothing item remains in the synthetic segmentation map generated by ACGPN. On the other hand, the segmentation generator in VITON-HD suc- cessfully predicts the segmentation map regardless of the original clothing item, due to our newly proposed clothing- agnostic person representation.Although our model sur- passes the baselines qualitatively, there are a few limitations w/o ALIAS norm with ALIAS norm w/o ALIAS norm with ALIAS normFigure 8: Effects of ALIAS normalization.The orange colored areas in the enlarged images indicate the misaligned regions.0.040.060.080.10.120.140.160.18 Small Medium LargeLPIPS Misaligned AreaCP-VTON ACGPN OursFigure 9: LPIPS scores according to the degree of misalignment.to VITON-HD, which are reported in the supplementary with the additional qualitative results.Effectiveness of the ALIAS Normalization.
We study the effectiveness of ALIAS normalization by comparing our model to VITON-HD*, where the standardization in ALIAS normalization is replaced by channel-wise standard- ization, as in the original instance normalization [30]. Fig.8 shows that ALIAS normalization has the capability to Ô¨Åll the misaligned areas with the target clothing texture by re- moving the misleading information.On the other hand, without utilizing ALIAS normalization, the artifacts are produced in the misaligned areas, because the background information in the warped clothing image is not removed as described in Section 3.4.ALIAS normalization, however, can handle the misaligned regions properly.4.3.Quantitative Analysis We perform the quantitative experiments in both a paired and an unpaired settings, in which a person wears the origi- nal clothes or the new clothes, respectively.We evaluate our method using three metrics widely used in virtual try-on.
The structural similarity (SSIM) [33] and the learned per- ceptual image patch similarity (LPIPS) [38] are used in the paired setting, and the frechet inception distance (FID) [11] score is adopted in the unpaired setting. The inception score [26] is not included in the experiments, since it cannot reÔ¨Çect whether the details of the clothing image are main- tained [10].The input of the each model contains different amount of information that offers advantages in reconstruct- ing the segmentation maps, thus we use the segmentation maps from the test set instead of the synthetic segmentation maps in the paired setting.Comparison across Different Resolutions.We com- pare the baselines quantitatively across different resolutions (256192, 512384, and 1024768) as shown in Table 1.Our model outperforms the baselines for SSIM and LPIPS across all resolutions.For FID score, our model signiÔ¨Å- cantly surpasses CP-VTON, regardless of the resolutions.
The FID score in ACGPN is slightly lower than that of our model at the 256 192 resolution. However, at the1024768 resolution, our model achieves a lower FID score than ACGPN with a large margin.The results indicate that the baselines cannot handle 1024 768 images, while our model is trained in a stable manner, even at a high resolu- tion.This may be due to the limited capability of the U-Net architecture employed in the baseline models.Comparison According to the Degree of Misalign- ment.To verify the ability of Ô¨Ålling the misaligned areas with the clothing texture, we perform experiments in the paired setting according to the degree of the misalignment.According to the number of pixels in the misaligned areas, we divide the test dataset in three types: small, medium, and large.For a fair comparison, each model uses the same seg- mentation maps and the same warped clothes as inputs to match the misaligned regions.
We evaluate LPIPS to mea- sure the semantic distances between the reference images and the reconstructed images. As shown in Fig.9, the wider the misaligned areas, the worse the performance of models, which means that the misalignment hinders the models from generating photo-realistic virtual try-on images.Compared to the baselines, our model consistently performs better, and the performance of our model decreases less as the degree of misalignment increases.5.Conclusions We propose the VITON-HD that synthesizes photo- realistic 1024768 virtual try-on images.The proposed ALIAS normalization can properly handle the misaligned areas and propagate the semantic information throughout the ALIAS generator, which preserves the details of the clothes via the multi-scale reÔ¨Ånement.Qualitative and quantitative experiments demonstrate that VITON-HD sur- passes existing virtual try-on methods with a large margin.Acknowledgments.
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. NRF-2019R1A2C4070420) and Seoul R&BD Program (CD200024) through the Seoul Business Agency (SBA) funded by the Seoul Metropolitan Government.References [1] Ivan Anokhin, Pavel Solovev, Denis Korzhenkov, Alexey Kharlamov, Taras Khakhulin, Aleksei Silvestrov, Sergey Nikolenko, Victor Lempitsky, and Gleb Sterkin.High- resolution daytime translation without domain labels.In Proc.of the IEEE conference on computer vision and pat- tern recognition (CVPR) , pages 7488‚Äì7497, 2020.3 [2] Andrew Brock, Jeff Donahue, and Karen Simonyan.Large scale gan training for high Ô¨Ådelity natural image synthesis.In Proc.the International Conference on Learning Representa- tions (ICLR) , 2018.3 [3] Z Cao, T Simon, SE Wei, YA Sheikh, et al.Openpose: Realtime multi-person 2d pose estimation using part afÔ¨Ån- ity Ô¨Åelds.
The IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2019. 4 [4] Harm De Vries, Florian Strub, J ¬¥er¬¥emie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C Courville.Modu- lating early visual processing by language.In Proc.the Ad- vances in Neural Information Processing Systems (NeurIPS) , pages 6594‚Äì6604, 2017.3 [5] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bochao Wang, Hanjiang Lai, Jia Zhu, Zhiting Hu, and Jian Yin.Towards multi-pose guided virtual try-on network.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 9026‚Äì9035, 2019.3 [6] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bowen Wu, Bing-Cheng Chen, and Jian Yin.Fw-gan: Flow-navigated warping gan for video virtual try-on.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 1161‚Äì1170, 2019.2 [7] Ke Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, Ming Yang, and Liang Lin.Instance-level human parsing via part grouping network.In Proc.
of the European Conference on Computer Vision (ECCV) , pages 770‚Äì785, 2018. 4 [8] Peng Guan, Loretta Reiss, David A Hirshberg, Alexander Weiss, and Michael J Black.Drape: Dressing any person.ACM Transactions on Graphics (TOG) , 31(4):1‚Äì10, 2012.3 [9] Xintong Han, Xiaojun Hu, Weilin Huang, and Matthew R Scott.ClothÔ¨Çow: A Ô¨Çow-based model for clothed person generation.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 10471‚Äì10480, 2019.2, 3, 13 [10] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry S Davis.Viton: An image-based virtual try-on network.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 7543‚Äì7552, 2018.2, 3, 6, 8 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.Gans trained by a two time-scale update rule converge to a local nash equilib- rium.In Proc.the Advances in Neural Information Process- ing Systems (NeurIPS) , pages 6629‚Äì6640, 2017.
8 [12] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 1501‚Äì1510, 2017.3 [13] Sergey Ioffe and Christian Szegedy.Batch normalization: Accelerating deep network training by reducing internal co-variate shift.In Proc.the International Conference on Ma- chine Learning (ICML) , pages 448‚Äì456, 2015.3 [14] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.Image-to-image translation with conditional adversar- ial networks.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 1125‚Äì1134, 2017.3 [15] Nikolay Jetchev and Urs Bergmann.The conditional analogy gan: Swapping fashion articles on people images.In Proc.of the IEEE international conference on computer vision work- shop (ICCVW) , pages 2287‚Äì2292, 2017.3 [16] Diederik P Kingma and Jimmy Ba.Adam: A method for stochastic optimization.arXiv:1412.6980 , 2014.
13 [17] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks.In Proc.of the IEEE international con- ference on computer vision (ICCV) , pages 2794‚Äì2802, 2017.12 [18] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.Spectral normalization for generative ad- versarial networks.In Proc.the International Conference on Learning Representations (ICLR) , 2018.11 [19] Augustus Odena, Christopher Olah, and Jonathon Shlens.Conditional image synthesis with auxiliary classiÔ¨Åer gans.InProc.the International Conference on Machine Learning (ICML) , pages 2642‚Äì2651, 2017.3 [20] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.Semantic image synthesis with spatially-adaptive nor- malization.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 2337‚Äì2346, 2019.3, 6, 12 [21] Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A.
Efros, and Richard Zhang. Swapping autoencoder for deep image manipulation.In Proc.the Ad- vances in Neural Information Processing Systems (NeurIPS) , 2020.3 [22] Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons- Moll.Tailornet: Predicting clothing in 3d as a function of hu- man pose, shape and garment style.In Proc.of the IEEE con- ference on computer vision and pattern recognition (CVPR) , pages 7365‚Äì7375, 2020.3 [23] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black.Clothcap: Seamless 4d clothing capture and retar- geting.ACM Transactions on Graphics (TOG) , 36(4):1‚Äì15, 2017.3 [24] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo- geswaran, Bernt Schiele, and Honglak Lee.Generative adversarial text to image synthesis.In Proc.the Inter- national Conference on Machine Learning (ICML) , pages 1060‚Äì1069, 2016.3 [25] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.U-net: Convolutional networks for biomedical image segmentation.
InInternational Conference on Medical Image Computing and Computer Assisted Intervention , pages 234‚Äì241, 2015. 2, 4, 11 [26] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.Improved techniques for training gans.In Proc.the Advances in Neural Informa- tion Processing Systems (NeurIPS) , pages 2234‚Äì2242, 2016.8 [27] Masahiro Sekine, Kaoru Sugita, Frank Perbet, Bj ¬®orn Stenger, and Masashi Nishiyama.Virtual Ô¨Åtting by single-shot body shape estimation.In International Conference on 3D Body Scanning Technologies , pages 406‚Äì413, 2014.3 [28] Wei Shen and Rujie Liu.Learning residual images for face attribute manipulation.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 4030‚Äì4038, 2017.3 [29] Karen Simonyan and Andrew Zisserman.Very deep convo- lutional networks for large-scale image recognition.arXiv preprint arXiv:1409.1556 , 2014.13 [30] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
In- stance normalization: The missing ingredient for fast styliza- tion. arXiv preprint arXiv:1607.08022 , 2016.3, 6, 8 [31] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang.Toward characteristic- preserving image-based virtual try-on network.In Proc.of the European Conference on Computer Vision (ECCV) , pages 589‚Äì604, 2018.2, 3, 4, 7, 13 [32] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.High-resolution image syn- thesis and semantic manipulation with conditional gans.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 8798‚Äì8807, 2018.2, 3, 6, 11, 12 [33] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si- moncelli.Image quality assessment: from error visibility to structural similarity.IEEE Transactions on Image Process- ing, 13(4):600‚Äì612, 2004.8 [34] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.
Attngan: Fine- grained text to image generation with attentional genera- tive adversarial networks. In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 1316‚Äì1324, 2018.3 [35] Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wang- meng Zuo, and Ping Luo.Towards photo-realistic virtual try-on by adaptively generating-preserving image content.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 7850‚Äì7859, 2020.2, 3, 5, 7, 12, 13 [36] Ruiyun Yu, Xiaoqi Wang, and Xiaohui Xie.Vtnfp: An image-based virtual try-on network with body and cloth- ing feature preservation.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 10511‚Äì10520, 2019.2, 3 [37] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus- tus Odena.Self-attention generative adversarial networks.InProc.the International Conference on Machine Learning (ICML) , pages 7354‚Äì7363, 2019.
13 [38] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric.In Proc.of the IEEE con- ference on computer vision and pattern recognition (CVPR) , 2018.8[39] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka.Sean: Image synthesis with semantic region-adaptive nor- malization.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 5104‚Äì5113, 2020.3 Supplementary Material A.Implementation Details A.1.Pre-processing Details This section introduces the details of generating our clothing-agnostic person representation.To remove the de- pendency on the clothing item originally worn by a person, regions that can provide any original clothing information, such as the arms that hint at the sleeve length, should be eliminated.Therefore, when generating a clothing-agnostic imageIa, we remove the arms from the reference image I.
For the same reason, legs should be removed if the pants are the target clothing items. We mask the regions with a gray color, so that the masked pixels of the normalized im- age would have a value of 0.We add padding to the masks to thoroughly remove these regions, and the width of the padding is empirically determined.A.2.Model Architectures This section introduces the architectures of the seg- mentation generator, the geometric matching module, and ALIAS generator in detail.Segmentation Generator.The segmentation generator has the structure of U-Net [25], which consists of convolu- tional layers, downsampling layers, and upsampling layers.Two multi-scale discriminators [32] are employed for the conditional adversarial loss.The details of the segmenta- tion generator architecture are shown in Fig.10.Geometric Matching Module.The geometric matching module consists of two feature extractors and a regression network.
A correlation matrix is calculated from the two extracted features, and the regression network predicts the TPS parameter with the correlation matrix. The feature extractor is composed of a series of convolutional layers, and the regression network consists of a series of convolu- tional layers followed by a fully connected layer.The de- tails are shown in Fig.11.ALIAS Generator.The architecture of the ALIAS gen- erator consists of a series of ALIAS ResBlks with nearest- neighbor upsampling layers.We employ two multi-scale discriminators with instance normalization.Spectral nor- malization [18] is applied to all the convolutional layers.Note that we separately standardize the activation based on the misalignment mask Mmisalign only in the Ô¨Årst Ô¨ÅveALIAS ResBlks.The details of the ALIAS generator archi- tecture is shown in Fig.12.A.3.
Training Details This section introduces the losses and the hyperparame- ters for the segmentation generator, the geometric matching module, and the ALIAS generator. Segmentation Generator.The segmentation genera- torGSuses the clothing-agnostic segmentation map Sa, the pose map P, and the clothing item cas inputs ( ^S= GS(Sa;P;c)) to predict the segmentation map ^Sof the person in the reference image wearing the target cloth- ing item.The segmentation generator is trained with the ConvBlk (64), MaxPool (2) ConvBlk (128), MaxPool (2) ConvBlk (256), MaxPool (2) ConvBlk (512), MaxPool (2) ConvBlk (1024) Upsample (2), ConvBlk (512) Upsample (2), ConvBlk (256) Upsample (2), ConvBlk (128) Upsample (2), ConvBlk (64) 3x3 Conv (13), Softmax Figure 10: Segmentation Generator.kkConv (x) denotes a convolutional layer where the kernel size is kand the out- put channel is x.
Also, ConvBlk ( x) denotes a block, which consists of two series of 3 3 convolutional layer, instance normalization, and ReLU activation. 4x4-‚Üì2 Conv (64), BN, ReLU 4x4-‚Üì2 Conv (64), BN, ReLU 4x4-‚Üì2 Conv (128), BN, ReLU 4x4-‚Üì2 Conv (128), BN, ReLU 4x4-‚Üì2 Conv (256), BN, ReLU 4x4-‚Üì2 Conv (256), BN, ReLU 4x4-‚Üì2 Conv (512), BN, ReLU 4x4-‚Üì2 Conv (512), BN, ReLU 3x3 Conv (512), BN, ReLU 3x3 Conv (512), BN, ReLU 4x4-‚Üì2 Conv (512), BN, ReLU 4x4-‚Üì2 Conv (256), BN, ReLU 3x3 Conv (128), BN, ReLU 3x3 Conv (64), BN, ReLU Linear (2x5x5), Tanh ùúÉ3x3 Conv (512), ReLU 3x3 Conv (512), ReLUFigure 11: Geometric Matching Module.kk#2 Conv (x) denotes a convolutional layer where the kernel size is k, the stride is 2, and the output channel is x.cross-entropy lossLCEand the conditional adversarial loss LcGAN , which is LSGAN loss [17].
The full loss LSfor the segmentation generator are written as LS=LcGAN +CELCE (8) LCE= 1 HWX k2C;y2H;x2WSk;y;xlog(^Sk;y;x) (9) LcGAN =E(X;S)[log(D(X;S))] +EX[1 log(D(X;^S))];(10) whereCEis the hyperparameter for the cross-entropy loss. In the experiment, CEis set to 10.In Eq.(9), Syxkand ^Syxkindicate the pixel values of the segmentation map of the reference image Sand^Scorresponding to the coordi- nates (x;y)in channelk.The symbols H,WandCindi- cate the height, width, and the number of channels of S.In Eq.(10), the symbol Xindicates the inputs of the generator (Sa;P;c ), andDdenotes the discriminator.The learning rate of the generator and the discriminator is 0.0004.We adopt the Adam optimizer with 1= 0:5 and 2= 0:999.We train the segmentation generator for 200,000 iterations with the batch size of 8.Geometric Matching Module.The inputs of the geo- metric matching module are c,P, clothing-agnostic image Ia, and ^Sc, which is the clothing area of ^S.
The output is 3x3 Conv (1024) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (512), Upsample (2) ALIAS ResBlk (256), Upsample (2) ALIAS ResBlk (128), Upsample (2) ALIAS ResBlk (64), Upsample (2) ALIAS ResBlk (32) 3x3 Conv (3), Tanh Figure 12: ALIAS Generator. The segmentation map Sand the misalignment mask Mmisalign are passed to the gener- ator through the proposed ALIAS ResBlks.the TPS transformation parameters .The overall objective function is written as Lwarp =jjIc W(c;)jj1;1+constLconst (11) Lconst =X p2Pj(jjjpp0jj2 jjpp1jj2j+jjjpp2jj2 jjpp3jj2j) +(jS(p;p0) S(p;p1)j+jS(p;p2) S(p;p3)j); (12) whereWis the function that deforms cusing, andIcis the clothing item extracted from the reference image I.Lconst is a second-order difference constraint [35], and const is the hyperparameter for Lconst .In the experiment, we set const to 0.04.In Eq.
(12), the symbol pindicates a sam- pled TPS control point from the entire control points set P, andp0,p1,p2, andp3are top, bottom, left and right point ofp, respectively. The function S(p;pi)denotes the slope betweenpandpi.The learning rate of the geometric matching module is 0.0002.We adopt the Adam optimizer with 1= 0:5and 2= 0:999.We train the geometric matching module for 50,000 iterations with the batch size of 8.ALIAS Generator.The loss function of ALIAS genera- tor follows those of SPADE [20] and pix2pixHD [32], as it Grid 5x5 Grid 10x10 Grid 20x20 ClothFlow Figure 13: Qualitative comparisons of TPS transformation with various grid numbers and the Ô¨Çow estimation from ClothFlow.Method Warp-SSIM"MACs#Mask-SSIM" ClothFlow 0.841?8.13G 0.803?VITON-HD 0.782 4.47G 0.852 Table 2:?denotes a score taken from the ClothFlow paper, and we train VITON-HD in the same setting ( e.g., dataset and resolution).We compute MACs of their warping mod- ules at 256192.
contains the conditional adversarial loss LcGAN , the feature matching lossLFM, and the perceptual loss Lpercept . Let DIbe the discriminator, Iandcbe the given reference and target clothing images, and ^Ibe the synthetic image gener- ated by the generator.Sdivis the modiÔ¨Åed version of the segmentation map S.The full lossLIof our generator is written as LI=LcGAN +FMLFM+perceptLpercept (13) LcGAN =EI[log(DI(Sdiv;I))] +E(I;c)[1 log(DI(Sdiv;^I))](14) LFM=E(I;c)TX i=11 Ki[jjD(i) I(Sdiv;I) D(i) I(Sdiv;^I)jj1;1] (15) Lpercept =E(I;c)VX i=11 Ri[jjF(i)(I) F(i)(^I)jj1;1];(16) whereFMandpercept are hyperparameters.In the ex- periment, both FMandpercept are set to 10.Tis the number of layers in DI, andD(i) IandKiare the activa- tion and the number of elements in the i-th layer of DI, respectively.Similarly, Vis the number of layers used in the VGG network F[29], andF(i)andRiare the acti- vation and the number of elements in the i-th layer of F, respectively.
We replace the standard adversarial loss with the Hinge loss [37]. The learning rate of the generator and the discriminator is 0.0001 and 0.0004, respectively.We adopt the Adam op- timizer [16] with 1= 0and 2= 0:9.We train the ALIAS generator for 200,000 iterations with the batch size of 4.B.Additional Experiments B.1.Comparison with ClothFlow To demonstrate that the optical Ô¨Çow estimation does not solve the misalignment completely, we re-implement the Ô¨Çow estimation module of ClothFlow [9] based on the orig- inal paper.Fig.13 shows that the misalignment still oc- curs, although both TPS with a higher grid number (e.g., a 1010 or 2020 grid) and the Ô¨Çow estimation module of ClothFlow can reduce the misaligned regions.The reason is that the regularization to avoid the artifacts ( e.g., TV loss) prevents the warped clothes from Ô¨Åtting perfectly into the target region.
In addition, we evaluate the accuracy and the computational cost of warping modules in VITON-HD and ClothFlow with Warp-SSIM [9] and MACs, respectively. We also measure how well the models reconstruct the cloth- ing using Mask-SSIM [9].Table 2 shows that the ClothFlow warping module has the better accuracy than ours, whereas the higher Mask-SSIM in VITON-HD proves that ALIAS normalization is more effective at solving the misalignment problem than the improved warping method.We found that the ClothFlow warping module needs a huge computational cost (MACs: 130.03G) at 1024 768, but the cost could be reduced when predicting the optical Ô¨Çow map at 256 192.Table 2 demonstrates that the ClothFlow warping module still needs more computational cost than ours, yet it is a viable option to combine the Ô¨Çow estimation module with ALIAS generator.
0.960.010.03 0.020.240.74 0.020.750.23 0% 20% 40% 60% 80% 100%OursACGPNCP-VTONQuality and Realism (%) Top1 Top2 Top3 0.880.030.09 0.10.280.62 0.020.690.29 0% 20% 40% 60% 80% 100%OursACGPNCP-VTONPreservation of Clothing Details (%) Top1 Top2 Top3 Figure 14: User study results. We compare our model with CP-VTON [31] and ACGPN [35].Figure 15: Failure cases of VITON-HD.B.2.User Study We further evaluate our model and other baselines via a user study in the unpaired setting.We randomly select 30 sets of a reference image and a target clothing image from the test dataset.Given the reference images and the target clothes, the users are asked to rank the 1024 768 outputs of our model and baselines according to the follow- ing questions: (1) Which image is the most photo-realistic?(2) Which image preserves the details of the target cloth- ing the most?As shown in Fig.14, it can be observed that our approach achieves the rank 1 votes more than 88% for the both questions.
The result demonstrates that our model generates more realistic images, and preserves the details of the clothing items compared to the baselines. B.3.Qualitative Results We provide additional qualitative results to demonstrate our model‚Äôs capability of handling high quality image syn- thesis.Fig.16, 17, 18, and 19 show the qualitative com- parison of the baselines across different resolutions.Fig.20, 21, 22, and 23 show additional results of VITON-HD at 1024768 resolution.C.Failure Cases and Limitations Fig.15 shows the failure cases of our model caused by the inaccurately predicted segmentation map or the inner collar region indistinguishable from the other clothing re- gion.Also, the boundaries of the clothing textures occa- sionally fade away.The limitations of our model are as follows.VITON-HD is trained to preserve the bottom clothing items, limiting the presentation of the target clothes ( e.g., whether they are tucked in).
It can be a valuable future direction to gener- ate multiple possible outputs from a single input pair. Next, our dataset mostly consists of slim women and top cloth- ing images, which makes VITON-HD handle only a limited range of body shapes and clothing during the inference.We believe that VITON-HD has the capability to cover more diverse cases when the images of various body shapes and clothing types are provided.Finally, existing virtual try-on methods including VITON-HD do not provide robust per- formance for in-the-wild images.We think generating real- istic try-on images for the in-the-wild images is an interest- ing topic for future work.Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Figure 16: Qualitative comparison of the baselines (256 192).Reference Image ACGPN VITON -HD (Ours) Target Clothes CP-VTON Figure 17: Qualitative comparison of the baselines (512 384).
Figure 18: Qualitative comparison of the baselines (1024 768). Figure 19: Qualitative comparison of the baselines (1024 768).Reference ImageTarget ClothesFigure 20: Additional qualitative results of VITON-HD.Figure 21: Sample 1 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.Figure 22: Sample 2 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.Figure 23: Sample 3 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.
Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.edu≈Åukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been Ô¨Årmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the Ô¨Årst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efÔ¨Åcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiÔ¨Åcant improvements in computational efÔ¨Åciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiÔ¨Åcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difÔ¨Åcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the Ô¨Årst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The Ô¨Årst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efÔ¨Åcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneÔ¨Åcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the Ô¨Ånal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information Ô¨Çow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and Ô¨Åxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any Ô¨Åxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneÔ¨Åt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiÔ¨Åcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the Ô¨Årst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conÔ¨Åguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of Ô¨Çoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision Ô¨Çoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneÔ¨Åcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-Ô¨Åtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the Ô¨Årst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiÔ¨Åcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efÔ¨Åciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770‚Äì778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber.Gradient Ô¨Çow in recurrent nets: the difÔ¨Åculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and J√ºrgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735‚Äì1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] ≈Åukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio ≈Åukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] OÔ¨År Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overÔ¨Åtting.Journal of Machine Learning Research , 15(1):1929‚Äì1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D.
D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440‚Äì2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104‚Äì3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google‚Äôs neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11 ÔªøInstruction,Input,Response Í≥†Í∞ùÏù¥ ÏöîÍ∏àÏ†ú Î≥ÄÍ≤ΩÏùÑ ÏöîÏ≤≠Ìï®,ÌòÑÏû¨ 5G ÏöîÍ∏àÏ†úÎ•º ÏÇ¨Ïö© Ï§ëÏù∏Îç∞ LTE ÏöîÍ∏àÏ†úÎ°ú Î≥ÄÍ≤ΩÌïòÍ≥† Ïã∂Ïñ¥Ïöî.,"Í≥†Í∞ùÎãò, ÌòÑÏû¨ ÏÇ¨Ïö© Ï§ëÏù∏ 5G ÏöîÍ∏àÏ†úÎ•º LTE ÏöîÍ∏àÏ†úÎ°ú Î≥ÄÍ≤ΩÌïòÏãúÎ†§Î©¥, Í∞ÄÍπåÏö¥ SKT ÎåÄÎ¶¨Ï†êÏùÑ Î∞©Î¨∏ÌïòÏãúÍ±∞ÎÇò TÏõîÎìú Ïï±ÏóêÏÑú Î≥ÄÍ≤Ω Í∞ÄÎä•Ìï©ÎãàÎã§.ÏõêÌïòÏãúÎäî ÏöîÍ∏àÏ†úÎ•º ÎßêÏîÄÌï¥ Ï£ºÏãúÎ©¥ ÏûêÏÑ∏Ìûà ÏïàÎÇ¥Ìï¥ ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©ÎüâÏùÑ Ï°∞ÌöåÌïòÍ≥† Ïã∂Ïñ¥Ìï®,Ïù¥Î≤à Îã¨ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©ÎüâÏù¥ ÏñºÎßàÎÇò ÎêòÎÇòÏöî?,"Í≥†Í∞ùÎãò, ÌòÑÏû¨ÍπåÏßÄ ÏÇ¨Ïö©ÌïòÏã† Îç∞Ïù¥ÌÑ∞Îäî 8.5GBÏù¥Î©∞, ÎÇ®ÏùÄ Îç∞Ïù¥ÌÑ∞Îäî 1.5GBÏûÖÎãàÎã§.Ï∂îÍ∞ÄÎ°ú Îç∞Ïù¥ÌÑ∞Í∞Ä ÌïÑÏöîÌïòÏãúÎ©¥ Îç∞Ïù¥ÌÑ∞ Ïø†Ìè∞ÏùÑ Íµ¨Îß§ÌïòÏã§ ÏàòÎèÑ ÏûàÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Ìï¥Ïô∏ Î°úÎ∞ç ÏöîÍ∏àÏ†úÎ•º Î¨∏ÏùòÌï®,"Îã§Ïùå Ï£ºÏóê ÏùºÎ≥∏ÏúºÎ°ú Ïó¨Ìñâ Í∞ÄÎäîÎç∞, Î°úÎ∞ç ÏöîÍ∏àÏ†ú Ïñ¥ÎñªÍ≤å ÎêòÎÇòÏöî?","Í≥†Í∞ùÎãò, ÏùºÎ≥∏ÏóêÏÑú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î°úÎ∞ç ÏöîÍ∏àÏ†úÎäî 'Î∞îÎ°ú Î°úÎ∞ç 300MB(9,900Ïõê/Ïùº)' ÎòêÎäî 'Î°úÎ∞ç Î¨¥Ï†úÌïú(13,900Ïõê/Ïùº)' Îì±Ïù¥ ÏûàÏäµÎãàÎã§.Îçî ÏûêÏÑ∏Ìïú ÏÇ¨Ìï≠ÏùÄ TÏõîÎìúÏóêÏÑú ÌôïÏù∏ Í∞ÄÎä•Ìï©ÎãàÎã§." Í≥†Í∞ùÏù¥ Í≤∞Ï†úÏùº Î≥ÄÍ≤ΩÏùÑ ÏöîÏ≤≠Ìï®,Îß§Ïõî ÏöîÍ∏à Í≤∞Ï†úÏùºÏùÑ Î∞îÍæ∏Í≥† Ïã∂ÏùÄÎç∞ Í∞ÄÎä•ÌïúÍ∞ÄÏöî?,"ÎÑ§, Í≥†Í∞ùÎãò.ÏöîÍ∏à Í≤∞Ï†úÏùº Î≥ÄÍ≤ΩÏùÄ Îß§Ïõî 1Ïùº, 15Ïùº, 25Ïùº Ï§ë ÏÑ†ÌÉù Í∞ÄÎä•Ìï©ÎãàÎã§.ÏõêÌïòÏãúÎäî ÎÇ†ÏßúÎ•º ÎßêÏîÄÌï¥ Ï£ºÏãúÎ©¥ Î≥ÄÍ≤Ω ÎèÑÏôÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Í∏∞Í∏∞ Ìï†Î∂ÄÍ∏à ÏûîÏï°ÏùÑ ÌôïÏù∏ÌïòÎ†§ Ìï®,ÌòÑÏû¨ ÏÇ¨Ïö© Ï§ëÏù∏ Ìú¥ÎåÄÌè∞ Ìï†Î∂ÄÍ∏àÏù¥ ÏñºÎßàÎÇò ÎÇ®ÏïòÎÇòÏöî?,"Í≥†Í∞ùÎãò, ÌòÑÏû¨ Îã®ÎßêÍ∏∞ Ìï†Î∂ÄÍ∏àÏùÄ 12Í∞úÏõî ÎÇ®ÏïÑ ÏûàÏúºÎ©∞, ÎÇ®ÏùÄ Í∏àÏï°ÏùÄ Ï¥ù 360,000ÏõêÏûÖÎãàÎã§.Ï°∞Í∏∞ ÏôÑÎÇ©ÏùÑ ÏõêÌïòÏãúÎ©¥ TÏõîÎìúÏóêÏÑú ÎÇ©Î∂Ä Í∞ÄÎä•Ìï©ÎãàÎã§." Í≥†Í∞ùÏù¥ Í∞ÄÏ°± Í≤∞Ìï© Ìï†Ïù∏ ÌòúÌÉùÏùÑ Î¨∏ÏùòÌï®,"Í∞ÄÏ°± Í≤∞Ìï© Ìï†Ïù∏ÏùÑ Î∞õÍ≥† Ïã∂ÏùÄÎç∞, Ïñ¥ÎñªÍ≤å ÌïòÎ©¥ ÎêòÎÇòÏöî?","Í≥†Í∞ùÎãò, Í∞ÄÏ°± Í≤∞Ìï© Ìï†Ïù∏ÏùÄ Í∞ÄÏ°± Íµ¨ÏÑ±Ïõê 2Ïù∏ Ïù¥ÏÉÅÏù¥ SKT ÌöåÏÑ†ÏùÑ ÏÇ¨Ïö© Ï§ëÏùº Í≤ΩÏö∞ Í∞ÄÎä•Ìï©ÎãàÎã§.Í∞ÄÏ°±Í¥ÄÍ≥Ñ Ï¶ùÎπôÏÑúÎ•òÎ•º Ï§ÄÎπÑÌïòÏÖîÏÑú Í∞ÄÍπåÏö¥ ÎåÄÎ¶¨Ï†êÏùÑ Î∞©Î¨∏Ìï¥ Ï£ºÏãúÎ©¥ Ïã†Ï≤≠ ÎèÑÏôÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Í∞úÌÜµ Í∞ÄÎä•Ìïú Ïä§ÎßàÌä∏Ìè∞ Í∏∞Ï¢ÖÏùÑ Î¨∏ÏùòÌï®,ÏïÑÏù¥Ìè∞ 15 Í∞úÌÜµ Í∞ÄÎä•ÌïúÍ∞ÄÏöî?,"Í≥†Í∞ùÎãò, ÌòÑÏû¨ ÏïÑÏù¥Ìè∞ 15Îäî Ï†ïÏÉÅ Í∞úÌÜµ Í∞ÄÎä•ÌïòÎ©∞, ÏÉâÏÉÅ Î∞è Ï†ÄÏû• Ïö©ÎüâÏóê Îî∞Îùº Ïû¨Í≥†Í∞Ä Îã§Î•º Ïàò ÏûàÏäµÎãàÎã§.
Í∞ÄÍπåÏö¥ SKT ÎåÄÎ¶¨Ï†êÏóê Î∞©Î¨∏ÌïòÏãúÎ©¥ Ï¶âÏãú Í∞úÌÜµ Í∞ÄÎä•Ìï©ÎãàÎã§." Í≥†Í∞ùÏù¥ Îç∞Ïù¥ÌÑ∞ Ïø†Ìè∞ Íµ¨Îß§ Î∞©Î≤ïÏùÑ Î¨∏ÏùòÌï®,Îç∞Ïù¥ÌÑ∞ 1GB Ï∂îÍ∞ÄÌïòÎ†§Î©¥ Ïñ¥ÎñªÍ≤å Ìï¥Ïïº ÌïòÎÇòÏöî?,"Í≥†Í∞ùÎãò, Îç∞Ïù¥ÌÑ∞ 1GB Ï∂îÍ∞ÄÎäî TÏõîÎìú Ïï± ÎòêÎäî SKT Í≥†Í∞ùÏÑºÌÑ∞(114)ÏóêÏÑú Íµ¨Îß§ Í∞ÄÎä•Ìï©ÎãàÎã§.
1GB Îç∞Ïù¥ÌÑ∞ Ïø†Ìè∞ÏùÄ 5,500ÏõêÏóê Ï†úÍ≥µÎêòÎ©∞, Ï¶âÏãú Ï†ÅÏö©Îê©ÎãàÎã§." Í≥†Í∞ùÏù¥ 5G ÏÜçÎèÑ Ï†ÄÌïò Î¨∏Ï†úÎ•º Ï†úÍ∏∞Ìï®,5GÏù∏Îç∞ ÏÜçÎèÑÍ∞Ä ÎÑàÎ¨¥ ÎäêÎ†§Ïöî.Ïôú Í∑∏Îü∞Í∞ÄÏöî?,"Í≥†Í∞ùÎãò, ÌòÑÏû¨ Í≥ÑÏã† ÏßÄÏó≠Ïùò ÎÑ§Ìä∏ÏõåÌÅ¨ ÌôòÍ≤ΩÏóê Îî∞Îùº ÏÜçÎèÑÍ∞Ä Ï†ÄÌïòÎê† Ïàò ÏûàÏäµÎãàÎã§.Ïû†Ïãú ÌõÑÏóêÎèÑ ÎèôÏùºÌïú Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌïòÎ©¥ SKT Í≥†Í∞ùÏÑºÌÑ∞Î°ú Î¨∏Ïùò Ï£ºÏãúÎ©¥ Ïã†ÏÜçÌûà Ï†êÍ≤Ä ÎèÑÏôÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Î≤àÌò∏ Î≥ÄÍ≤ΩÏùÑ ÏöîÏ≤≠Ìï®,ÌòÑÏû¨ Ï†ÑÌôîÎ≤àÌò∏Î•º Î∞îÍæ∏Í≥† Ïã∂ÏùÄÎç∞ Ïñ¥ÎñªÍ≤å ÌïòÎ©¥ ÎêòÎÇòÏöî?,"Í≥†Í∞ùÎãò, Î≤àÌò∏ Î≥ÄÍ≤ΩÏùÄ SKT ÎåÄÎ¶¨Ï†ê Î∞©Î¨∏ ÎòêÎäî Í≥†Í∞ùÏÑºÌÑ∞Î•º ÌÜµÌï¥ Í∞ÄÎä•Ìï©ÎãàÎã§.Î≤àÌò∏ Î≥ÄÍ≤Ω Ïãú Í∏∞Ï°¥ Î≤àÌò∏Î°ú Í±∏Î†§Ïò§Îäî Ï†ÑÌôîÎäî ÏùºÏ†ï Í∏∞Í∞Ñ ÏïàÎÇ¥ Î©îÏãúÏßÄÍ∞Ä Ï†úÍ≥µÎê©ÎãàÎã§." Í≥†Í∞ùÏù¥ 2G ÏÑúÎπÑÏä§ Ï¢ÖÎ£åÏóê ÎåÄÌï¥ Î¨∏ÏùòÌï®,"2G ÏÑúÎπÑÏä§Í∞Ä Ï¢ÖÎ£åÎêúÎã§Í≥† ÌïòÎçòÎç∞, Ïñ¥ÎñªÍ≤å Ìï¥Ïïº ÌïòÎÇòÏöî?","Í≥†Í∞ùÎãò, SKT 2G ÏÑúÎπÑÏä§Îäî Ï¢ÖÎ£åÎêòÏóàÏúºÎ©∞, 3G, LTE, 5G Í∏∞Í∏∞Î°ú Í∏∞Í∏∞ Î≥ÄÍ≤ΩÏùÑ Í∂åÏû•ÎìúÎ¶ΩÎãàÎã§.Î≥¥ÏÉÅ ÌîÑÎ°úÍ∑∏Îû®ÎèÑ ÏßÑÌñâ Ï§ëÏù¥Îãà ÏûêÏÑ∏Ìïú ÏÇ¨Ìï≠ÏùÄ Í≥†Í∞ùÏÑºÌÑ∞Î°ú Î¨∏Ïùò Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§." Í≥†Í∞ùÏù¥ Î∂ÑÏã§Ìè∞ ÏúÑÏπò Ï∂îÏ†ÅÏùÑ ÏöîÏ≤≠Ìï®,Ìï∏ÎìúÌè∞ÏùÑ ÏûÉÏñ¥Î≤ÑÎ†∏ÎäîÎç∞ ÏúÑÏπò Ï∂îÏ†ÅÏù¥ Í∞ÄÎä•Ìï†ÍπåÏöî?,"Í≥†Í∞ùÎãò, 'ÎÇ¥ ÎîîÎ∞îÏù¥Ïä§ Ï∞æÍ∏∞' Í∏∞Îä•ÏùÑ Ïù¥Ïö©ÌïòÏãúÎ©¥ ÏúÑÏπò Ï∂îÏ†ÅÏù¥ Í∞ÄÎä•Ìï©ÎãàÎã§.Ï∂îÍ∞ÄÎ°ú Í≥†Í∞ùÏÑºÌÑ∞Î•º ÌÜµÌï¥ Î∂ÑÏã§ Ïã†Í≥†Î•º ÌïòÏãúÎ©¥ ÏûÑÏãú Ï†ïÏßÄ Î∞è Î≥¥Ìò∏ Ï°∞ÏπòÎ•º ÎèÑÏôÄÎìúÎ¶¥ Ïàò ÏûàÏäµÎãàÎã§." Í≥†Í∞ùÏù¥ Î©§Î≤ÑÏã≠ ÌòúÌÉùÏùÑ Î¨∏ÏùòÌï®,SKT VIP Î©§Î≤ÑÏã≠ ÌòúÌÉùÏóêÎäî Ïñ¥Îñ§ Í≤ÉÏù¥ ÏûàÎÇòÏöî?,"Í≥†Í∞ùÎãò, SKT VIP Î©§Î≤ÑÏã≠ ÌòúÌÉùÏúºÎ°ú ÏòÅÌôî Ìï†Ïù∏, Ïπ¥Ìéò Î¨¥Î£å ÏùåÎ£å, Îç∞Ïù¥ÌÑ∞ Î¶¨ÌïÑ Ïø†Ìè∞ Ï†úÍ≥µ Îì±Ïù¥ ÏûàÏäµÎãàÎã§.ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ TÏõîÎìú Ïï±ÏóêÏÑú ÌôïÏù∏ Í∞ÄÎä•Ìï©ÎãàÎã§." Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.edu≈Åukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been Ô¨Årmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the Ô¨Årst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efÔ¨Åcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiÔ¨Åcant improvements in computational efÔ¨Åciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiÔ¨Åcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difÔ¨Åcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the Ô¨Årst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The Ô¨Årst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efÔ¨Åcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneÔ¨Åcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the Ô¨Ånal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information Ô¨Çow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and Ô¨Åxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any Ô¨Åxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneÔ¨Åt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiÔ¨Åcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the Ô¨Årst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conÔ¨Åguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of Ô¨Çoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision Ô¨Çoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneÔ¨Åcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-Ô¨Åtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the Ô¨Årst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiÔ¨Åcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efÔ¨Åciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770‚Äì778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber.Gradient Ô¨Çow in recurrent nets: the difÔ¨Åculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and J√ºrgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735‚Äì1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] ≈Åukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio ≈Åukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] OÔ¨År Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overÔ¨Åtting.Journal of Machine Learning Research , 15(1):1929‚Äì1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D.
D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440‚Äì2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104‚Äì3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google‚Äôs neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11 Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.edu≈Åukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been Ô¨Årmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the Ô¨Årst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efÔ¨Åcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiÔ¨Åcant improvements in computational efÔ¨Åciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiÔ¨Åcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difÔ¨Åcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the Ô¨Årst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The Ô¨Årst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efÔ¨Åcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneÔ¨Åcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the Ô¨Ånal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information Ô¨Çow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and Ô¨Åxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any Ô¨Åxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneÔ¨Åt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiÔ¨Åcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the Ô¨Årst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conÔ¨Åguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of Ô¨Çoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision Ô¨Çoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneÔ¨Åcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-Ô¨Åtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the Ô¨Årst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiÔ¨Åcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efÔ¨Åciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770‚Äì778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber.Gradient Ô¨Çow in recurrent nets: the difÔ¨Åculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and J√ºrgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735‚Äì1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] ≈Åukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio ≈Åukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] OÔ¨År Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overÔ¨Åtting.Journal of Machine Learning Research , 15(1):1929‚Äì1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D. D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440‚Äì2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104‚Äì3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google‚Äôs neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11
