VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization Seunghwan Choi*Sunghyun Park*Minsoo Lee*Jaegul Choo KAIST, Daejeon, South Korea fshadow2496, psh01087, alstn2022, jchoo g@kaist.ac.kr Reference Image Synthetic Image Figure 1: Given a pair of a reference image (containing a person) and a target clothing image, our method successfully synthesizes 1024768 virtual try-on images.Abstract The task of image-based virtual try-on aims to transfer a target clothing item onto the corresponding region of a person, which is commonly tackled by ﬁtting the item to the desired body part and fusing the warped item with the person.While an increasing number of studies have been conducted, the resolution of synthesized images is still lim- ited to low ( e.g., 256192), which acts as the critical lim- itation against satisfying online consumers.
We argue that the limitation stems from several challenges: as the resolu- tion increases, the artifacts in the misaligned areas between the warped clothes and the desired clothing regions become noticeable in the ﬁnal results; the architectures used in ex- *These authors contributed equally.isting methods have low performance in generating high- quality body parts and maintaining the texture sharpness of the clothes. To address the challenges, we propose a novel virtual try-on method called VITON-HD that successfully synthesizes 1024768 virtual try-on images.Speciﬁcally, we ﬁrst prepare the segmentation map to guide our virtual try-on synthesis, and then roughly ﬁt the target clothing item to a given person’s body.Next, we propose ALIgnment- Aware Segment (ALIAS) normalization and ALIAS genera- tor to handle the misaligned areas and preserve the details of 1024768 inputs.
Through rigorous comparison with ex- isting methods, we demonstrate that VITON-HD highly sur- passes the baselines in terms of synthesized image quality both qualitatively and quantitatively. Code is available at https://github.com/shadow2496/VITON-HD .arXiv:2103.16874v2 [cs.CV] 10 Sep 2021 1.Introduction Image-based virtual try-on refers to the image generation task of changing the clothing item on a person into a differ- ent item, given in a separate product image.With a growing trend toward online shopping, virtually wearing the clothes can enrich a customer’s experience, as it gives an idea about how these items would look on them.Virtual try-on is similar to image synthesis, but it has unique and challenging aspects.Given images of a person and a clothing product, the synthetic image should meet the following criteria: (1) The person’s pose, body shape, and identity should be preserved.
(2) The clothing prod- uct should be naturally deformed to the desired clothing re- gion of the given person, by reﬂecting his/her pose and body shape. (3) Details of the clothing product should be kept in- tact.(4) The body parts initially occluded by the person’s clothes in the original image should be properly rendered.Since the given clothing image is not initially ﬁtted to the person image, fulﬁlling these requirements is challenging, which leaves the development of virtual try-on still far be- hind the expectations of online consumers.In particular, the resolution of virtual try-on images is low compared to the one of normal pictures on online shopping websites.After Han et al.[10] proposed VITON, various image- based virtual try-on methods have been proposed [31, 36, 35, 6].
These methods follow two processes in common: (1) warping the clothing image initially to ﬁt the human body; (2) fusing the warped clothing image and the image of the person that includes pixel-level reﬁnement. Also, several recent methods [9, 36, 35] add a module that generates seg- mentation maps and determine the person’s layout from the ﬁnal image in advance.However, the resolution of the synthetic images from the previous methods is low ( e.g., 256192) due to the follow- ing reasons.First, the misalignment between the warped clothes and a person’s body results in the artifacts in the misaligned regions, which become noticeable as the image size increases.It is difﬁcult to warp clothing images to ﬁt the body perfectly, so the misalignment occurs as shown in Fig.2.Most of previous approaches utilize the thin-plate spline (TPS) transformation to deform clothing images.
To accurately deform clothes, ClothFlow [9] predicts the op- tical ﬂow maps of the clothes and the desired clothing re- gions. However, the optical ﬂow maps does not remove the misalignment completely on account of the regulariza- tion.In addition, the process requires more computational costs than other methods due to the need of predicting the movement of clothes at a pixel level.(The detailed analysis of ClothFlow is included in the supplementary.) Second, a simple U-Net architecture [25] used in existing approaches is insufﬁcient in synthesizing initially occluded body parts in ﬁnal high-resolution ( e.g., 1024768) images.As noted in Wang et al.[32], applying a simple U-Net-based archi- (c)Misaligned Regions (b)Warped Clothes on Segmentation(a)Warped Clothes on Reference ImageFigure 2: An example of misaligned regions.tecture to generate high-resolution images leads to unstable training as well as unsatisfactory quality of generated im- ages.
Also, reﬁning the images once at the pixel level is in- sufﬁcient in preserving the details of high-resolution cloth- ing images. To address the above-mentioned challenges, we pro- pose a novel high-resolution virtual try-on method, called VITON-HD.In particular, we introduce a new clothing- agnostic person representation that leverages the pose in- formation and the segmentation map so that the clothing information is eliminated thoroughly.Afterwards, we feed the segmentation map and the clothing item deformed to ﬁt the given human body to the model.Using the addi- tional information, our novel ALIgnment-Aware Segment (ALIAS) normalization removes information irrelevant to the clothing texture in the misaligned regions and propa- gates the semantic information throughout the network.The normalization separately standardizes the activations corre- sponding to the misaligned regions and the other regions, and modulates the standardized activations using the seg- mentation map.
Our ALIAS generator employing ALIAS normalization synthesizes the person image wearing the tar- get product while ﬁlling the misaligned regions with the clothing texture and preserving the details of the clothing item through the multi-scale reﬁnement at a feature level. To validate the performance of our framework, we col- lected a 1024768 dataset that consists of pairs of a per- son and a clothing item for our research purpose.Our ex- periments demonstrate that VITON-HD signiﬁcantly out- performs the existing methods in generating 1024 768 im- ages, both quantitatively and qualitatively.We also conﬁrm the superior capability of our novel ALIAS normalization module in dealing with the misaligned regions.We summarize our contributions as follows: • We propose a novel image-based virtual try-on ap- proach called VITON-HD, which is, to the best of our knowledge, the ﬁrst model to successfully synthesize 1024768 images.
• We introduce a clothing-agnostic person representa- tion that allows our model to remove the dependency on the clothing item originally worn by the person. • To address the misalignment between the warped clothes and the desired clothing regions, we propose ALIAS normalization and ALIAS generator, which is effective in maintaining the details of clothes.• We demonstrate the superior performance of our method through experiments with baselines on the newly collected dataset.2.Related Work Conditional Image Synthesis.Conditional generative adversarial networks (cGANs) utilize additional informa- tion, such as class labels [19, 2], text [24, 34], and at- tributes [28], to steer the image generation process.Since the emergence of pix2pix [14], numerous cGANs condi- tioned on input images have been proposed to generate high-resolution images in a stable manner [32, 1, 21].
How- ever, these methods tend to generate blurry images when handling a large spatial deformation between the input im- age and the target image. In this paper, we propose a method that can address the spatial deformation of input images and properly generate 1024 768 images.Normalization Layers.Normalization layers [13, 30] have been widely applied in modern deep neural networks.Normalization layers, whose afﬁne parameters are esti- mated with external data, are called conditional normaliza- tion layers.Conditional batch normalization [4] and adap- tive instance normalization [12] are such conditional nor- malization techniques and have been used in style transfer tasks.SPADE [20] and SEAN [39] utilize segmentation maps to apply spatially varying afﬁne transformations.Us- ing the misalignment mask as external data, our proposed normalization layer computes the means and the variances of the misaligned area and the other area within an instance separately.
After standardization, we modulate standard- ized activation maps with afﬁne parameters inferred from human-parsing maps to preserve semantic information. Virtual Try-On Approaches.There are two main cat- egories for virtual try-on approaches: 3D model-based approaches [8, 27, 23, 22] and 2D image-based ap- proaches [10, 31, 9, 36, 35, 5].3D model-based approaches can accurately simulate the clothes but are not widely appli- cable due to their dependency on 3D measurement data.2D image-based approaches do not rely on any 3D in- formation, thus being computationally efﬁcient and appro- priate for practical use.Jetchev and Bergmann [15] pro- posed CAGAN, which ﬁrst introduced the task of swap- ping fashion articles on human images.VITON [10] ad- dressed the same problem by proposing a coarse-to-ﬁne synthesis framework that involves TPS transformation of clothes.
Most existing virtual try-on methods tackle differ- ent aspects of VITON to synthesize perceptually convincing photo-realistic images. CP-VTON [31] adopted a geometric matching module to learn the parameters of TPS transfor-mation, which improves the accuracy of deformation.VT- NFP [36] and ACGPN [35] predicted the human-parsing maps of a person wearing the target clothes in advance to guide the try-on image synthesis.Even though the image quality at high resolution is an essential factor in evaluating the practicality of the generated images, none of the meth- ods listed above could generate such photo-realistic images at high resolution.3.Proposed Method Model Overview.As described in Fig.
3, given a ref- erence image I2R3HWof a person and a clothing imagec2R3HW(HandWdenote the image height and width, respectively), the goal of VITON-HD is to gen- erate a synthetic image ^I2R3HWof the same person wearing the target clothes c, where the pose and body shape ofIand the details of care preserved. While training the model with (I;c;^I)triplets is straightforward, construction of such dataset is costly.Instead, we use (I;c;I )where the person in the reference image Iis already wearing c.Since directly training on (I;c;I )can harm the model’s generalization ability at test time, we ﬁrst compose a clothing-agnostic person representation that leaves out the clothing information in Iand use it as an input.Our new clothing-agnostic person representation uses both the pose map and the segmentation map of the person to eliminate the clothing information in I(Section 3.1).
The model gen- erates the segmentation map from the clothing-agnostic per- son representation to help the generation of ^I(Section 3.2). We then deform cto roughly align it to the human body (Section 3.3).Lastly, we propose the ALIgnment-Aware Segment (ALIAS) normalization that removes the mislead- ing information in the misaligned area after deforming c.ALIAS generator ﬁlls the misaligned area with the clothing texture and maintains the clothing details (Section 3.4).3.1.Clothing-Agnostic Person Representation To train the model with pairs of candIalready wearing c, a person representation without the clothing information inIhas been utilized in the virtual try-on task.
Such rep- resentations have to satisfy the following conditions: (1) the original clothing item to be replaced should be deleted; (2) sufﬁcient information to predict the pose and the body shape of the person should be maintained; (3) the regions to be preserved ( e.g., face and hands) should be kept to main- tain the person’s identity. Problems of Existing Person Representations.In or- der to maintain the person’s shape, several approaches [10, 31, 36] provide a coarse body shape mask as a cue to syn- thesize the image, but fail to reproduce the body parts elab- orately ( e.g., hands).
To tackle this issue, ACGPN [35] em- ploys the detailed body shape mask as the input, and the neural network attempts to discard the clothing informa- መ𝑆 𝜃 Warp𝐼𝑎⨁𝑃⨁መ𝑆𝑐 𝑐𝒲(𝑐,𝜃) 𝑆𝑆𝑎 Remove Clothing & Arms 𝐼 𝑃 Remove Clothing & Arms 𝑆𝑎⨁𝑃⨁𝑐 መ𝐼(c) Clothes Deformation(a) Pre -processing (b) Segmentation Generation (d) Try -On Synthesis ALIAS GeneratorSegmentation Generator 𝐼𝑎 መ𝑆 Geometric Matching Module 𝐼𝑎⨁𝑃⨁𝒲(𝑐,𝜃)𝑀𝑚𝑖𝑠𝑎𝑙𝑖𝑔𝑛Figure 3: Overview of a VITON-HD. (a) First, given a reference image Icontaining a target person, we predict the seg- mentation map Sand the pose map P, and utilize them to pre-process IandSas a clothing-agnostic person image Ia and segmentation Sa.(b) Segmentation generator produces the synthetic segmentation ^Sfrom (Sa;P;c).(c) Geometric matching module deforms the clothing image caccording to the predicted clothing segmentation ^Scextracted from ^S.
(d) Finally, ALIAS generator synthesizes the ﬁnal output image ^Ibased on the outputs from the previous stages via our ALIAS normalization. tion to be replaced.However, since the body shape mask includes the shape of the clothing item, neither the coarse body shape mask nor the neural network could perfectly eliminate the clothing information.As a result, the original clothing item that is not completely removed causes prob- lems in the test phase.Clothing-Agnostic Person Representation.We pro- pose a clothing-agnostic image Iaand a clothing-agnostic segmentation map Saas inputs of each stage, which truly eliminate the shape of clothing item and preserve the body parts that need to be reproduced.We ﬁrst predict the segmentation map S2LHWand the pose map P2 R3HWof the image Iby utilizing the pre-trained net- works [7, 3], where Lis a set of integers indicating the se- mantic labels.
The segmentation map Sis used to remove the clothing region to be replaced and preserve the rest of the image. The pose map Pis utilized to remove the arms, but not the hands, as they are difﬁcult to reproduce.Based onSandP, we generate the clothing-agnostic image Ia and the clothing-agnostic segmentation map Sa, which al- low the model to remove the original clothing information thoroughly, and preserve the rest of the image.In addition, unlike other previous work, which adopts the pose heatmap with each channel corresponded to one keypoint, we con-catenateIaorSato the RGB pose map Prepresenting a skeletal structure that improves generation quality.3.2.Segmentation Generation Given the clothing-agnostic person representation (Sa;P), and the target clothing item c, the segmentation generatorGSpredicts the segmentation map ^S2LHW of the person in the reference image wearing c.
We train GSto learn the mapping between Sand(Sa;P;c), in which the original clothing item information is completely removed. As the architecture of GS, we adopt U-Net [25], and the total loss LSof the segmentation generator are written as LS=LcGAN +CELCE; (1) whereLCEandLcGAN denote the pixel-wise cross-entropy loss and conditional adversarial loss between ^SandS, re- spectively.CEis the hyperparameter corresponding to the relative importance between two losses.3.3.Clothing Image Deformation In this stage, we deform the target clothing item cto align it with ^Sc, which is the clothing area of ^S.We employ the geometric matching module proposed in CP- VTON [31] with the clothing-agnostic person representa- ConvALIAS ResBlkUpALIAS ResBlk ⋯ConvResize Resize Resize Resize ⋯ 1024x768መ𝐼 Conv ALIAS NormReLUConvALIAS NormReLUConv ALIAS NormReLUConvመ𝑆,𝑀𝑚𝑖𝑠𝑎𝑙𝑖𝑔𝑛 ℎ𝑖ℎ𝑖+1 𝑯i×𝑾𝒊Concat Add(b)ALIAS ResBlk 𝐼𝑎⨁𝑃⨁𝒲(𝑐,𝜃) (a)ALIAS GeneratorConv 𝑀𝑚𝑖𝑠𝑎𝑙𝑖𝑔𝑛መ𝑆𝐼𝑎⨁𝑃⨁𝑊(𝑐,𝜃)Figure 4: ALIAS generator.
(a) The ALIAS generator is composed of a series of ALIAS residual blocks, along with up- sampling layers. The input (Ia;P;W(c;))is resized and injected into each layer of the generator.(b) A detailed view of a ALIAS residual block.Resized (Ia;P;W(c;))is concatenated to hiafter passing through a convolution layer.Each ALIAS normalization layer leverages resized ^SandMmisalign to normalize the activation.tion(Ia;P)and^Scas inputs.A correlation matrix be- tween the features extracted from (Ia;P)andcis ﬁrst cal- culated .With the correlation matrix as an input, the regres- sion network predicts the TPS transformation parameters 2R255, and thencis warped by .In the training phase, the model takes Scextracted from Sinstead of ^Sc.The module is trained with the L1 loss between the warped clothes and the clothes Icthat is extracted from I.In addi- tion, the second-order difference constraint [35] is adopted to reduce obvious distortions in the warped clothing images from deformation.
The overall objective function to warp the clothes to ﬁt the human body is written as Lwarp =jjIc W(c;)jj1;1+constLconst; (2) whereWis the function that deforms cusing,Lconst is a second-order difference constraint, and const is the hyper- parameter forLconst . 3.4.Try-On Synthesis via ALIAS Normalization We aim to generate the ﬁnal synthetic image ^Ibased on the outputs from the previous stages.Overall, we fuse the clothing-agnostic person representation (Ia;P) and the warped clothing image W(c;), guided by ^S.(Ia;P;W(c;))is injected into each layer of the generator.For^S, we propose a new conditional normalization method called the ALIgnment-Aware Segment (ALIAS) normaliza- tion.ALIAS normalization enables the preservation of se- mantic information, and the removal of misleading infor- mation from the misaligned regions by leveraging ^Sand the mask of these regions.Alignment-Aware Segment Normalization.
Let us de- notehi2RNCiHiWias the activation of the i-th layer of a network for a batch of Nsamples, where Hi,Wi, and Ciindicate the height, width, and the number of channels ofhi, respectively. ALIAS normalization has two inputs: StandardizationConv Conv𝛽 𝛾 𝑀𝑚𝑖𝑠𝑎𝑙𝑖𝑔𝑛 −= 𝑀𝑎𝑙𝑖𝑔𝑛መ𝑆𝑐Conv መ𝑆𝑑𝑖𝑣Figure 5: ALIAS normalization.First, the activation is separately standardized according to the regions divided by Mmisalign , which can be obtained from the difference be- tween ^ScandMalign .Next, ^Sdivis convolved to create the modulation parameters and , and then the standardized activation is modulated with the parameters and .(1) the synthetic segmentation map ^S; (2) the misalign- ment binary mask Mmisalign2LHW, which excludes the warped mask of the target clothing image W(Mc;)from ^Sc(Mcdenotes the target clothing mask), i.e., Malign =^Sc\W(Mc;) (3) Mmisalign =^Sc Malign: (4) Fig.5 illustrates the workﬂow of the ALIAS normaliza- tion.We ﬁrst obtain Malign andMmisalign from Eq.
(3) and Eq. (4).We deﬁne the modiﬁed version of ^Sas^Sdiv, where ^Scin^Sseparates into Malign andMmisalign .ALIAS nor- malization standardizes the regions of Mmisalign and the other regions in hiseparately, and then modulates the stan- dardized activation using afﬁne transformation parameters 256192 512 384 1024 768 SSIM "LPIPS #FID # SSIM "LPIPS #FID # SSIM "LPIPS #FID # CP-VTON 0.739 0.159 56.23 0.791 0.141 31.96 0.786 0.158 43.28 ACGPN 0.842 0.064 26.45 0.863 0.067 15.22 0.856 0.102 43.39 VITON-HD* - - - - - - 0.893 0.054 12.47 VITON-HD 0.844 0.062 27.83 0.870 0.052 14.05 0.895 0.053 11.74 Table 1: Quantitative comparison with baselines across different resolutions.VITON-HD* is a VITON-HD variant where the standardization in ALIAS normalization is replaced by channel-wise standardization as in the original instance normalization.For the SSIM, higher is better.For the LPIPS and the FID, lower is better.inferred from ^Sdiv.
The activation value at site ( n2N;k2 Ci;y2Hi;x2Wi) is calculated by i k;y;x(^Sdiv)hi n;k;y;x i;m n;k i;m n;k+ i k;y;x(^Sdiv); (5) wherehi n;k;y;x is the activation at the site before normal- ization and i k;y;x and i k;y;x are the functions that convert ^Sdivto modulation parameters of the normalization layer. i;m n;kandi;m n;kare the mean and standard deviation of the activation in sample nand channel k.i;m n;kandi;m n;kare calculated by i;m n;k=1 j i;m njX (y;x)2 i;m nhi n;k;y;x (6) i;m n;k=vuut1 j i;m njX (y;x)2 i;m n(hi n;k;y;x i;m n;k)2;(7) where i;m ndenotes the set of pixels in region m, which is Mmisalign or the other region, and j i;m njis the number of pixels in i;m n.Similar to instance normalization [30], the activation is standardized per channel.However, ALIAS normalization divides the activation in channel kinto the activation in the misaligned region and the other region.
The rationale behind this strategy is to remove the mis- leading information in the misaligned regions. Speciﬁcally, the misaligned regions in the warped clothing image match the background that is irrelevant to the clothing texture.Per- forming a standardization separately on these regions leads to a removal of the background information that causes the artifacts in the ﬁnal results.In modulation, afﬁne pa- rameters inferred from the segmentation map modulate the standardized activation.Due to injecting semantic informa- tion at each ALIAS normalization layer, the layout of the human-parsing map in the ﬁnal result is preserved.ALIAS Generator.Fig.4 describes the overview of the ALIAS generator, which adopts the simpliﬁed architecture that discards the encoder part of an encoder-decoder net- work.The generator employs a series of residual blocks (ResBlk) with upsampling layers.Each ALIAS ResBlkconsists of three convolutional layers and three ALIAS nor- malization layers.
Due to the different resolutions that Res- Blks operate at, we resize the inputs of the normalization layers, ^SandMmisalign , before injecting them into each layer. Similarly, the input of the generator, (Ia;P;W(c;)), is resized to different resolutions.Before each ResBlk, the resized inputs (Ia;P;W(c;))are concatenated to the acti- vation of the previous layer after passing through a convolu- tion layer, and each ResBlk utilizes the concatenated inputs to reﬁne the activation.In this manner, the network per- forms the multi-scale reﬁnement at a feature level that bet- ter preserves the clothing details than a single reﬁnement at the pixel level.We train the ALIAS generator with the con- ditional adversarial loss, the feature matching loss, and the perceptual loss following SPADE [20] and pix2pixHD [32].Details of the model architecture, hyperparameters, and the loss function are described in the supplementary.4.Experiments 4.1.Experiment Setup Dataset.
We collected 1024 768 virtual try-on dataset for our research purpose, since the resolution of images on the dataset provided by Han et al. [10] was low.Speciﬁcally, we crawled 13,679 frontal-view woman and top clothing image pairs on an online shopping mall website.The pairs are split into a training and a test set with 11,647 and 2,032 pairs, respectively.We use the pairs of a person and a cloth- ing image to evaluate a paired setting, and we shufﬂe the clothing images for an unpaired setting.The paired setting is to reconstruct the person image with the original clothing item, and the unpaired setting is to change the clothing item on the person image with a different item.Training and Inference.With the goal of reconstruct- ingIfrom (Ia;c), the training of each stage proceeds in- dividually.During the training of the geometric matching module and the ALIAS generator, we use Sinstead of ^S.
While we aim to generate 1024 768 try-on images, we train the segmentation generator and the geometric match- ing module at 256 192. In the inference phase, after be- ing predicted by the segmentation generator at 256 192, the segmentation map is upscaled to 1024 768 and passed Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Segmentation (Ours) Figure 6: Qualitative comparison of the baselines.Target ClothesAgnostic (ACGPN)Output (ACGPN)Agnostic (Ours)Reference ImageOutput (Ours) Figure 7: Qualitative comparison of the segmentation gen- erator of ACGPN and VITON-HD.The clothing-agnostic segmentation map used by each model is also reported.to subsequent stages.Similarly, the geometric matching module predicts the TPS parameters at 256192, and the 1024768 clothing image deformed by the parameters is used in the ALIAS generator.We empirically found that this approach makes these two modules perform better with a lower memory cost than those trained at 1024 768.
De- tails of the model architecture and hyperparameters are de- scribed in the supplementary.4.2. Qualitative Analysis We compare VITON-HD with CP-VTON [31] and ACGPN [35], whose codes are publicly available.Follow- ing the training and inference procedure of our model, seg- mentation generators and geometric matching modules of the baselines are trained at 256 192, and the outputs from the modules are upscaled to 1024 768 during the inference.Comparison with Baselines.Fig.6 demonstrates that VITON-HD generates more perceptually convincing 1024768 images compared to the baselines.Our model clearly preserves the details of the target clothes, such as the logos and the clothing textures, due to the multi-scale reﬁnement at a feature level.In addition, regardless of what clothes the person is wearing in the reference image, our model synthesizes the body shape naturally.As shown in Fig.
7, the shape of the original clothing item remains in the synthetic segmentation map generated by ACGPN. On the other hand, the segmentation generator in VITON-HD suc- cessfully predicts the segmentation map regardless of the original clothing item, due to our newly proposed clothing- agnostic person representation.Although our model sur- passes the baselines qualitatively, there are a few limitations w/o ALIAS norm with ALIAS norm w/o ALIAS norm with ALIAS normFigure 8: Effects of ALIAS normalization.The orange colored areas in the enlarged images indicate the misaligned regions.0.040.060.080.10.120.140.160.18 Small Medium LargeLPIPS Misaligned AreaCP-VTON ACGPN OursFigure 9: LPIPS scores according to the degree of misalignment.to VITON-HD, which are reported in the supplementary with the additional qualitative results.Effectiveness of the ALIAS Normalization.
We study the effectiveness of ALIAS normalization by comparing our model to VITON-HD*, where the standardization in ALIAS normalization is replaced by channel-wise standard- ization, as in the original instance normalization [30]. Fig.8 shows that ALIAS normalization has the capability to ﬁll the misaligned areas with the target clothing texture by re- moving the misleading information.On the other hand, without utilizing ALIAS normalization, the artifacts are produced in the misaligned areas, because the background information in the warped clothing image is not removed as described in Section 3.4.ALIAS normalization, however, can handle the misaligned regions properly.4.3.Quantitative Analysis We perform the quantitative experiments in both a paired and an unpaired settings, in which a person wears the origi- nal clothes or the new clothes, respectively.We evaluate our method using three metrics widely used in virtual try-on.
The structural similarity (SSIM) [33] and the learned per- ceptual image patch similarity (LPIPS) [38] are used in the paired setting, and the frechet inception distance (FID) [11] score is adopted in the unpaired setting. The inception score [26] is not included in the experiments, since it cannot reﬂect whether the details of the clothing image are main- tained [10].The input of the each model contains different amount of information that offers advantages in reconstruct- ing the segmentation maps, thus we use the segmentation maps from the test set instead of the synthetic segmentation maps in the paired setting.Comparison across Different Resolutions.We com- pare the baselines quantitatively across different resolutions (256192, 512384, and 1024768) as shown in Table 1.Our model outperforms the baselines for SSIM and LPIPS across all resolutions.For FID score, our model signiﬁ- cantly surpasses CP-VTON, regardless of the resolutions.
The FID score in ACGPN is slightly lower than that of our model at the 256 192 resolution. However, at the1024768 resolution, our model achieves a lower FID score than ACGPN with a large margin.The results indicate that the baselines cannot handle 1024 768 images, while our model is trained in a stable manner, even at a high resolu- tion.This may be due to the limited capability of the U-Net architecture employed in the baseline models.Comparison According to the Degree of Misalign- ment.To verify the ability of ﬁlling the misaligned areas with the clothing texture, we perform experiments in the paired setting according to the degree of the misalignment.According to the number of pixels in the misaligned areas, we divide the test dataset in three types: small, medium, and large.For a fair comparison, each model uses the same seg- mentation maps and the same warped clothes as inputs to match the misaligned regions.
We evaluate LPIPS to mea- sure the semantic distances between the reference images and the reconstructed images. As shown in Fig.9, the wider the misaligned areas, the worse the performance of models, which means that the misalignment hinders the models from generating photo-realistic virtual try-on images.Compared to the baselines, our model consistently performs better, and the performance of our model decreases less as the degree of misalignment increases.5.Conclusions We propose the VITON-HD that synthesizes photo- realistic 1024768 virtual try-on images.The proposed ALIAS normalization can properly handle the misaligned areas and propagate the semantic information throughout the ALIAS generator, which preserves the details of the clothes via the multi-scale reﬁnement.Qualitative and quantitative experiments demonstrate that VITON-HD sur- passes existing virtual try-on methods with a large margin.Acknowledgments.
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. NRF-2019R1A2C4070420) and Seoul R&BD Program (CD200024) through the Seoul Business Agency (SBA) funded by the Seoul Metropolitan Government.References [1] Ivan Anokhin, Pavel Solovev, Denis Korzhenkov, Alexey Kharlamov, Taras Khakhulin, Aleksei Silvestrov, Sergey Nikolenko, Victor Lempitsky, and Gleb Sterkin.High- resolution daytime translation without domain labels.In Proc.of the IEEE conference on computer vision and pat- tern recognition (CVPR) , pages 7488–7497, 2020.3 [2] Andrew Brock, Jeff Donahue, and Karen Simonyan.Large scale gan training for high ﬁdelity natural image synthesis.In Proc.the International Conference on Learning Representa- tions (ICLR) , 2018.3 [3] Z Cao, T Simon, SE Wei, YA Sheikh, et al.Openpose: Realtime multi-person 2d pose estimation using part afﬁn- ity ﬁelds.
The IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2019. 4 [4] Harm De Vries, Florian Strub, J ´er´emie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C Courville.Modu- lating early visual processing by language.In Proc.the Ad- vances in Neural Information Processing Systems (NeurIPS) , pages 6594–6604, 2017.3 [5] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bochao Wang, Hanjiang Lai, Jia Zhu, Zhiting Hu, and Jian Yin.Towards multi-pose guided virtual try-on network.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 9026–9035, 2019.3 [6] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bowen Wu, Bing-Cheng Chen, and Jian Yin.Fw-gan: Flow-navigated warping gan for video virtual try-on.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 1161–1170, 2019.2 [7] Ke Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, Ming Yang, and Liang Lin.Instance-level human parsing via part grouping network.In Proc.
of the European Conference on Computer Vision (ECCV) , pages 770–785, 2018. 4 [8] Peng Guan, Loretta Reiss, David A Hirshberg, Alexander Weiss, and Michael J Black.Drape: Dressing any person.ACM Transactions on Graphics (TOG) , 31(4):1–10, 2012.3 [9] Xintong Han, Xiaojun Hu, Weilin Huang, and Matthew R Scott.Clothﬂow: A ﬂow-based model for clothed person generation.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 10471–10480, 2019.2, 3, 13 [10] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry S Davis.Viton: An image-based virtual try-on network.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 7543–7552, 2018.2, 3, 6, 8 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.Gans trained by a two time-scale update rule converge to a local nash equilib- rium.In Proc.the Advances in Neural Information Process- ing Systems (NeurIPS) , pages 6629–6640, 2017.
8 [12] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 1501–1510, 2017.3 [13] Sergey Ioffe and Christian Szegedy.Batch normalization: Accelerating deep network training by reducing internal co-variate shift.In Proc.the International Conference on Ma- chine Learning (ICML) , pages 448–456, 2015.3 [14] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.Image-to-image translation with conditional adversar- ial networks.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 1125–1134, 2017.3 [15] Nikolay Jetchev and Urs Bergmann.The conditional analogy gan: Swapping fashion articles on people images.In Proc.of the IEEE international conference on computer vision work- shop (ICCVW) , pages 2287–2292, 2017.3 [16] Diederik P Kingma and Jimmy Ba.Adam: A method for stochastic optimization.arXiv:1412.6980 , 2014.
13 [17] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks.In Proc.of the IEEE international con- ference on computer vision (ICCV) , pages 2794–2802, 2017.12 [18] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.Spectral normalization for generative ad- versarial networks.In Proc.the International Conference on Learning Representations (ICLR) , 2018.11 [19] Augustus Odena, Christopher Olah, and Jonathon Shlens.Conditional image synthesis with auxiliary classiﬁer gans.InProc.the International Conference on Machine Learning (ICML) , pages 2642–2651, 2017.3 [20] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.Semantic image synthesis with spatially-adaptive nor- malization.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 2337–2346, 2019.3, 6, 12 [21] Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A.
Efros, and Richard Zhang. Swapping autoencoder for deep image manipulation.In Proc.the Ad- vances in Neural Information Processing Systems (NeurIPS) , 2020.3 [22] Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons- Moll.Tailornet: Predicting clothing in 3d as a function of hu- man pose, shape and garment style.In Proc.of the IEEE con- ference on computer vision and pattern recognition (CVPR) , pages 7365–7375, 2020.3 [23] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black.Clothcap: Seamless 4d clothing capture and retar- geting.ACM Transactions on Graphics (TOG) , 36(4):1–15, 2017.3 [24] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo- geswaran, Bernt Schiele, and Honglak Lee.Generative adversarial text to image synthesis.In Proc.the Inter- national Conference on Machine Learning (ICML) , pages 1060–1069, 2016.3 [25] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.U-net: Convolutional networks for biomedical image segmentation.
InInternational Conference on Medical Image Computing and Computer Assisted Intervention , pages 234–241, 2015. 2, 4, 11 [26] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.Improved techniques for training gans.In Proc.the Advances in Neural Informa- tion Processing Systems (NeurIPS) , pages 2234–2242, 2016.8 [27] Masahiro Sekine, Kaoru Sugita, Frank Perbet, Bj ¨orn Stenger, and Masashi Nishiyama.Virtual ﬁtting by single-shot body shape estimation.In International Conference on 3D Body Scanning Technologies , pages 406–413, 2014.3 [28] Wei Shen and Rujie Liu.Learning residual images for face attribute manipulation.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 4030–4038, 2017.3 [29] Karen Simonyan and Andrew Zisserman.Very deep convo- lutional networks for large-scale image recognition.arXiv preprint arXiv:1409.1556 , 2014.13 [30] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
In- stance normalization: The missing ingredient for fast styliza- tion. arXiv preprint arXiv:1607.08022 , 2016.3, 6, 8 [31] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang.Toward characteristic- preserving image-based virtual try-on network.In Proc.of the European Conference on Computer Vision (ECCV) , pages 589–604, 2018.2, 3, 4, 7, 13 [32] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.High-resolution image syn- thesis and semantic manipulation with conditional gans.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 8798–8807, 2018.2, 3, 6, 11, 12 [33] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si- moncelli.Image quality assessment: from error visibility to structural similarity.IEEE Transactions on Image Process- ing, 13(4):600–612, 2004.8 [34] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.
Attngan: Fine- grained text to image generation with attentional genera- tive adversarial networks. In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 1316–1324, 2018.3 [35] Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wang- meng Zuo, and Ping Luo.Towards photo-realistic virtual try-on by adaptively generating-preserving image content.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 7850–7859, 2020.2, 3, 5, 7, 12, 13 [36] Ruiyun Yu, Xiaoqi Wang, and Xiaohui Xie.Vtnfp: An image-based virtual try-on network with body and cloth- ing feature preservation.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 10511–10520, 2019.2, 3 [37] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus- tus Odena.Self-attention generative adversarial networks.InProc.the International Conference on Machine Learning (ICML) , pages 7354–7363, 2019.
13 [38] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric.In Proc.of the IEEE con- ference on computer vision and pattern recognition (CVPR) , 2018.8[39] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka.Sean: Image synthesis with semantic region-adaptive nor- malization.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 5104–5113, 2020.3 Supplementary Material A.Implementation Details A.1.Pre-processing Details This section introduces the details of generating our clothing-agnostic person representation.To remove the de- pendency on the clothing item originally worn by a person, regions that can provide any original clothing information, such as the arms that hint at the sleeve length, should be eliminated.Therefore, when generating a clothing-agnostic imageIa, we remove the arms from the reference image I.
For the same reason, legs should be removed if the pants are the target clothing items. We mask the regions with a gray color, so that the masked pixels of the normalized im- age would have a value of 0.We add padding to the masks to thoroughly remove these regions, and the width of the padding is empirically determined.A.2.Model Architectures This section introduces the architectures of the seg- mentation generator, the geometric matching module, and ALIAS generator in detail.Segmentation Generator.The segmentation generator has the structure of U-Net [25], which consists of convolu- tional layers, downsampling layers, and upsampling layers.Two multi-scale discriminators [32] are employed for the conditional adversarial loss.The details of the segmenta- tion generator architecture are shown in Fig.10.Geometric Matching Module.The geometric matching module consists of two feature extractors and a regression network.
A correlation matrix is calculated from the two extracted features, and the regression network predicts the TPS parameter with the correlation matrix. The feature extractor is composed of a series of convolutional layers, and the regression network consists of a series of convolu- tional layers followed by a fully connected layer.The de- tails are shown in Fig.11.ALIAS Generator.The architecture of the ALIAS gen- erator consists of a series of ALIAS ResBlks with nearest- neighbor upsampling layers.We employ two multi-scale discriminators with instance normalization.Spectral nor- malization [18] is applied to all the convolutional layers.Note that we separately standardize the activation based on the misalignment mask Mmisalign only in the ﬁrst ﬁveALIAS ResBlks.The details of the ALIAS generator archi- tecture is shown in Fig.12.A.3.
Training Details This section introduces the losses and the hyperparame- ters for the segmentation generator, the geometric matching module, and the ALIAS generator. Segmentation Generator.The segmentation genera- torGSuses the clothing-agnostic segmentation map Sa, the pose map P, and the clothing item cas inputs ( ^S= GS(Sa;P;c)) to predict the segmentation map ^Sof the person in the reference image wearing the target cloth- ing item.The segmentation generator is trained with the ConvBlk (64), MaxPool (2) ConvBlk (128), MaxPool (2) ConvBlk (256), MaxPool (2) ConvBlk (512), MaxPool (2) ConvBlk (1024) Upsample (2), ConvBlk (512) Upsample (2), ConvBlk (256) Upsample (2), ConvBlk (128) Upsample (2), ConvBlk (64) 3x3 Conv (13), Softmax Figure 10: Segmentation Generator.kkConv (x) denotes a convolutional layer where the kernel size is kand the out- put channel is x.
Also, ConvBlk ( x) denotes a block, which consists of two series of 3 3 convolutional layer, instance normalization, and ReLU activation. 4x4-↓2 Conv (64), BN, ReLU 4x4-↓2 Conv (64), BN, ReLU 4x4-↓2 Conv (128), BN, ReLU 4x4-↓2 Conv (128), BN, ReLU 4x4-↓2 Conv (256), BN, ReLU 4x4-↓2 Conv (256), BN, ReLU 4x4-↓2 Conv (512), BN, ReLU 4x4-↓2 Conv (512), BN, ReLU 3x3 Conv (512), BN, ReLU 3x3 Conv (512), BN, ReLU 4x4-↓2 Conv (512), BN, ReLU 4x4-↓2 Conv (256), BN, ReLU 3x3 Conv (128), BN, ReLU 3x3 Conv (64), BN, ReLU Linear (2x5x5), Tanh 𝜃3x3 Conv (512), ReLU 3x3 Conv (512), ReLUFigure 11: Geometric Matching Module.kk#2 Conv (x) denotes a convolutional layer where the kernel size is k, the stride is 2, and the output channel is x.cross-entropy lossLCEand the conditional adversarial loss LcGAN , which is LSGAN loss [17].
The full loss LSfor the segmentation generator are written as LS=LcGAN +CELCE (8) LCE= 1 HWX k2C;y2H;x2WSk;y;xlog(^Sk;y;x) (9) LcGAN =E(X;S)[log(D(X;S))] +EX[1 log(D(X;^S))];(10) whereCEis the hyperparameter for the cross-entropy loss. In the experiment, CEis set to 10.In Eq.(9), Syxkand ^Syxkindicate the pixel values of the segmentation map of the reference image Sand^Scorresponding to the coordi- nates (x;y)in channelk.The symbols H,WandCindi- cate the height, width, and the number of channels of S.In Eq.(10), the symbol Xindicates the inputs of the generator (Sa;P;c ), andDdenotes the discriminator.The learning rate of the generator and the discriminator is 0.0004.We adopt the Adam optimizer with 1= 0:5 and 2= 0:999.We train the segmentation generator for 200,000 iterations with the batch size of 8.Geometric Matching Module.The inputs of the geo- metric matching module are c,P, clothing-agnostic image Ia, and ^Sc, which is the clothing area of ^S.
The output is 3x3 Conv (1024) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (512), Upsample (2) ALIAS ResBlk (256), Upsample (2) ALIAS ResBlk (128), Upsample (2) ALIAS ResBlk (64), Upsample (2) ALIAS ResBlk (32) 3x3 Conv (3), Tanh Figure 12: ALIAS Generator. The segmentation map Sand the misalignment mask Mmisalign are passed to the gener- ator through the proposed ALIAS ResBlks.the TPS transformation parameters .The overall objective function is written as Lwarp =jjIc W(c;)jj1;1+constLconst (11) Lconst =X p2Pj(jjjpp0jj2 jjpp1jj2j+jjjpp2jj2 jjpp3jj2j) +(jS(p;p0) S(p;p1)j+jS(p;p2) S(p;p3)j); (12) whereWis the function that deforms cusing, andIcis the clothing item extracted from the reference image I.Lconst is a second-order difference constraint [35], and const is the hyperparameter for Lconst .In the experiment, we set const to 0.04.In Eq.
(12), the symbol pindicates a sam- pled TPS control point from the entire control points set P, andp0,p1,p2, andp3are top, bottom, left and right point ofp, respectively. The function S(p;pi)denotes the slope betweenpandpi.The learning rate of the geometric matching module is 0.0002.We adopt the Adam optimizer with 1= 0:5and 2= 0:999.We train the geometric matching module for 50,000 iterations with the batch size of 8.ALIAS Generator.The loss function of ALIAS genera- tor follows those of SPADE [20] and pix2pixHD [32], as it Grid 5x5 Grid 10x10 Grid 20x20 ClothFlow Figure 13: Qualitative comparisons of TPS transformation with various grid numbers and the ﬂow estimation from ClothFlow.Method Warp-SSIM"MACs#Mask-SSIM" ClothFlow 0.841?8.13G 0.803?VITON-HD 0.782 4.47G 0.852 Table 2:?denotes a score taken from the ClothFlow paper, and we train VITON-HD in the same setting ( e.g., dataset and resolution).We compute MACs of their warping mod- ules at 256192.
contains the conditional adversarial loss LcGAN , the feature matching lossLFM, and the perceptual loss Lpercept . Let DIbe the discriminator, Iandcbe the given reference and target clothing images, and ^Ibe the synthetic image gener- ated by the generator.Sdivis the modiﬁed version of the segmentation map S.The full lossLIof our generator is written as LI=LcGAN +FMLFM+perceptLpercept (13) LcGAN =EI[log(DI(Sdiv;I))] +E(I;c)[1 log(DI(Sdiv;^I))](14) LFM=E(I;c)TX i=11 Ki[jjD(i) I(Sdiv;I) D(i) I(Sdiv;^I)jj1;1] (15) Lpercept =E(I;c)VX i=11 Ri[jjF(i)(I) F(i)(^I)jj1;1];(16) whereFMandpercept are hyperparameters.In the ex- periment, both FMandpercept are set to 10.Tis the number of layers in DI, andD(i) IandKiare the activa- tion and the number of elements in the i-th layer of DI, respectively.Similarly, Vis the number of layers used in the VGG network F[29], andF(i)andRiare the acti- vation and the number of elements in the i-th layer of F, respectively.
We replace the standard adversarial loss with the Hinge loss [37]. The learning rate of the generator and the discriminator is 0.0001 and 0.0004, respectively.We adopt the Adam op- timizer [16] with 1= 0and 2= 0:9.We train the ALIAS generator for 200,000 iterations with the batch size of 4.B.Additional Experiments B.1.Comparison with ClothFlow To demonstrate that the optical ﬂow estimation does not solve the misalignment completely, we re-implement the ﬂow estimation module of ClothFlow [9] based on the orig- inal paper.Fig.13 shows that the misalignment still oc- curs, although both TPS with a higher grid number (e.g., a 1010 or 2020 grid) and the ﬂow estimation module of ClothFlow can reduce the misaligned regions.The reason is that the regularization to avoid the artifacts ( e.g., TV loss) prevents the warped clothes from ﬁtting perfectly into the target region.
In addition, we evaluate the accuracy and the computational cost of warping modules in VITON-HD and ClothFlow with Warp-SSIM [9] and MACs, respectively. We also measure how well the models reconstruct the cloth- ing using Mask-SSIM [9].Table 2 shows that the ClothFlow warping module has the better accuracy than ours, whereas the higher Mask-SSIM in VITON-HD proves that ALIAS normalization is more effective at solving the misalignment problem than the improved warping method.We found that the ClothFlow warping module needs a huge computational cost (MACs: 130.03G) at 1024 768, but the cost could be reduced when predicting the optical ﬂow map at 256 192.Table 2 demonstrates that the ClothFlow warping module still needs more computational cost than ours, yet it is a viable option to combine the ﬂow estimation module with ALIAS generator.
0.960.010.03 0.020.240.74 0.020.750.23 0% 20% 40% 60% 80% 100%OursACGPNCP-VTONQuality and Realism (%) Top1 Top2 Top3 0.880.030.09 0.10.280.62 0.020.690.29 0% 20% 40% 60% 80% 100%OursACGPNCP-VTONPreservation of Clothing Details (%) Top1 Top2 Top3 Figure 14: User study results. We compare our model with CP-VTON [31] and ACGPN [35].Figure 15: Failure cases of VITON-HD.B.2.User Study We further evaluate our model and other baselines via a user study in the unpaired setting.We randomly select 30 sets of a reference image and a target clothing image from the test dataset.Given the reference images and the target clothes, the users are asked to rank the 1024 768 outputs of our model and baselines according to the follow- ing questions: (1) Which image is the most photo-realistic?(2) Which image preserves the details of the target cloth- ing the most?As shown in Fig.14, it can be observed that our approach achieves the rank 1 votes more than 88% for the both questions.
The result demonstrates that our model generates more realistic images, and preserves the details of the clothing items compared to the baselines. B.3.Qualitative Results We provide additional qualitative results to demonstrate our model’s capability of handling high quality image syn- thesis.Fig.16, 17, 18, and 19 show the qualitative com- parison of the baselines across different resolutions.Fig.20, 21, 22, and 23 show additional results of VITON-HD at 1024768 resolution.C.Failure Cases and Limitations Fig.15 shows the failure cases of our model caused by the inaccurately predicted segmentation map or the inner collar region indistinguishable from the other clothing re- gion.Also, the boundaries of the clothing textures occa- sionally fade away.The limitations of our model are as follows.VITON-HD is trained to preserve the bottom clothing items, limiting the presentation of the target clothes ( e.g., whether they are tucked in).
It can be a valuable future direction to gener- ate multiple possible outputs from a single input pair. Next, our dataset mostly consists of slim women and top cloth- ing images, which makes VITON-HD handle only a limited range of body shapes and clothing during the inference.We believe that VITON-HD has the capability to cover more diverse cases when the images of various body shapes and clothing types are provided.Finally, existing virtual try-on methods including VITON-HD do not provide robust per- formance for in-the-wild images.We think generating real- istic try-on images for the in-the-wild images is an interest- ing topic for future work.Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Figure 16: Qualitative comparison of the baselines (256 192).Reference Image ACGPN VITON -HD (Ours) Target Clothes CP-VTON Figure 17: Qualitative comparison of the baselines (512 384).
Figure 18: Qualitative comparison of the baselines (1024 768). Figure 19: Qualitative comparison of the baselines (1024 768).Reference ImageTarget ClothesFigure 20: Additional qualitative results of VITON-HD.Figure 21: Sample 1 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.Figure 22: Sample 2 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.Figure 23: Sample 3 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.
Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.eduŁukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the ﬁrst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efﬁcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the ﬁnal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information ﬂow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and ﬁxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneﬁt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiﬁcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the ﬁrst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conﬁguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of ﬂoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision ﬂoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and Jürgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735–1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] Łukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio Łukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] Oﬁr Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overﬁtting.Journal of Machine Learning Research , 15(1):1929–1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D. D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440–2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google’s neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11
﻿Instruction,Input,Response 고객이 요금제 변경을 요청함,현재 5G 요금제를 사용 중인데 LTE 요금제로 변경하고 싶어요.,"고객님, 현재 사용 중인 5G 요금제를 LTE 요금제로 변경하시려면, 가까운 SKT 대리점을 방문하시거나 T월드 앱에서 변경 가능합니다.원하시는 요금제를 말씀해 주시면 자세히 안내해 드리겠습니다." 고객이 데이터 사용량을 조회하고 싶어함,이번 달 데이터 사용량이 얼마나 되나요?,"고객님, 현재까지 사용하신 데이터는 8.5GB이며, 남은 데이터는 1.5GB입니다.추가로 데이터가 필요하시면 데이터 쿠폰을 구매하실 수도 있습니다." 고객이 해외 로밍 요금제를 문의함,"다음 주에 일본으로 여행 가는데, 로밍 요금제 어떻게 되나요?","고객님, 일본에서 사용 가능한 로밍 요금제는 '바로 로밍 300MB(9,900원/일)' 또는 '로밍 무제한(13,900원/일)' 등이 있습니다.더 자세한 사항은 T월드에서 확인 가능합니다." 고객이 결제일 변경을 요청함,매월 요금 결제일을 바꾸고 싶은데 가능한가요?,"네, 고객님.요금 결제일 변경은 매월 1일, 15일, 25일 중 선택 가능합니다.원하시는 날짜를 말씀해 주시면 변경 도와드리겠습니다." 고객이 기기 할부금 잔액을 확인하려 함,현재 사용 중인 휴대폰 할부금이 얼마나 남았나요?,"고객님, 현재 단말기 할부금은 12개월 남아 있으며, 남은 금액은 총 360,000원입니다.조기 완납을 원하시면 T월드에서 납부 가능합니다." 고객이 가족 결합 할인 혜택을 문의함,"가족 결합 할인을 받고 싶은데, 어떻게 하면 되나요?","고객님, 가족 결합 할인은 가족 구성원 2인 이상이 SKT 회선을 사용 중일 경우 가능합니다.가족관계 증빙서류를 준비하셔서 가까운 대리점을 방문해 주시면 신청 도와드리겠습니다." 고객이 개통 가능한 스마트폰 기종을 문의함,아이폰 15 개통 가능한가요?,"고객님, 현재 아이폰 15는 정상 개통 가능하며, 색상 및 저장 용량에 따라 재고가 다를 수 있습니다.
가까운 SKT 대리점에 방문하시면 즉시 개통 가능합니다." 고객이 데이터 쿠폰 구매 방법을 문의함,데이터 1GB 추가하려면 어떻게 해야 하나요?,"고객님, 데이터 1GB 추가는 T월드 앱 또는 SKT 고객센터(114)에서 구매 가능합니다. 1GB 데이터 쿠폰은 5,500원에 제공되며, 즉시 적용됩니다." 고객이 5G 속도 저하 문제를 제기함,5G인데 속도가 너무 느려요.왜 그런가요?,"고객님, 현재 계신 지역의 네트워크 환경에 따라 속도가 저하될 수 있습니다.잠시 후에도 동일한 문제가 발생하면 SKT 고객센터로 문의 주시면 신속히 점검 도와드리겠습니다." 고객이 번호 변경을 요청함,현재 전화번호를 바꾸고 싶은데 어떻게 하면 되나요?,"고객님, 번호 변경은 SKT 대리점 방문 또는 고객센터를 통해 가능합니다.번호 변경 시 기존 번호로 걸려오는 전화는 일정 기간 안내 메시지가 제공됩니다." 고객이 2G 서비스 종료에 대해 문의함,"2G 서비스가 종료된다고 하던데, 어떻게 해야 하나요?","고객님, SKT 2G 서비스는 종료되었으며, 3G, LTE, 5G 기기로 기기 변경을 권장드립니다.보상 프로그램도 진행 중이니 자세한 사항은 고객센터로 문의 부탁드립니다." 고객이 분실폰 위치 추적을 요청함,핸드폰을 잃어버렸는데 위치 추적이 가능할까요?,"고객님, '내 디바이스 찾기' 기능을 이용하시면 위치 추적이 가능합니다.추가로 고객센터를 통해 분실 신고를 하시면 임시 정지 및 보호 조치를 도와드릴 수 있습니다." 고객이 멤버십 혜택을 문의함,SKT VIP 멤버십 혜택에는 어떤 것이 있나요?,"고객님, SKT VIP 멤버십 혜택으로 영화 할인, 카페 무료 음료, 데이터 리필 쿠폰 제공 등이 있습니다.자세한 내용은 T월드 앱에서 확인 가능합니다."
Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.eduŁukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the ﬁrst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efﬁcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the ﬁnal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information ﬂow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and ﬁxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneﬁt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiﬁcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the ﬁrst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conﬁguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of ﬂoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision ﬂoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and Jürgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735–1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] Łukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio Łukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] Oﬁr Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overﬁtting.Journal of Machine Learning Research , 15(1):1929–1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D. D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440–2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google’s neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11
Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.eduŁukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the ﬁrst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efﬁcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the ﬁnal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information ﬂow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and ﬁxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneﬁt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiﬁcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the ﬁrst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conﬁguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of ﬂoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision ﬂoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and Jürgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735–1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] Łukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio Łukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] Oﬁr Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overﬁtting.Journal of Machine Learning Research , 15(1):1929–1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D. D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440–2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google’s neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11
VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization Seunghwan Choi*Sunghyun Park*Minsoo Lee*Jaegul Choo KAIST, Daejeon, South Korea fshadow2496, psh01087, alstn2022, jchoo g@kaist.ac.kr Reference Image Synthetic Image Figure 1: Given a pair of a reference image (containing a person) and a target clothing image, our method successfully synthesizes 1024768 virtual try-on images.Abstract The task of image-based virtual try-on aims to transfer a target clothing item onto the corresponding region of a person, which is commonly tackled by ﬁtting the item to the desired body part and fusing the warped item with the person.While an increasing number of studies have been conducted, the resolution of synthesized images is still lim- ited to low ( e.g., 256192), which acts as the critical lim- itation against satisfying online consumers.
We argue that the limitation stems from several challenges: as the resolu- tion increases, the artifacts in the misaligned areas between the warped clothes and the desired clothing regions become noticeable in the ﬁnal results; the architectures used in ex- *These authors contributed equally.isting methods have low performance in generating high- quality body parts and maintaining the texture sharpness of the clothes. To address the challenges, we propose a novel virtual try-on method called VITON-HD that successfully synthesizes 1024768 virtual try-on images.Speciﬁcally, we ﬁrst prepare the segmentation map to guide our virtual try-on synthesis, and then roughly ﬁt the target clothing item to a given person’s body.Next, we propose ALIgnment- Aware Segment (ALIAS) normalization and ALIAS genera- tor to handle the misaligned areas and preserve the details of 1024768 inputs.
Through rigorous comparison with ex- isting methods, we demonstrate that VITON-HD highly sur- passes the baselines in terms of synthesized image quality both qualitatively and quantitatively. Code is available at https://github.com/shadow2496/VITON-HD .arXiv:2103.16874v2 [cs.CV] 10 Sep 2021 1.Introduction Image-based virtual try-on refers to the image generation task of changing the clothing item on a person into a differ- ent item, given in a separate product image.With a growing trend toward online shopping, virtually wearing the clothes can enrich a customer’s experience, as it gives an idea about how these items would look on them.Virtual try-on is similar to image synthesis, but it has unique and challenging aspects.Given images of a person and a clothing product, the synthetic image should meet the following criteria: (1) The person’s pose, body shape, and identity should be preserved.
(2) The clothing prod- uct should be naturally deformed to the desired clothing re- gion of the given person, by reﬂecting his/her pose and body shape. (3) Details of the clothing product should be kept in- tact.(4) The body parts initially occluded by the person’s clothes in the original image should be properly rendered.Since the given clothing image is not initially ﬁtted to the person image, fulﬁlling these requirements is challenging, which leaves the development of virtual try-on still far be- hind the expectations of online consumers.In particular, the resolution of virtual try-on images is low compared to the one of normal pictures on online shopping websites.After Han et al.[10] proposed VITON, various image- based virtual try-on methods have been proposed [31, 36, 35, 6].
These methods follow two processes in common: (1) warping the clothing image initially to ﬁt the human body; (2) fusing the warped clothing image and the image of the person that includes pixel-level reﬁnement. Also, several recent methods [9, 36, 35] add a module that generates seg- mentation maps and determine the person’s layout from the ﬁnal image in advance.However, the resolution of the synthetic images from the previous methods is low ( e.g., 256192) due to the follow- ing reasons.First, the misalignment between the warped clothes and a person’s body results in the artifacts in the misaligned regions, which become noticeable as the image size increases.It is difﬁcult to warp clothing images to ﬁt the body perfectly, so the misalignment occurs as shown in Fig.2.Most of previous approaches utilize the thin-plate spline (TPS) transformation to deform clothing images.
To accurately deform clothes, ClothFlow [9] predicts the op- tical ﬂow maps of the clothes and the desired clothing re- gions. However, the optical ﬂow maps does not remove the misalignment completely on account of the regulariza- tion.In addition, the process requires more computational costs than other methods due to the need of predicting the movement of clothes at a pixel level.(The detailed analysis of ClothFlow is included in the supplementary.) Second, a simple U-Net architecture [25] used in existing approaches is insufﬁcient in synthesizing initially occluded body parts in ﬁnal high-resolution ( e.g., 1024768) images.As noted in Wang et al.[32], applying a simple U-Net-based archi- (c)Misaligned Regions (b)Warped Clothes on Segmentation(a)Warped Clothes on Reference ImageFigure 2: An example of misaligned regions.tecture to generate high-resolution images leads to unstable training as well as unsatisfactory quality of generated im- ages.
Also, reﬁning the images once at the pixel level is in- sufﬁcient in preserving the details of high-resolution cloth- ing images. To address the above-mentioned challenges, we pro- pose a novel high-resolution virtual try-on method, called VITON-HD.In particular, we introduce a new clothing- agnostic person representation that leverages the pose in- formation and the segmentation map so that the clothing information is eliminated thoroughly.Afterwards, we feed the segmentation map and the clothing item deformed to ﬁt the given human body to the model.Using the addi- tional information, our novel ALIgnment-Aware Segment (ALIAS) normalization removes information irrelevant to the clothing texture in the misaligned regions and propa- gates the semantic information throughout the network.The normalization separately standardizes the activations corre- sponding to the misaligned regions and the other regions, and modulates the standardized activations using the seg- mentation map.
Our ALIAS generator employing ALIAS normalization synthesizes the person image wearing the tar- get product while ﬁlling the misaligned regions with the clothing texture and preserving the details of the clothing item through the multi-scale reﬁnement at a feature level. To validate the performance of our framework, we col- lected a 1024768 dataset that consists of pairs of a per- son and a clothing item for our research purpose.Our ex- periments demonstrate that VITON-HD signiﬁcantly out- performs the existing methods in generating 1024 768 im- ages, both quantitatively and qualitatively.We also conﬁrm the superior capability of our novel ALIAS normalization module in dealing with the misaligned regions.We summarize our contributions as follows: • We propose a novel image-based virtual try-on ap- proach called VITON-HD, which is, to the best of our knowledge, the ﬁrst model to successfully synthesize 1024768 images.
• We introduce a clothing-agnostic person representa- tion that allows our model to remove the dependency on the clothing item originally worn by the person. • To address the misalignment between the warped clothes and the desired clothing regions, we propose ALIAS normalization and ALIAS generator, which is effective in maintaining the details of clothes.• We demonstrate the superior performance of our method through experiments with baselines on the newly collected dataset.2.Related Work Conditional Image Synthesis.Conditional generative adversarial networks (cGANs) utilize additional informa- tion, such as class labels [19, 2], text [24, 34], and at- tributes [28], to steer the image generation process.Since the emergence of pix2pix [14], numerous cGANs condi- tioned on input images have been proposed to generate high-resolution images in a stable manner [32, 1, 21].
How- ever, these methods tend to generate blurry images when handling a large spatial deformation between the input im- age and the target image. In this paper, we propose a method that can address the spatial deformation of input images and properly generate 1024 768 images.Normalization Layers.Normalization layers [13, 30] have been widely applied in modern deep neural networks.Normalization layers, whose afﬁne parameters are esti- mated with external data, are called conditional normaliza- tion layers.Conditional batch normalization [4] and adap- tive instance normalization [12] are such conditional nor- malization techniques and have been used in style transfer tasks.SPADE [20] and SEAN [39] utilize segmentation maps to apply spatially varying afﬁne transformations.Us- ing the misalignment mask as external data, our proposed normalization layer computes the means and the variances of the misaligned area and the other area within an instance separately.
After standardization, we modulate standard- ized activation maps with afﬁne parameters inferred from human-parsing maps to preserve semantic information. Virtual Try-On Approaches.There are two main cat- egories for virtual try-on approaches: 3D model-based approaches [8, 27, 23, 22] and 2D image-based ap- proaches [10, 31, 9, 36, 35, 5].3D model-based approaches can accurately simulate the clothes but are not widely appli- cable due to their dependency on 3D measurement data.2D image-based approaches do not rely on any 3D in- formation, thus being computationally efﬁcient and appro- priate for practical use.Jetchev and Bergmann [15] pro- posed CAGAN, which ﬁrst introduced the task of swap- ping fashion articles on human images.VITON [10] ad- dressed the same problem by proposing a coarse-to-ﬁne synthesis framework that involves TPS transformation of clothes.
Most existing virtual try-on methods tackle differ- ent aspects of VITON to synthesize perceptually convincing photo-realistic images. CP-VTON [31] adopted a geometric matching module to learn the parameters of TPS transfor-mation, which improves the accuracy of deformation.VT- NFP [36] and ACGPN [35] predicted the human-parsing maps of a person wearing the target clothes in advance to guide the try-on image synthesis.Even though the image quality at high resolution is an essential factor in evaluating the practicality of the generated images, none of the meth- ods listed above could generate such photo-realistic images at high resolution.3.Proposed Method Model Overview.As described in Fig.
3, given a ref- erence image I2R3HWof a person and a clothing imagec2R3HW(HandWdenote the image height and width, respectively), the goal of VITON-HD is to gen- erate a synthetic image ^I2R3HWof the same person wearing the target clothes c, where the pose and body shape ofIand the details of care preserved. While training the model with (I;c;^I)triplets is straightforward, construction of such dataset is costly.Instead, we use (I;c;I )where the person in the reference image Iis already wearing c.Since directly training on (I;c;I )can harm the model’s generalization ability at test time, we ﬁrst compose a clothing-agnostic person representation that leaves out the clothing information in Iand use it as an input.Our new clothing-agnostic person representation uses both the pose map and the segmentation map of the person to eliminate the clothing information in I(Section 3.1).
The model gen- erates the segmentation map from the clothing-agnostic per- son representation to help the generation of ^I(Section 3.2). We then deform cto roughly align it to the human body (Section 3.3).Lastly, we propose the ALIgnment-Aware Segment (ALIAS) normalization that removes the mislead- ing information in the misaligned area after deforming c.ALIAS generator ﬁlls the misaligned area with the clothing texture and maintains the clothing details (Section 3.4).3.1.Clothing-Agnostic Person Representation To train the model with pairs of candIalready wearing c, a person representation without the clothing information inIhas been utilized in the virtual try-on task.
Such rep- resentations have to satisfy the following conditions: (1) the original clothing item to be replaced should be deleted; (2) sufﬁcient information to predict the pose and the body shape of the person should be maintained; (3) the regions to be preserved ( e.g., face and hands) should be kept to main- tain the person’s identity. Problems of Existing Person Representations.In or- der to maintain the person’s shape, several approaches [10, 31, 36] provide a coarse body shape mask as a cue to syn- thesize the image, but fail to reproduce the body parts elab- orately ( e.g., hands).
To tackle this issue, ACGPN [35] em- ploys the detailed body shape mask as the input, and the neural network attempts to discard the clothing informa- መ𝑆 𝜃 Warp𝐼𝑎⨁𝑃⨁መ𝑆𝑐 𝑐𝒲(𝑐,𝜃) 𝑆𝑆𝑎 Remove Clothing & Arms 𝐼 𝑃 Remove Clothing & Arms 𝑆𝑎⨁𝑃⨁𝑐 መ𝐼(c) Clothes Deformation(a) Pre -processing (b) Segmentation Generation (d) Try -On Synthesis ALIAS GeneratorSegmentation Generator 𝐼𝑎 መ𝑆 Geometric Matching Module 𝐼𝑎⨁𝑃⨁𝒲(𝑐,𝜃)𝑀𝑚𝑖𝑠𝑎𝑙𝑖𝑔𝑛Figure 3: Overview of a VITON-HD. (a) First, given a reference image Icontaining a target person, we predict the seg- mentation map Sand the pose map P, and utilize them to pre-process IandSas a clothing-agnostic person image Ia and segmentation Sa.(b) Segmentation generator produces the synthetic segmentation ^Sfrom (Sa;P;c).(c) Geometric matching module deforms the clothing image caccording to the predicted clothing segmentation ^Scextracted from ^S.
(d) Finally, ALIAS generator synthesizes the ﬁnal output image ^Ibased on the outputs from the previous stages via our ALIAS normalization. tion to be replaced.However, since the body shape mask includes the shape of the clothing item, neither the coarse body shape mask nor the neural network could perfectly eliminate the clothing information.As a result, the original clothing item that is not completely removed causes prob- lems in the test phase.Clothing-Agnostic Person Representation.We pro- pose a clothing-agnostic image Iaand a clothing-agnostic segmentation map Saas inputs of each stage, which truly eliminate the shape of clothing item and preserve the body parts that need to be reproduced.We ﬁrst predict the segmentation map S2LHWand the pose map P2 R3HWof the image Iby utilizing the pre-trained net- works [7, 3], where Lis a set of integers indicating the se- mantic labels.
The segmentation map Sis used to remove the clothing region to be replaced and preserve the rest of the image. The pose map Pis utilized to remove the arms, but not the hands, as they are difﬁcult to reproduce.Based onSandP, we generate the clothing-agnostic image Ia and the clothing-agnostic segmentation map Sa, which al- low the model to remove the original clothing information thoroughly, and preserve the rest of the image.In addition, unlike other previous work, which adopts the pose heatmap with each channel corresponded to one keypoint, we con-catenateIaorSato the RGB pose map Prepresenting a skeletal structure that improves generation quality.3.2.Segmentation Generation Given the clothing-agnostic person representation (Sa;P), and the target clothing item c, the segmentation generatorGSpredicts the segmentation map ^S2LHW of the person in the reference image wearing c.
We train GSto learn the mapping between Sand(Sa;P;c), in which the original clothing item information is completely removed. As the architecture of GS, we adopt U-Net [25], and the total loss LSof the segmentation generator are written as LS=LcGAN +CELCE; (1) whereLCEandLcGAN denote the pixel-wise cross-entropy loss and conditional adversarial loss between ^SandS, re- spectively.CEis the hyperparameter corresponding to the relative importance between two losses.3.3.Clothing Image Deformation In this stage, we deform the target clothing item cto align it with ^Sc, which is the clothing area of ^S.We employ the geometric matching module proposed in CP- VTON [31] with the clothing-agnostic person representa- ConvALIAS ResBlkUpALIAS ResBlk ⋯ConvResize Resize Resize Resize ⋯ 1024x768መ𝐼 Conv ALIAS NormReLUConvALIAS NormReLUConv ALIAS NormReLUConvመ𝑆,𝑀𝑚𝑖𝑠𝑎𝑙𝑖𝑔𝑛 ℎ𝑖ℎ𝑖+1 𝑯i×𝑾𝒊Concat Add(b)ALIAS ResBlk 𝐼𝑎⨁𝑃⨁𝒲(𝑐,𝜃) (a)ALIAS GeneratorConv 𝑀𝑚𝑖𝑠𝑎𝑙𝑖𝑔𝑛መ𝑆𝐼𝑎⨁𝑃⨁𝑊(𝑐,𝜃)Figure 4: ALIAS generator.
(a) The ALIAS generator is composed of a series of ALIAS residual blocks, along with up- sampling layers. The input (Ia;P;W(c;))is resized and injected into each layer of the generator.(b) A detailed view of a ALIAS residual block.Resized (Ia;P;W(c;))is concatenated to hiafter passing through a convolution layer.Each ALIAS normalization layer leverages resized ^SandMmisalign to normalize the activation.tion(Ia;P)and^Scas inputs.A correlation matrix be- tween the features extracted from (Ia;P)andcis ﬁrst cal- culated .With the correlation matrix as an input, the regres- sion network predicts the TPS transformation parameters 2R255, and thencis warped by .In the training phase, the model takes Scextracted from Sinstead of ^Sc.The module is trained with the L1 loss between the warped clothes and the clothes Icthat is extracted from I.In addi- tion, the second-order difference constraint [35] is adopted to reduce obvious distortions in the warped clothing images from deformation.
The overall objective function to warp the clothes to ﬁt the human body is written as Lwarp =jjIc W(c;)jj1;1+constLconst; (2) whereWis the function that deforms cusing,Lconst is a second-order difference constraint, and const is the hyper- parameter forLconst . 3.4.Try-On Synthesis via ALIAS Normalization We aim to generate the ﬁnal synthetic image ^Ibased on the outputs from the previous stages.Overall, we fuse the clothing-agnostic person representation (Ia;P) and the warped clothing image W(c;), guided by ^S.(Ia;P;W(c;))is injected into each layer of the generator.For^S, we propose a new conditional normalization method called the ALIgnment-Aware Segment (ALIAS) normaliza- tion.ALIAS normalization enables the preservation of se- mantic information, and the removal of misleading infor- mation from the misaligned regions by leveraging ^Sand the mask of these regions.Alignment-Aware Segment Normalization.
Let us de- notehi2RNCiHiWias the activation of the i-th layer of a network for a batch of Nsamples, where Hi,Wi, and Ciindicate the height, width, and the number of channels ofhi, respectively. ALIAS normalization has two inputs: StandardizationConv Conv𝛽 𝛾 𝑀𝑚𝑖𝑠𝑎𝑙𝑖𝑔𝑛 −= 𝑀𝑎𝑙𝑖𝑔𝑛መ𝑆𝑐Conv መ𝑆𝑑𝑖𝑣Figure 5: ALIAS normalization.First, the activation is separately standardized according to the regions divided by Mmisalign , which can be obtained from the difference be- tween ^ScandMalign .Next, ^Sdivis convolved to create the modulation parameters and , and then the standardized activation is modulated with the parameters and .(1) the synthetic segmentation map ^S; (2) the misalign- ment binary mask Mmisalign2LHW, which excludes the warped mask of the target clothing image W(Mc;)from ^Sc(Mcdenotes the target clothing mask), i.e., Malign =^Sc\W(Mc;) (3) Mmisalign =^Sc Malign: (4) Fig.5 illustrates the workﬂow of the ALIAS normaliza- tion.We ﬁrst obtain Malign andMmisalign from Eq.
(3) and Eq. (4).We deﬁne the modiﬁed version of ^Sas^Sdiv, where ^Scin^Sseparates into Malign andMmisalign .ALIAS nor- malization standardizes the regions of Mmisalign and the other regions in hiseparately, and then modulates the stan- dardized activation using afﬁne transformation parameters 256192 512 384 1024 768 SSIM "LPIPS #FID # SSIM "LPIPS #FID # SSIM "LPIPS #FID # CP-VTON 0.739 0.159 56.23 0.791 0.141 31.96 0.786 0.158 43.28 ACGPN 0.842 0.064 26.45 0.863 0.067 15.22 0.856 0.102 43.39 VITON-HD* - - - - - - 0.893 0.054 12.47 VITON-HD 0.844 0.062 27.83 0.870 0.052 14.05 0.895 0.053 11.74 Table 1: Quantitative comparison with baselines across different resolutions.VITON-HD* is a VITON-HD variant where the standardization in ALIAS normalization is replaced by channel-wise standardization as in the original instance normalization.For the SSIM, higher is better.For the LPIPS and the FID, lower is better.inferred from ^Sdiv.
The activation value at site ( n2N;k2 Ci;y2Hi;x2Wi) is calculated by i k;y;x(^Sdiv)hi n;k;y;x i;m n;k i;m n;k+ i k;y;x(^Sdiv); (5) wherehi n;k;y;x is the activation at the site before normal- ization and i k;y;x and i k;y;x are the functions that convert ^Sdivto modulation parameters of the normalization layer. i;m n;kandi;m n;kare the mean and standard deviation of the activation in sample nand channel k.i;m n;kandi;m n;kare calculated by i;m n;k=1 j i;m njX (y;x)2 i;m nhi n;k;y;x (6) i;m n;k=vuut1 j i;m njX (y;x)2 i;m n(hi n;k;y;x i;m n;k)2;(7) where i;m ndenotes the set of pixels in region m, which is Mmisalign or the other region, and j i;m njis the number of pixels in i;m n.Similar to instance normalization [30], the activation is standardized per channel.However, ALIAS normalization divides the activation in channel kinto the activation in the misaligned region and the other region.
The rationale behind this strategy is to remove the mis- leading information in the misaligned regions. Speciﬁcally, the misaligned regions in the warped clothing image match the background that is irrelevant to the clothing texture.Per- forming a standardization separately on these regions leads to a removal of the background information that causes the artifacts in the ﬁnal results.In modulation, afﬁne pa- rameters inferred from the segmentation map modulate the standardized activation.Due to injecting semantic informa- tion at each ALIAS normalization layer, the layout of the human-parsing map in the ﬁnal result is preserved.ALIAS Generator.Fig.4 describes the overview of the ALIAS generator, which adopts the simpliﬁed architecture that discards the encoder part of an encoder-decoder net- work.The generator employs a series of residual blocks (ResBlk) with upsampling layers.Each ALIAS ResBlkconsists of three convolutional layers and three ALIAS nor- malization layers.
Due to the different resolutions that Res- Blks operate at, we resize the inputs of the normalization layers, ^SandMmisalign , before injecting them into each layer. Similarly, the input of the generator, (Ia;P;W(c;)), is resized to different resolutions.Before each ResBlk, the resized inputs (Ia;P;W(c;))are concatenated to the acti- vation of the previous layer after passing through a convolu- tion layer, and each ResBlk utilizes the concatenated inputs to reﬁne the activation.In this manner, the network per- forms the multi-scale reﬁnement at a feature level that bet- ter preserves the clothing details than a single reﬁnement at the pixel level.We train the ALIAS generator with the con- ditional adversarial loss, the feature matching loss, and the perceptual loss following SPADE [20] and pix2pixHD [32].Details of the model architecture, hyperparameters, and the loss function are described in the supplementary.4.Experiments 4.1.Experiment Setup Dataset.
We collected 1024 768 virtual try-on dataset for our research purpose, since the resolution of images on the dataset provided by Han et al. [10] was low.Speciﬁcally, we crawled 13,679 frontal-view woman and top clothing image pairs on an online shopping mall website.The pairs are split into a training and a test set with 11,647 and 2,032 pairs, respectively.We use the pairs of a person and a cloth- ing image to evaluate a paired setting, and we shufﬂe the clothing images for an unpaired setting.The paired setting is to reconstruct the person image with the original clothing item, and the unpaired setting is to change the clothing item on the person image with a different item.Training and Inference.With the goal of reconstruct- ingIfrom (Ia;c), the training of each stage proceeds in- dividually.During the training of the geometric matching module and the ALIAS generator, we use Sinstead of ^S.
While we aim to generate 1024 768 try-on images, we train the segmentation generator and the geometric match- ing module at 256 192. In the inference phase, after be- ing predicted by the segmentation generator at 256 192, the segmentation map is upscaled to 1024 768 and passed Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Segmentation (Ours) Figure 6: Qualitative comparison of the baselines.Target ClothesAgnostic (ACGPN)Output (ACGPN)Agnostic (Ours)Reference ImageOutput (Ours) Figure 7: Qualitative comparison of the segmentation gen- erator of ACGPN and VITON-HD.The clothing-agnostic segmentation map used by each model is also reported.to subsequent stages.Similarly, the geometric matching module predicts the TPS parameters at 256192, and the 1024768 clothing image deformed by the parameters is used in the ALIAS generator.We empirically found that this approach makes these two modules perform better with a lower memory cost than those trained at 1024 768.
De- tails of the model architecture and hyperparameters are de- scribed in the supplementary.4.2. Qualitative Analysis We compare VITON-HD with CP-VTON [31] and ACGPN [35], whose codes are publicly available.Follow- ing the training and inference procedure of our model, seg- mentation generators and geometric matching modules of the baselines are trained at 256 192, and the outputs from the modules are upscaled to 1024 768 during the inference.Comparison with Baselines.Fig.6 demonstrates that VITON-HD generates more perceptually convincing 1024768 images compared to the baselines.Our model clearly preserves the details of the target clothes, such as the logos and the clothing textures, due to the multi-scale reﬁnement at a feature level.In addition, regardless of what clothes the person is wearing in the reference image, our model synthesizes the body shape naturally.As shown in Fig.
7, the shape of the original clothing item remains in the synthetic segmentation map generated by ACGPN. On the other hand, the segmentation generator in VITON-HD suc- cessfully predicts the segmentation map regardless of the original clothing item, due to our newly proposed clothing- agnostic person representation.Although our model sur- passes the baselines qualitatively, there are a few limitations w/o ALIAS norm with ALIAS norm w/o ALIAS norm with ALIAS normFigure 8: Effects of ALIAS normalization.The orange colored areas in the enlarged images indicate the misaligned regions.0.040.060.080.10.120.140.160.18 Small Medium LargeLPIPS Misaligned AreaCP-VTON ACGPN OursFigure 9: LPIPS scores according to the degree of misalignment.to VITON-HD, which are reported in the supplementary with the additional qualitative results.Effectiveness of the ALIAS Normalization.
We study the effectiveness of ALIAS normalization by comparing our model to VITON-HD*, where the standardization in ALIAS normalization is replaced by channel-wise standard- ization, as in the original instance normalization [30]. Fig.8 shows that ALIAS normalization has the capability to ﬁll the misaligned areas with the target clothing texture by re- moving the misleading information.On the other hand, without utilizing ALIAS normalization, the artifacts are produced in the misaligned areas, because the background information in the warped clothing image is not removed as described in Section 3.4.ALIAS normalization, however, can handle the misaligned regions properly.4.3.Quantitative Analysis We perform the quantitative experiments in both a paired and an unpaired settings, in which a person wears the origi- nal clothes or the new clothes, respectively.We evaluate our method using three metrics widely used in virtual try-on.
The structural similarity (SSIM) [33] and the learned per- ceptual image patch similarity (LPIPS) [38] are used in the paired setting, and the frechet inception distance (FID) [11] score is adopted in the unpaired setting. The inception score [26] is not included in the experiments, since it cannot reﬂect whether the details of the clothing image are main- tained [10].The input of the each model contains different amount of information that offers advantages in reconstruct- ing the segmentation maps, thus we use the segmentation maps from the test set instead of the synthetic segmentation maps in the paired setting.Comparison across Different Resolutions.We com- pare the baselines quantitatively across different resolutions (256192, 512384, and 1024768) as shown in Table 1.Our model outperforms the baselines for SSIM and LPIPS across all resolutions.For FID score, our model signiﬁ- cantly surpasses CP-VTON, regardless of the resolutions.
The FID score in ACGPN is slightly lower than that of our model at the 256 192 resolution. However, at the1024768 resolution, our model achieves a lower FID score than ACGPN with a large margin.The results indicate that the baselines cannot handle 1024 768 images, while our model is trained in a stable manner, even at a high resolu- tion.This may be due to the limited capability of the U-Net architecture employed in the baseline models.Comparison According to the Degree of Misalign- ment.To verify the ability of ﬁlling the misaligned areas with the clothing texture, we perform experiments in the paired setting according to the degree of the misalignment.According to the number of pixels in the misaligned areas, we divide the test dataset in three types: small, medium, and large.For a fair comparison, each model uses the same seg- mentation maps and the same warped clothes as inputs to match the misaligned regions.
We evaluate LPIPS to mea- sure the semantic distances between the reference images and the reconstructed images. As shown in Fig.9, the wider the misaligned areas, the worse the performance of models, which means that the misalignment hinders the models from generating photo-realistic virtual try-on images.Compared to the baselines, our model consistently performs better, and the performance of our model decreases less as the degree of misalignment increases.5.Conclusions We propose the VITON-HD that synthesizes photo- realistic 1024768 virtual try-on images.The proposed ALIAS normalization can properly handle the misaligned areas and propagate the semantic information throughout the ALIAS generator, which preserves the details of the clothes via the multi-scale reﬁnement.Qualitative and quantitative experiments demonstrate that VITON-HD sur- passes existing virtual try-on methods with a large margin.Acknowledgments.
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. NRF-2019R1A2C4070420) and Seoul R&BD Program (CD200024) through the Seoul Business Agency (SBA) funded by the Seoul Metropolitan Government.References [1] Ivan Anokhin, Pavel Solovev, Denis Korzhenkov, Alexey Kharlamov, Taras Khakhulin, Aleksei Silvestrov, Sergey Nikolenko, Victor Lempitsky, and Gleb Sterkin.High- resolution daytime translation without domain labels.In Proc.of the IEEE conference on computer vision and pat- tern recognition (CVPR) , pages 7488–7497, 2020.3 [2] Andrew Brock, Jeff Donahue, and Karen Simonyan.Large scale gan training for high ﬁdelity natural image synthesis.In Proc.the International Conference on Learning Representa- tions (ICLR) , 2018.3 [3] Z Cao, T Simon, SE Wei, YA Sheikh, et al.Openpose: Realtime multi-person 2d pose estimation using part afﬁn- ity ﬁelds.
The IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2019. 4 [4] Harm De Vries, Florian Strub, J ´er´emie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C Courville.Modu- lating early visual processing by language.In Proc.the Ad- vances in Neural Information Processing Systems (NeurIPS) , pages 6594–6604, 2017.3 [5] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bochao Wang, Hanjiang Lai, Jia Zhu, Zhiting Hu, and Jian Yin.Towards multi-pose guided virtual try-on network.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 9026–9035, 2019.3 [6] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bowen Wu, Bing-Cheng Chen, and Jian Yin.Fw-gan: Flow-navigated warping gan for video virtual try-on.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 1161–1170, 2019.2 [7] Ke Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, Ming Yang, and Liang Lin.Instance-level human parsing via part grouping network.In Proc.
of the European Conference on Computer Vision (ECCV) , pages 770–785, 2018. 4 [8] Peng Guan, Loretta Reiss, David A Hirshberg, Alexander Weiss, and Michael J Black.Drape: Dressing any person.ACM Transactions on Graphics (TOG) , 31(4):1–10, 2012.3 [9] Xintong Han, Xiaojun Hu, Weilin Huang, and Matthew R Scott.Clothﬂow: A ﬂow-based model for clothed person generation.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 10471–10480, 2019.2, 3, 13 [10] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry S Davis.Viton: An image-based virtual try-on network.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 7543–7552, 2018.2, 3, 6, 8 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.Gans trained by a two time-scale update rule converge to a local nash equilib- rium.In Proc.the Advances in Neural Information Process- ing Systems (NeurIPS) , pages 6629–6640, 2017.
8 [12] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 1501–1510, 2017.3 [13] Sergey Ioffe and Christian Szegedy.Batch normalization: Accelerating deep network training by reducing internal co-variate shift.In Proc.the International Conference on Ma- chine Learning (ICML) , pages 448–456, 2015.3 [14] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.Image-to-image translation with conditional adversar- ial networks.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 1125–1134, 2017.3 [15] Nikolay Jetchev and Urs Bergmann.The conditional analogy gan: Swapping fashion articles on people images.In Proc.of the IEEE international conference on computer vision work- shop (ICCVW) , pages 2287–2292, 2017.3 [16] Diederik P Kingma and Jimmy Ba.Adam: A method for stochastic optimization.arXiv:1412.6980 , 2014.
13 [17] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks.In Proc.of the IEEE international con- ference on computer vision (ICCV) , pages 2794–2802, 2017.12 [18] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.Spectral normalization for generative ad- versarial networks.In Proc.the International Conference on Learning Representations (ICLR) , 2018.11 [19] Augustus Odena, Christopher Olah, and Jonathon Shlens.Conditional image synthesis with auxiliary classiﬁer gans.InProc.the International Conference on Machine Learning (ICML) , pages 2642–2651, 2017.3 [20] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.Semantic image synthesis with spatially-adaptive nor- malization.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 2337–2346, 2019.3, 6, 12 [21] Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A.
Efros, and Richard Zhang. Swapping autoencoder for deep image manipulation.In Proc.the Ad- vances in Neural Information Processing Systems (NeurIPS) , 2020.3 [22] Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons- Moll.Tailornet: Predicting clothing in 3d as a function of hu- man pose, shape and garment style.In Proc.of the IEEE con- ference on computer vision and pattern recognition (CVPR) , pages 7365–7375, 2020.3 [23] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black.Clothcap: Seamless 4d clothing capture and retar- geting.ACM Transactions on Graphics (TOG) , 36(4):1–15, 2017.3 [24] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo- geswaran, Bernt Schiele, and Honglak Lee.Generative adversarial text to image synthesis.In Proc.the Inter- national Conference on Machine Learning (ICML) , pages 1060–1069, 2016.3 [25] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.U-net: Convolutional networks for biomedical image segmentation.
InInternational Conference on Medical Image Computing and Computer Assisted Intervention , pages 234–241, 2015. 2, 4, 11 [26] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.Improved techniques for training gans.In Proc.the Advances in Neural Informa- tion Processing Systems (NeurIPS) , pages 2234–2242, 2016.8 [27] Masahiro Sekine, Kaoru Sugita, Frank Perbet, Bj ¨orn Stenger, and Masashi Nishiyama.Virtual ﬁtting by single-shot body shape estimation.In International Conference on 3D Body Scanning Technologies , pages 406–413, 2014.3 [28] Wei Shen and Rujie Liu.Learning residual images for face attribute manipulation.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 4030–4038, 2017.3 [29] Karen Simonyan and Andrew Zisserman.Very deep convo- lutional networks for large-scale image recognition.arXiv preprint arXiv:1409.1556 , 2014.13 [30] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
In- stance normalization: The missing ingredient for fast styliza- tion. arXiv preprint arXiv:1607.08022 , 2016.3, 6, 8 [31] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang.Toward characteristic- preserving image-based virtual try-on network.In Proc.of the European Conference on Computer Vision (ECCV) , pages 589–604, 2018.2, 3, 4, 7, 13 [32] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.High-resolution image syn- thesis and semantic manipulation with conditional gans.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 8798–8807, 2018.2, 3, 6, 11, 12 [33] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si- moncelli.Image quality assessment: from error visibility to structural similarity.IEEE Transactions on Image Process- ing, 13(4):600–612, 2004.8 [34] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.
Attngan: Fine- grained text to image generation with attentional genera- tive adversarial networks. In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 1316–1324, 2018.3 [35] Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wang- meng Zuo, and Ping Luo.Towards photo-realistic virtual try-on by adaptively generating-preserving image content.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 7850–7859, 2020.2, 3, 5, 7, 12, 13 [36] Ruiyun Yu, Xiaoqi Wang, and Xiaohui Xie.Vtnfp: An image-based virtual try-on network with body and cloth- ing feature preservation.In Proc.of the IEEE international conference on computer vision (ICCV) , pages 10511–10520, 2019.2, 3 [37] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus- tus Odena.Self-attention generative adversarial networks.InProc.the International Conference on Machine Learning (ICML) , pages 7354–7363, 2019.
13 [38] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric.In Proc.of the IEEE con- ference on computer vision and pattern recognition (CVPR) , 2018.8[39] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka.Sean: Image synthesis with semantic region-adaptive nor- malization.In Proc.of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 5104–5113, 2020.3 Supplementary Material A.Implementation Details A.1.Pre-processing Details This section introduces the details of generating our clothing-agnostic person representation.To remove the de- pendency on the clothing item originally worn by a person, regions that can provide any original clothing information, such as the arms that hint at the sleeve length, should be eliminated.Therefore, when generating a clothing-agnostic imageIa, we remove the arms from the reference image I.
For the same reason, legs should be removed if the pants are the target clothing items. We mask the regions with a gray color, so that the masked pixels of the normalized im- age would have a value of 0.We add padding to the masks to thoroughly remove these regions, and the width of the padding is empirically determined.A.2.Model Architectures This section introduces the architectures of the seg- mentation generator, the geometric matching module, and ALIAS generator in detail.Segmentation Generator.The segmentation generator has the structure of U-Net [25], which consists of convolu- tional layers, downsampling layers, and upsampling layers.Two multi-scale discriminators [32] are employed for the conditional adversarial loss.The details of the segmenta- tion generator architecture are shown in Fig.10.Geometric Matching Module.The geometric matching module consists of two feature extractors and a regression network.
A correlation matrix is calculated from the two extracted features, and the regression network predicts the TPS parameter with the correlation matrix. The feature extractor is composed of a series of convolutional layers, and the regression network consists of a series of convolu- tional layers followed by a fully connected layer.The de- tails are shown in Fig.11.ALIAS Generator.The architecture of the ALIAS gen- erator consists of a series of ALIAS ResBlks with nearest- neighbor upsampling layers.We employ two multi-scale discriminators with instance normalization.Spectral nor- malization [18] is applied to all the convolutional layers.Note that we separately standardize the activation based on the misalignment mask Mmisalign only in the ﬁrst ﬁveALIAS ResBlks.The details of the ALIAS generator archi- tecture is shown in Fig.12.A.3.
Training Details This section introduces the losses and the hyperparame- ters for the segmentation generator, the geometric matching module, and the ALIAS generator. Segmentation Generator.The segmentation genera- torGSuses the clothing-agnostic segmentation map Sa, the pose map P, and the clothing item cas inputs ( ^S= GS(Sa;P;c)) to predict the segmentation map ^Sof the person in the reference image wearing the target cloth- ing item.The segmentation generator is trained with the ConvBlk (64), MaxPool (2) ConvBlk (128), MaxPool (2) ConvBlk (256), MaxPool (2) ConvBlk (512), MaxPool (2) ConvBlk (1024) Upsample (2), ConvBlk (512) Upsample (2), ConvBlk (256) Upsample (2), ConvBlk (128) Upsample (2), ConvBlk (64) 3x3 Conv (13), Softmax Figure 10: Segmentation Generator.kkConv (x) denotes a convolutional layer where the kernel size is kand the out- put channel is x.
Also, ConvBlk ( x) denotes a block, which consists of two series of 3 3 convolutional layer, instance normalization, and ReLU activation. 4x4-↓2 Conv (64), BN, ReLU 4x4-↓2 Conv (64), BN, ReLU 4x4-↓2 Conv (128), BN, ReLU 4x4-↓2 Conv (128), BN, ReLU 4x4-↓2 Conv (256), BN, ReLU 4x4-↓2 Conv (256), BN, ReLU 4x4-↓2 Conv (512), BN, ReLU 4x4-↓2 Conv (512), BN, ReLU 3x3 Conv (512), BN, ReLU 3x3 Conv (512), BN, ReLU 4x4-↓2 Conv (512), BN, ReLU 4x4-↓2 Conv (256), BN, ReLU 3x3 Conv (128), BN, ReLU 3x3 Conv (64), BN, ReLU Linear (2x5x5), Tanh 𝜃3x3 Conv (512), ReLU 3x3 Conv (512), ReLUFigure 11: Geometric Matching Module.kk#2 Conv (x) denotes a convolutional layer where the kernel size is k, the stride is 2, and the output channel is x.cross-entropy lossLCEand the conditional adversarial loss LcGAN , which is LSGAN loss [17].
The full loss LSfor the segmentation generator are written as LS=LcGAN +CELCE (8) LCE= 1 HWX k2C;y2H;x2WSk;y;xlog(^Sk;y;x) (9) LcGAN =E(X;S)[log(D(X;S))] +EX[1 log(D(X;^S))];(10) whereCEis the hyperparameter for the cross-entropy loss. In the experiment, CEis set to 10.In Eq.(9), Syxkand ^Syxkindicate the pixel values of the segmentation map of the reference image Sand^Scorresponding to the coordi- nates (x;y)in channelk.The symbols H,WandCindi- cate the height, width, and the number of channels of S.In Eq.(10), the symbol Xindicates the inputs of the generator (Sa;P;c ), andDdenotes the discriminator.The learning rate of the generator and the discriminator is 0.0004.We adopt the Adam optimizer with 1= 0:5 and 2= 0:999.We train the segmentation generator for 200,000 iterations with the batch size of 8.Geometric Matching Module.The inputs of the geo- metric matching module are c,P, clothing-agnostic image Ia, and ^Sc, which is the clothing area of ^S.
The output is 3x3 Conv (1024) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (1024), Upsample (2) ALIAS ResBlk (512), Upsample (2) ALIAS ResBlk (256), Upsample (2) ALIAS ResBlk (128), Upsample (2) ALIAS ResBlk (64), Upsample (2) ALIAS ResBlk (32) 3x3 Conv (3), Tanh Figure 12: ALIAS Generator. The segmentation map Sand the misalignment mask Mmisalign are passed to the gener- ator through the proposed ALIAS ResBlks.the TPS transformation parameters .The overall objective function is written as Lwarp =jjIc W(c;)jj1;1+constLconst (11) Lconst =X p2Pj(jjjpp0jj2 jjpp1jj2j+jjjpp2jj2 jjpp3jj2j) +(jS(p;p0) S(p;p1)j+jS(p;p2) S(p;p3)j); (12) whereWis the function that deforms cusing, andIcis the clothing item extracted from the reference image I.Lconst is a second-order difference constraint [35], and const is the hyperparameter for Lconst .In the experiment, we set const to 0.04.In Eq.
(12), the symbol pindicates a sam- pled TPS control point from the entire control points set P, andp0,p1,p2, andp3are top, bottom, left and right point ofp, respectively. The function S(p;pi)denotes the slope betweenpandpi.The learning rate of the geometric matching module is 0.0002.We adopt the Adam optimizer with 1= 0:5and 2= 0:999.We train the geometric matching module for 50,000 iterations with the batch size of 8.ALIAS Generator.The loss function of ALIAS genera- tor follows those of SPADE [20] and pix2pixHD [32], as it Grid 5x5 Grid 10x10 Grid 20x20 ClothFlow Figure 13: Qualitative comparisons of TPS transformation with various grid numbers and the ﬂow estimation from ClothFlow.Method Warp-SSIM"MACs#Mask-SSIM" ClothFlow 0.841?8.13G 0.803?VITON-HD 0.782 4.47G 0.852 Table 2:?denotes a score taken from the ClothFlow paper, and we train VITON-HD in the same setting ( e.g., dataset and resolution).We compute MACs of their warping mod- ules at 256192.
contains the conditional adversarial loss LcGAN , the feature matching lossLFM, and the perceptual loss Lpercept . Let DIbe the discriminator, Iandcbe the given reference and target clothing images, and ^Ibe the synthetic image gener- ated by the generator.Sdivis the modiﬁed version of the segmentation map S.The full lossLIof our generator is written as LI=LcGAN +FMLFM+perceptLpercept (13) LcGAN =EI[log(DI(Sdiv;I))] +E(I;c)[1 log(DI(Sdiv;^I))](14) LFM=E(I;c)TX i=11 Ki[jjD(i) I(Sdiv;I) D(i) I(Sdiv;^I)jj1;1] (15) Lpercept =E(I;c)VX i=11 Ri[jjF(i)(I) F(i)(^I)jj1;1];(16) whereFMandpercept are hyperparameters.In the ex- periment, both FMandpercept are set to 10.Tis the number of layers in DI, andD(i) IandKiare the activa- tion and the number of elements in the i-th layer of DI, respectively.Similarly, Vis the number of layers used in the VGG network F[29], andF(i)andRiare the acti- vation and the number of elements in the i-th layer of F, respectively.
We replace the standard adversarial loss with the Hinge loss [37]. The learning rate of the generator and the discriminator is 0.0001 and 0.0004, respectively.We adopt the Adam op- timizer [16] with 1= 0and 2= 0:9.We train the ALIAS generator for 200,000 iterations with the batch size of 4.B.Additional Experiments B.1.Comparison with ClothFlow To demonstrate that the optical ﬂow estimation does not solve the misalignment completely, we re-implement the ﬂow estimation module of ClothFlow [9] based on the orig- inal paper.Fig.13 shows that the misalignment still oc- curs, although both TPS with a higher grid number (e.g., a 1010 or 2020 grid) and the ﬂow estimation module of ClothFlow can reduce the misaligned regions.The reason is that the regularization to avoid the artifacts ( e.g., TV loss) prevents the warped clothes from ﬁtting perfectly into the target region.
In addition, we evaluate the accuracy and the computational cost of warping modules in VITON-HD and ClothFlow with Warp-SSIM [9] and MACs, respectively. We also measure how well the models reconstruct the cloth- ing using Mask-SSIM [9].Table 2 shows that the ClothFlow warping module has the better accuracy than ours, whereas the higher Mask-SSIM in VITON-HD proves that ALIAS normalization is more effective at solving the misalignment problem than the improved warping method.We found that the ClothFlow warping module needs a huge computational cost (MACs: 130.03G) at 1024 768, but the cost could be reduced when predicting the optical ﬂow map at 256 192.Table 2 demonstrates that the ClothFlow warping module still needs more computational cost than ours, yet it is a viable option to combine the ﬂow estimation module with ALIAS generator.
0.960.010.03 0.020.240.74 0.020.750.23 0% 20% 40% 60% 80% 100%OursACGPNCP-VTONQuality and Realism (%) Top1 Top2 Top3 0.880.030.09 0.10.280.62 0.020.690.29 0% 20% 40% 60% 80% 100%OursACGPNCP-VTONPreservation of Clothing Details (%) Top1 Top2 Top3 Figure 14: User study results. We compare our model with CP-VTON [31] and ACGPN [35].Figure 15: Failure cases of VITON-HD.B.2.User Study We further evaluate our model and other baselines via a user study in the unpaired setting.We randomly select 30 sets of a reference image and a target clothing image from the test dataset.Given the reference images and the target clothes, the users are asked to rank the 1024 768 outputs of our model and baselines according to the follow- ing questions: (1) Which image is the most photo-realistic?(2) Which image preserves the details of the target cloth- ing the most?As shown in Fig.14, it can be observed that our approach achieves the rank 1 votes more than 88% for the both questions.
The result demonstrates that our model generates more realistic images, and preserves the details of the clothing items compared to the baselines. B.3.Qualitative Results We provide additional qualitative results to demonstrate our model’s capability of handling high quality image syn- thesis.Fig.16, 17, 18, and 19 show the qualitative com- parison of the baselines across different resolutions.Fig.20, 21, 22, and 23 show additional results of VITON-HD at 1024768 resolution.C.Failure Cases and Limitations Fig.15 shows the failure cases of our model caused by the inaccurately predicted segmentation map or the inner collar region indistinguishable from the other clothing re- gion.Also, the boundaries of the clothing textures occa- sionally fade away.The limitations of our model are as follows.VITON-HD is trained to preserve the bottom clothing items, limiting the presentation of the target clothes ( e.g., whether they are tucked in).
It can be a valuable future direction to gener- ate multiple possible outputs from a single input pair. Next, our dataset mostly consists of slim women and top cloth- ing images, which makes VITON-HD handle only a limited range of body shapes and clothing during the inference.We believe that VITON-HD has the capability to cover more diverse cases when the images of various body shapes and clothing types are provided.Finally, existing virtual try-on methods including VITON-HD do not provide robust per- formance for in-the-wild images.We think generating real- istic try-on images for the in-the-wild images is an interest- ing topic for future work.Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Reference Image Target Clothes CP-VTON ACGPN VITON -HD (Ours) Figure 16: Qualitative comparison of the baselines (256 192).Reference Image ACGPN VITON -HD (Ours) Target Clothes CP-VTON Figure 17: Qualitative comparison of the baselines (512 384).
Figure 18: Qualitative comparison of the baselines (1024 768). Figure 19: Qualitative comparison of the baselines (1024 768).Reference ImageTarget ClothesFigure 20: Additional qualitative results of VITON-HD.Figure 21: Sample 1 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.Figure 22: Sample 2 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.Figure 23: Sample 3 of VITON-HD.( Left) The synthetic image.( Right ) The reference image and the target clothing item.
Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.eduŁukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the ﬁrst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efﬁcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the ﬁnal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information ﬂow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and ﬁxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneﬁt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiﬁcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the ﬁrst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conﬁguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of ﬂoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision ﬂoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and Jürgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735–1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] Łukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio Łukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] Oﬁr Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overﬁtting.Journal of Machine Learning Research , 15(1):1929–1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D.
D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440–2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google’s neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11 ﻿Instruction,Input,Response 고객이 요금제 변경을 요청함,현재 5G 요금제를 사용 중인데 LTE 요금제로 변경하고 싶어요.,"고객님, 현재 사용 중인 5G 요금제를 LTE 요금제로 변경하시려면, 가까운 SKT 대리점을 방문하시거나 T월드 앱에서 변경 가능합니다.원하시는 요금제를 말씀해 주시면 자세히 안내해 드리겠습니다." 고객이 데이터 사용량을 조회하고 싶어함,이번 달 데이터 사용량이 얼마나 되나요?,"고객님, 현재까지 사용하신 데이터는 8.5GB이며, 남은 데이터는 1.5GB입니다.추가로 데이터가 필요하시면 데이터 쿠폰을 구매하실 수도 있습니다." 고객이 해외 로밍 요금제를 문의함,"다음 주에 일본으로 여행 가는데, 로밍 요금제 어떻게 되나요?","고객님, 일본에서 사용 가능한 로밍 요금제는 '바로 로밍 300MB(9,900원/일)' 또는 '로밍 무제한(13,900원/일)' 등이 있습니다.더 자세한 사항은 T월드에서 확인 가능합니다." 고객이 결제일 변경을 요청함,매월 요금 결제일을 바꾸고 싶은데 가능한가요?,"네, 고객님.요금 결제일 변경은 매월 1일, 15일, 25일 중 선택 가능합니다.원하시는 날짜를 말씀해 주시면 변경 도와드리겠습니다." 고객이 기기 할부금 잔액을 확인하려 함,현재 사용 중인 휴대폰 할부금이 얼마나 남았나요?,"고객님, 현재 단말기 할부금은 12개월 남아 있으며, 남은 금액은 총 360,000원입니다.조기 완납을 원하시면 T월드에서 납부 가능합니다." 고객이 가족 결합 할인 혜택을 문의함,"가족 결합 할인을 받고 싶은데, 어떻게 하면 되나요?","고객님, 가족 결합 할인은 가족 구성원 2인 이상이 SKT 회선을 사용 중일 경우 가능합니다.가족관계 증빙서류를 준비하셔서 가까운 대리점을 방문해 주시면 신청 도와드리겠습니다." 고객이 개통 가능한 스마트폰 기종을 문의함,아이폰 15 개통 가능한가요?,"고객님, 현재 아이폰 15는 정상 개통 가능하며, 색상 및 저장 용량에 따라 재고가 다를 수 있습니다.
가까운 SKT 대리점에 방문하시면 즉시 개통 가능합니다." 고객이 데이터 쿠폰 구매 방법을 문의함,데이터 1GB 추가하려면 어떻게 해야 하나요?,"고객님, 데이터 1GB 추가는 T월드 앱 또는 SKT 고객센터(114)에서 구매 가능합니다.
1GB 데이터 쿠폰은 5,500원에 제공되며, 즉시 적용됩니다." 고객이 5G 속도 저하 문제를 제기함,5G인데 속도가 너무 느려요.왜 그런가요?,"고객님, 현재 계신 지역의 네트워크 환경에 따라 속도가 저하될 수 있습니다.잠시 후에도 동일한 문제가 발생하면 SKT 고객센터로 문의 주시면 신속히 점검 도와드리겠습니다." 고객이 번호 변경을 요청함,현재 전화번호를 바꾸고 싶은데 어떻게 하면 되나요?,"고객님, 번호 변경은 SKT 대리점 방문 또는 고객센터를 통해 가능합니다.번호 변경 시 기존 번호로 걸려오는 전화는 일정 기간 안내 메시지가 제공됩니다." 고객이 2G 서비스 종료에 대해 문의함,"2G 서비스가 종료된다고 하던데, 어떻게 해야 하나요?","고객님, SKT 2G 서비스는 종료되었으며, 3G, LTE, 5G 기기로 기기 변경을 권장드립니다.보상 프로그램도 진행 중이니 자세한 사항은 고객센터로 문의 부탁드립니다." 고객이 분실폰 위치 추적을 요청함,핸드폰을 잃어버렸는데 위치 추적이 가능할까요?,"고객님, '내 디바이스 찾기' 기능을 이용하시면 위치 추적이 가능합니다.추가로 고객센터를 통해 분실 신고를 하시면 임시 정지 및 보호 조치를 도와드릴 수 있습니다." 고객이 멤버십 혜택을 문의함,SKT VIP 멤버십 혜택에는 어떤 것이 있나요?,"고객님, SKT VIP 멤버십 혜택으로 영화 할인, 카페 무료 음료, 데이터 리필 쿠폰 제공 등이 있습니다.자세한 내용은 T월드 앱에서 확인 가능합니다." Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.eduŁukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the ﬁrst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efﬁcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the ﬁnal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information ﬂow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and ﬁxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneﬁt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiﬁcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the ﬁrst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conﬁguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of ﬂoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision ﬂoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and Jürgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735–1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] Łukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio Łukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] Oﬁr Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overﬁtting.Journal of Machine Learning Research , 15(1):1929–1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D.
D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440–2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google’s neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11 Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N.Gomezy University of Toronto aidan@cs.toronto.eduŁukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.The best performing models also connect the encoder and decoder through an attention mechanism.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5].Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the ﬁrst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, and efﬁcient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.yWork performed while at Google Brain.zWork performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden statesht, as a function of the previous hidden state ht 1and the input for position t.This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [ 18] and conditional computation [ 26], while also improving model performance in case of the latter.The fundamental constraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16].
In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.
In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11].In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n).Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time.
At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers.Each layer has two sub-layers.The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2 Figure 1: The Transformer - model architecture.wise fully connected feed-forward network.We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1].That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i.
3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.3.2.1 Scaled Dot-Product Attention We call our particular attention "Scaled Dot-Product Attention" (Figure 2).The input consists of queries and keys of dimension dk, and values of dimension dv.We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of several attention layers running in parallel.query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the values.In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk.Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3].We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.These are concatenated and once again projected, resulting in the ﬁnal values, as depicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.With a single attention head, averaging inhibits this.4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1.
Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk. 4 MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO where head i= Attention( QWQ i;KWK i;VWV i) Where the projections are parameter matrices WQ i2Rdmodeldk,WK i2Rdmodeldk,WV i2Rdmodeldv andWO2Rhdvdmodel.In this work we employ h= 8 parallel attention layers, or heads.For each of these we use dk=dv=dmodel=h= 64 .Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.This allows every position in the decoder to attend over all positions in the input sequence.This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].
The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.Each position in the encoder can attend to all positions in the previous layer of the encoder.Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.We need to prevent leftward information ﬂow in the decoder to preserve the auto-regressive property.We implement this inside of scaled dot-product attention by masking out (setting to  1) all values in the input of the softmax which correspond to illegal connections.See Figure 2.3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0;xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24].In the embedding layers, we multiply those weights bypdmodel.
3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.
There are many choices of positional encodings, learned and ﬁxed [8]. In this work, we use sine and cosine functions of different frequencies: PE(pos;2i)=sin(pos=100002i=d model) PE(pos;2i+1)=cos(pos=100002i=d model) whereposis the position and iis the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid.The wavelengths form a geometric progression from 2to100002.We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of PEpos.We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-range dependencies is a key challenge in many sequence transduction tasks.One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.
The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations.
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n=r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions.Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k.Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side beneﬁt, self-attention could yield more interpretable models.We inspect attention distributions from our models and present and discuss examples in the appendix.Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.5 Training This section describes the training regime for our models.5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens.
For English-French, we used the signiﬁcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.We trained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps (3.5 days).5.3 Optimizer We used the Adam optimizer [ 17] with 1= 0:9, 2= 0:98and= 10 9.
We varied the learning rate over the course of training, according to the formula: lrate =d 0:5 modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3) This corresponds to increasing the learning rate linearly for the ﬁrst warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 .5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the sub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.For the base model, we use a rate of Pdrop= 0:1.7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1:01020 GNMT + RL [31] 24.6 39.92 2:310191:41020 ConvS2S [8] 25.16 40.46 9:610181:51020 MoE [26] 26.03 40.56 2:010191:21020 Deep-Att + PosUnk Ensemble [32] 40.4 8:01020 GNMT + RL Ensemble [31] 26.30 41.16 1:810201:11021 ConvS2S Ensemble [8] 26.36 41.29 7:710191:21021 Transformer (base model) 27.3 38.1 3:31018 Transformer (big) 28.4 41.0 2:31019 Label Smoothing During training, we employed label smoothing of value ls= 0:1[30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4.
The conﬁguration of this model is listed in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs.Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model.The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We used beam search with a beam size of 4and length penalty = 0:6[31].These hyperparameters were chosen after experimentation on the development set.
We set the maximum output length during inference to input length + 50, but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.We estimate the number of ﬂoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision ﬂoating-point capacity of each GPU5.6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.We used beam search as described in the previous section, but no checkpoint averaging.We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8 Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the base model.All metrics are on the English-to-German translation development set, newstest2013.Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
N d modeldffh d kdvPdroplstrain PPL BLEU params steps (dev) (dev)106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial.We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting.
In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.In the former task our best model outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.9 References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprint arXiv:1607.06450 , 2016.[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointly learning to align and translate.CoRR , abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .Le.Massive exploration of neural machine translation architectures.CoRR , abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.arXiv preprint arXiv:1601.06733 , 2016.[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statistical machine translation.CoRR , abs/1406.1078, 2014.[6]Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXiv preprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluation of gated recurrent neural networks on sequence modeling.CoRR , abs/1412.3555, 2014.[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolu- tional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2 , 2017.[9]Alex Graves.Generating sequences with recurrent neural networks.arXiv preprint arXiv:1308.0850 , 2013.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016.[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies, 2001.[12] Sepp Hochreiter and Jürgen Schmidhuber.Long short-term memory.Neural computation , 9(8):1735–1780, 1997.[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploring the limits of language modeling.arXiv preprint arXiv:1602.02410 , 2016.[14] Łukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conference on Learning Representations (ICLR) , 2016.[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2 , 2017.[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.
Structured attention networks. InInternational Conference on Learning Representations , 2017.[17] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR , 2015.[18] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprint arXiv:1703.10722 , 2017.[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130 , 2017.[20] Samy Bengio Łukasz Kaiser.Can active memory replace attention?In Advances in Neural Information Processing Systems, (NIPS) , 2016.10 [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attention- based neural machine translation.arXiv preprint arXiv:1508.04025 , 2015.[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.A decomposable attention model.In Empirical Methods in Natural Language Processing , 2016.
[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization.arXiv preprint arXiv:1705.04304 , 2017.[24] Oﬁr Press and Lior Wolf.Using the output embedding to improve language models.arXiv preprint arXiv:1608.05859 , 2016.[25] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with subword units.arXiv preprint arXiv:1508.07909 , 2015.[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.arXiv preprint arXiv:1701.06538 , 2017.[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov.Dropout: a simple way to prevent neural networks from overﬁtting.Journal of Machine Learning Research , 15(1):1929–1958, 2014.[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.End-to-end memory networks.In C.Cortes, N.D.
Lawrence, D. D.Lee, M.Sugiyama, and R.Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440–2448.Curran Associates, Inc., 2015.[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR , abs/1512.00567, 2015.[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google’s neural machine translation system: Bridging the gap between human and machine translation.arXiv preprint arXiv:1609.08144 , 2016.[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models with fast-forward connections for neural machine translation.CoRR , abs/1606.04199, 2016.11
